{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "class MissingEntryDataset(Dataset):\n",
    "    def __init__(self, data: torch.tensor, min_missing: float, max_missing: float):\n",
    "        self.data = data\n",
    "\n",
    "        n, d = data.shape\n",
    "        self.min_missing = int(min_missing * d)\n",
    "        self.max_missing = int(max_missing * d)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        n_missing = np.random.randint(self.min_missing, self.max_missing)\n",
    "        idx = np.arange(len(sample))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:n_missing]\n",
    "        mask = torch.zeros(sample.shape, dtype=torch.long)\n",
    "        mask[idx] = 1\n",
    "        mask = mask.bool()\n",
    "\n",
    "        return sample, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3784, 946)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"TCPA_data_sel.csv\")\n",
    "\n",
    "# select the 189 real valued columns only\n",
    "X = df.iloc[:, 2:].values.astype(\"float32\")\n",
    "Xs = (X - X.mean(axis=0)) / X.var(axis=0)\n",
    "\n",
    "# split train/test\n",
    "n = len(Xs)\n",
    "idx = np.arange(len(Xs))\n",
    "ncut = int(n * 0.8)\n",
    "Xtrain = MissingEntryDataset(torch.tensor(Xs[idx[:ncut]]), 0.1, 0.4)\n",
    "Xtest = MissingEntryDataset(torch.tensor(Xs[idx[ncut:]]), 0.1, 0.4)\n",
    "len(Xtrain), len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (enc): Sequential(\n",
      "    (0): Linear(in_features=189, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (mean): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (log_variance): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (dec): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=512, out_features=189, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, ins=189, hidden=512, latent=64, variational=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.variational = variational\n",
    "        \n",
    "        self.enc = nn.Sequential(nn.Linear(ins, hidden), \n",
    "                                nn.ReLU(),\n",
    "                                #nn.BatchNorm1d(hidden),\n",
    "                                #nn.Linear(hidden, hidden),\n",
    "                                #nn.ReLU(),\n",
    "                                nn.BatchNorm1d(hidden))\n",
    "        \n",
    "        self.mean = nn.Linear(hidden, latent)\n",
    "        self.log_variance = nn.Linear(hidden, latent)\n",
    "        \n",
    "        self.dec = nn.Sequential(nn.Linear(latent, hidden),\n",
    "                                nn.ReLU(),\n",
    "                                #nn.BatchNorm1d(hidden),\n",
    "                                #nn.Linear(hidden, hidden),\n",
    "                                #nn.ReLU(),\n",
    "                                nn.BatchNorm1d(hidden),\n",
    "                                nn.Linear(hidden, ins))\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        mu = self.mean(x)\n",
    "        log_var = self.log_variance(x)\n",
    "        z = self.sample(mu, log_var) if self.variational else mu\n",
    "        x = self.dec(z)\n",
    "        return x, mu, log_var\n",
    "    \n",
    "    def gibbs(self, x0, mask):\n",
    "        # initalize unobserved (masked out) with random entries\n",
    "        xn = x0[:]\n",
    "        xn[mask] = torch.randn(mask.shape)[mask]\n",
    "        # iterativly predict\n",
    "        for _ in range(20):\n",
    "            # reconstruction step\n",
    "            xn,_,_ = self.forward(xn)\n",
    "            # reset observed values\n",
    "            xn[~mask] = x0[~mask]\n",
    "        return xn\n",
    "    \n",
    "vae = VAE(hidden=512, latent=64, variational=False)\n",
    "print(vae)\n",
    "assert vae(torch.tensor(np.random.randn(20, 189).astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    sample, mask = Xtrain[:]\n",
    "    reconstruction = sample[:]\n",
    "    reconstruction[mask] = torch.randn(mask.shape)[mask]\n",
    "    for n in range(10):\n",
    "        vae.train()\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, log_var = vae(reconstruction)\n",
    "        mse = F.mse_loss(reconstruction, sample, reduction=\"mean\")\n",
    "        kl = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = mse + 0 * kl  # set to 1 for variational regularization (centered gaussian)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Train Epoch {epoch}.{n}\", \n",
    "              f\"var loss {loss.item()}\", \n",
    "              f\"reconstruction mse {mse.item()}\",\n",
    "              f\"imputation mse {F.mse_loss(reconstruction[mask], sample[mask], reduction='mean')}\")\n",
    "        reconstruction = reconstruction.detach() # drop previous computation graph before going into next iter\n",
    "    \n",
    "def test():\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        sample, mask = Xtest[:]\n",
    "        reconstruction = vae.gibbs(sample, mask)   \n",
    "        test_loss = F.mse_loss(reconstruction[mask], sample[mask], reduction=\"mean\").item()\n",
    "    print('====> Test imputation mse: {:.8f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test imputation mse: 1.01946390\n",
      "====> Test imputation mse: 1.01727521\n",
      "====> Test imputation mse: 1.00465620\n",
      "Train Epoch 1.0 var loss 6.3818159103393555 reconstruction mse 6.3818159103393555 imputation mse 1.1431632041931152\n",
      "Train Epoch 1.1 var loss 6.330203533172607 reconstruction mse 6.330203533172607 imputation mse 1.1528018712997437\n",
      "Train Epoch 1.2 var loss 6.226535320281982 reconstruction mse 6.226535320281982 imputation mse 1.1786030530929565\n",
      "Train Epoch 1.3 var loss 6.111794948577881 reconstruction mse 6.111794948577881 imputation mse 1.214613914489746\n",
      "Train Epoch 1.4 var loss 6.003492832183838 reconstruction mse 6.003492832183838 imputation mse 1.2472885847091675\n",
      "Train Epoch 1.5 var loss 5.942350387573242 reconstruction mse 5.942350387573242 imputation mse 1.2768481969833374\n",
      "Train Epoch 1.6 var loss 5.953159809112549 reconstruction mse 5.953159809112549 imputation mse 1.2974590063095093\n",
      "Train Epoch 1.7 var loss 6.0398125648498535 reconstruction mse 6.0398125648498535 imputation mse 1.3067443370819092\n",
      "Train Epoch 1.8 var loss 6.121847629547119 reconstruction mse 6.121847629547119 imputation mse 1.3499032258987427\n",
      "Train Epoch 1.9 var loss 6.126466751098633 reconstruction mse 6.126466751098633 imputation mse 1.39347243309021\n",
      "Train Epoch 2.0 var loss 4.9999823570251465 reconstruction mse 4.9999823570251465 imputation mse 1.1350347995758057\n",
      "Train Epoch 2.1 var loss 5.403014659881592 reconstruction mse 5.403014659881592 imputation mse 1.1808059215545654\n",
      "Train Epoch 2.2 var loss 5.621696472167969 reconstruction mse 5.621696472167969 imputation mse 1.1732122898101807\n",
      "Train Epoch 2.3 var loss 5.6739678382873535 reconstruction mse 5.6739678382873535 imputation mse 1.2361700534820557\n",
      "Train Epoch 2.4 var loss 5.615025043487549 reconstruction mse 5.615025043487549 imputation mse 1.277368187904358\n",
      "Train Epoch 2.5 var loss 5.534485340118408 reconstruction mse 5.534485340118408 imputation mse 1.3178050518035889\n",
      "Train Epoch 2.6 var loss 5.515666961669922 reconstruction mse 5.515666961669922 imputation mse 1.330404281616211\n",
      "Train Epoch 2.7 var loss 5.593625068664551 reconstruction mse 5.593625068664551 imputation mse 1.3284715414047241\n",
      "Train Epoch 2.8 var loss 5.739469528198242 reconstruction mse 5.739469528198242 imputation mse 1.3148044347763062\n",
      "Train Epoch 2.9 var loss 5.886325836181641 reconstruction mse 5.886325836181641 imputation mse 1.2686344385147095\n",
      "Train Epoch 3.0 var loss 4.61113977432251 reconstruction mse 4.61113977432251 imputation mse 1.0610032081604004\n",
      "Train Epoch 3.1 var loss 5.052647590637207 reconstruction mse 5.052647590637207 imputation mse 1.0943195819854736\n",
      "Train Epoch 3.2 var loss 5.243844985961914 reconstruction mse 5.243844985961914 imputation mse 1.1255766153335571\n",
      "Train Epoch 3.3 var loss 5.308938980102539 reconstruction mse 5.308938980102539 imputation mse 1.1428600549697876\n",
      "Train Epoch 3.4 var loss 5.328283786773682 reconstruction mse 5.328283786773682 imputation mse 1.158136248588562\n",
      "Train Epoch 3.5 var loss 5.323554515838623 reconstruction mse 5.323554515838623 imputation mse 1.187758445739746\n",
      "Train Epoch 3.6 var loss 5.285766124725342 reconstruction mse 5.285766124725342 imputation mse 1.236639142036438\n",
      "Train Epoch 3.7 var loss 5.21172571182251 reconstruction mse 5.21172571182251 imputation mse 1.2914822101593018\n",
      "Train Epoch 3.8 var loss 5.121552467346191 reconstruction mse 5.121552467346191 imputation mse 1.3381725549697876\n",
      "Train Epoch 3.9 var loss 5.066656112670898 reconstruction mse 5.066656112670898 imputation mse 1.3766635656356812\n",
      "Train Epoch 4.0 var loss 4.371985912322998 reconstruction mse 4.371985912322998 imputation mse 1.1189920902252197\n",
      "Train Epoch 4.1 var loss 4.9346022605896 reconstruction mse 4.9346022605896 imputation mse 1.2167067527770996\n",
      "Train Epoch 4.2 var loss 5.458178520202637 reconstruction mse 5.458178520202637 imputation mse 1.3474628925323486\n",
      "Train Epoch 4.3 var loss 5.879698276519775 reconstruction mse 5.879698276519775 imputation mse 1.4851412773132324\n",
      "Train Epoch 4.4 var loss 6.058226108551025 reconstruction mse 6.058226108551025 imputation mse 1.5717170238494873\n",
      "Train Epoch 4.5 var loss 5.995477199554443 reconstruction mse 5.995477199554443 imputation mse 1.5705852508544922\n",
      "Train Epoch 4.6 var loss 5.77628231048584 reconstruction mse 5.77628231048584 imputation mse 1.4978891611099243\n",
      "Train Epoch 4.7 var loss 5.496405124664307 reconstruction mse 5.496405124664307 imputation mse 1.389006495475769\n",
      "Train Epoch 4.8 var loss 5.262477397918701 reconstruction mse 5.262477397918701 imputation mse 1.280965805053711\n",
      "Train Epoch 4.9 var loss 5.190009117126465 reconstruction mse 5.190009117126465 imputation mse 1.2184480428695679\n",
      "Train Epoch 5.0 var loss 4.50632905960083 reconstruction mse 4.50632905960083 imputation mse 1.0673463344573975\n",
      "Train Epoch 5.1 var loss 5.227902889251709 reconstruction mse 5.227902889251709 imputation mse 1.1320768594741821\n",
      "Train Epoch 5.2 var loss 5.687877655029297 reconstruction mse 5.687877655029297 imputation mse 1.209179162979126\n",
      "Train Epoch 5.3 var loss 5.739406585693359 reconstruction mse 5.739406585693359 imputation mse 1.3097443580627441\n",
      "Train Epoch 5.4 var loss 5.582474231719971 reconstruction mse 5.582474231719971 imputation mse 1.3392869234085083\n",
      "Train Epoch 5.5 var loss 5.446739196777344 reconstruction mse 5.446739196777344 imputation mse 1.3171507120132446\n",
      "Train Epoch 5.6 var loss 5.393472194671631 reconstruction mse 5.393472194671631 imputation mse 1.306146502494812\n",
      "Train Epoch 5.7 var loss 5.310532093048096 reconstruction mse 5.310532093048096 imputation mse 1.30353581905365\n",
      "Train Epoch 5.8 var loss 5.2409820556640625 reconstruction mse 5.2409820556640625 imputation mse 1.2908308506011963\n",
      "Train Epoch 5.9 var loss 5.289614677429199 reconstruction mse 5.289614677429199 imputation mse 1.2925074100494385\n",
      "Train Epoch 6.0 var loss 4.879326820373535 reconstruction mse 4.879326820373535 imputation mse 1.114822506904602\n",
      "Train Epoch 6.1 var loss 5.252758979797363 reconstruction mse 5.252758979797363 imputation mse 1.27200186252594\n",
      "Train Epoch 6.2 var loss 5.423244476318359 reconstruction mse 5.423244476318359 imputation mse 1.2950791120529175\n",
      "Train Epoch 6.3 var loss 5.520299434661865 reconstruction mse 5.520299434661865 imputation mse 1.2302342653274536\n",
      "Train Epoch 6.4 var loss 5.412148952484131 reconstruction mse 5.412148952484131 imputation mse 1.1668479442596436\n",
      "Train Epoch 6.5 var loss 5.313198566436768 reconstruction mse 5.313198566436768 imputation mse 1.1550847291946411\n",
      "Train Epoch 6.6 var loss 5.246194362640381 reconstruction mse 5.246194362640381 imputation mse 1.164732575416565\n",
      "Train Epoch 6.7 var loss 5.214980602264404 reconstruction mse 5.214980602264404 imputation mse 1.1626583337783813\n",
      "Train Epoch 6.8 var loss 5.186520099639893 reconstruction mse 5.186520099639893 imputation mse 1.1489453315734863\n",
      "Train Epoch 6.9 var loss 5.168928146362305 reconstruction mse 5.168928146362305 imputation mse 1.14297616481781\n",
      "Train Epoch 7.0 var loss 4.804782390594482 reconstruction mse 4.804782390594482 imputation mse 1.158549189567566\n",
      "Train Epoch 7.1 var loss 4.854928970336914 reconstruction mse 4.854928970336914 imputation mse 1.1191045045852661\n",
      "Train Epoch 7.2 var loss 4.988907814025879 reconstruction mse 4.988907814025879 imputation mse 1.157970666885376\n",
      "Train Epoch 7.3 var loss 4.915857315063477 reconstruction mse 4.915857315063477 imputation mse 1.1393743753433228\n",
      "Train Epoch 7.4 var loss 4.949680328369141 reconstruction mse 4.949680328369141 imputation mse 1.1624196767807007\n",
      "Train Epoch 7.5 var loss 4.890460968017578 reconstruction mse 4.890460968017578 imputation mse 1.1248620748519897\n",
      "Train Epoch 7.6 var loss 4.9130635261535645 reconstruction mse 4.9130635261535645 imputation mse 1.1293221712112427\n",
      "Train Epoch 7.7 var loss 4.873894214630127 reconstruction mse 4.873894214630127 imputation mse 1.1054317951202393\n",
      "Train Epoch 7.8 var loss 4.888603210449219 reconstruction mse 4.888603210449219 imputation mse 1.1123579740524292\n",
      "Train Epoch 7.9 var loss 4.862049579620361 reconstruction mse 4.862049579620361 imputation mse 1.0973691940307617\n",
      "Train Epoch 8.0 var loss 4.480774879455566 reconstruction mse 4.480774879455566 imputation mse 1.0342650413513184\n",
      "Train Epoch 8.1 var loss 4.567102909088135 reconstruction mse 4.567102909088135 imputation mse 1.0350005626678467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 8.2 var loss 4.7006120681762695 reconstruction mse 4.7006120681762695 imputation mse 1.043976068496704\n",
      "Train Epoch 8.3 var loss 4.7307047843933105 reconstruction mse 4.7307047843933105 imputation mse 1.048693299293518\n",
      "Train Epoch 8.4 var loss 4.770837306976318 reconstruction mse 4.770837306976318 imputation mse 1.059625267982483\n",
      "Train Epoch 8.5 var loss 4.782680511474609 reconstruction mse 4.782680511474609 imputation mse 1.0709208250045776\n",
      "Train Epoch 8.6 var loss 4.779810428619385 reconstruction mse 4.779810428619385 imputation mse 1.080039381980896\n",
      "Train Epoch 8.7 var loss 4.763060569763184 reconstruction mse 4.763060569763184 imputation mse 1.0831209421157837\n",
      "Train Epoch 8.8 var loss 4.74363374710083 reconstruction mse 4.74363374710083 imputation mse 1.0828630924224854\n",
      "Train Epoch 8.9 var loss 4.7332892417907715 reconstruction mse 4.7332892417907715 imputation mse 1.088200330734253\n",
      "Train Epoch 9.0 var loss 4.222461223602295 reconstruction mse 4.222461223602295 imputation mse 1.020870566368103\n",
      "Train Epoch 9.1 var loss 4.444196701049805 reconstruction mse 4.444196701049805 imputation mse 1.0538790225982666\n",
      "Train Epoch 9.2 var loss 4.647917747497559 reconstruction mse 4.647917747497559 imputation mse 1.0962462425231934\n",
      "Train Epoch 9.3 var loss 4.779030799865723 reconstruction mse 4.779030799865723 imputation mse 1.1220850944519043\n",
      "Train Epoch 9.4 var loss 4.842953205108643 reconstruction mse 4.842953205108643 imputation mse 1.132533073425293\n",
      "Train Epoch 9.5 var loss 4.8409271240234375 reconstruction mse 4.8409271240234375 imputation mse 1.1392953395843506\n",
      "Train Epoch 9.6 var loss 4.8015031814575195 reconstruction mse 4.8015031814575195 imputation mse 1.1512744426727295\n",
      "Train Epoch 9.7 var loss 4.775759220123291 reconstruction mse 4.775759220123291 imputation mse 1.1670151948928833\n",
      "Train Epoch 9.8 var loss 4.803390026092529 reconstruction mse 4.803390026092529 imputation mse 1.171264410018921\n",
      "Train Epoch 9.9 var loss 4.892444610595703 reconstruction mse 4.892444610595703 imputation mse 1.1563024520874023\n",
      "Train Epoch 10.0 var loss 4.02311897277832 reconstruction mse 4.02311897277832 imputation mse 1.0410292148590088\n",
      "Train Epoch 10.1 var loss 4.361352920532227 reconstruction mse 4.361352920532227 imputation mse 1.0978821516036987\n",
      "Train Epoch 10.2 var loss 4.59763240814209 reconstruction mse 4.59763240814209 imputation mse 1.1457229852676392\n",
      "Train Epoch 10.3 var loss 4.7480268478393555 reconstruction mse 4.7480268478393555 imputation mse 1.1920254230499268\n",
      "Train Epoch 10.4 var loss 4.8063859939575195 reconstruction mse 4.8063859939575195 imputation mse 1.227096676826477\n",
      "Train Epoch 10.5 var loss 4.782566547393799 reconstruction mse 4.782566547393799 imputation mse 1.2338943481445312\n",
      "Train Epoch 10.6 var loss 4.723602771759033 reconstruction mse 4.723602771759033 imputation mse 1.2266778945922852\n",
      "Train Epoch 10.7 var loss 4.68501615524292 reconstruction mse 4.68501615524292 imputation mse 1.2312995195388794\n",
      "Train Epoch 10.8 var loss 4.698797702789307 reconstruction mse 4.698797702789307 imputation mse 1.250455379486084\n",
      "Train Epoch 10.9 var loss 4.782005310058594 reconstruction mse 4.782005310058594 imputation mse 1.276304841041565\n",
      "====> Test imputation mse: 2.87454820\n",
      "====> Test imputation mse: 2.65343499\n",
      "====> Test imputation mse: 2.84436584\n",
      "Train Epoch 11.0 var loss 3.936075210571289 reconstruction mse 3.936075210571289 imputation mse 1.132697343826294\n",
      "Train Epoch 11.1 var loss 4.442646503448486 reconstruction mse 4.442646503448486 imputation mse 1.1572811603546143\n",
      "Train Epoch 11.2 var loss 4.745110988616943 reconstruction mse 4.745110988616943 imputation mse 1.1404609680175781\n",
      "Train Epoch 11.3 var loss 4.903504848480225 reconstruction mse 4.903504848480225 imputation mse 1.1371854543685913\n",
      "Train Epoch 11.4 var loss 4.929557800292969 reconstruction mse 4.929557800292969 imputation mse 1.1538618803024292\n",
      "Train Epoch 11.5 var loss 4.852743625640869 reconstruction mse 4.852743625640869 imputation mse 1.1644212007522583\n",
      "Train Epoch 11.6 var loss 4.723646640777588 reconstruction mse 4.723646640777588 imputation mse 1.1479891538619995\n",
      "Train Epoch 11.7 var loss 4.640258312225342 reconstruction mse 4.640258312225342 imputation mse 1.1376428604125977\n",
      "Train Epoch 11.8 var loss 4.764110565185547 reconstruction mse 4.764110565185547 imputation mse 1.220697045326233\n",
      "Train Epoch 11.9 var loss 5.240104675292969 reconstruction mse 5.240104675292969 imputation mse 1.4471546411514282\n",
      "Train Epoch 12.0 var loss 4.081243515014648 reconstruction mse 4.081243515014648 imputation mse 1.0794216394424438\n",
      "Train Epoch 12.1 var loss 4.844988822937012 reconstruction mse 4.844988822937012 imputation mse 1.1414680480957031\n",
      "Train Epoch 12.2 var loss 5.318607330322266 reconstruction mse 5.318607330322266 imputation mse 1.222152590751648\n",
      "Train Epoch 12.3 var loss 5.380120277404785 reconstruction mse 5.380120277404785 imputation mse 1.2900224924087524\n",
      "Train Epoch 12.4 var loss 5.252009391784668 reconstruction mse 5.252009391784668 imputation mse 1.3082835674285889\n",
      "Train Epoch 12.5 var loss 5.088517189025879 reconstruction mse 5.088517189025879 imputation mse 1.2833267450332642\n",
      "Train Epoch 12.6 var loss 4.975691795349121 reconstruction mse 4.975691795349121 imputation mse 1.2613309621810913\n",
      "Train Epoch 12.7 var loss 4.95155668258667 reconstruction mse 4.95155668258667 imputation mse 1.2656573057174683\n",
      "Train Epoch 12.8 var loss 5.06047248840332 reconstruction mse 5.06047248840332 imputation mse 1.2951416969299316\n",
      "Train Epoch 12.9 var loss 5.272087574005127 reconstruction mse 5.272087574005127 imputation mse 1.3484714031219482\n",
      "Train Epoch 13.0 var loss 4.588943958282471 reconstruction mse 4.588943958282471 imputation mse 1.1302409172058105\n",
      "Train Epoch 13.1 var loss 5.078444480895996 reconstruction mse 5.078444480895996 imputation mse 1.1924678087234497\n",
      "Train Epoch 13.2 var loss 5.0561370849609375 reconstruction mse 5.0561370849609375 imputation mse 1.1275352239608765\n",
      "Train Epoch 13.3 var loss 5.01472806930542 reconstruction mse 5.01472806930542 imputation mse 1.11123526096344\n",
      "Train Epoch 13.4 var loss 4.9621968269348145 reconstruction mse 4.9621968269348145 imputation mse 1.1352168321609497\n",
      "Train Epoch 13.5 var loss 4.9391889572143555 reconstruction mse 4.9391889572143555 imputation mse 1.1209429502487183\n",
      "Train Epoch 13.6 var loss 4.911637783050537 reconstruction mse 4.911637783050537 imputation mse 1.1312683820724487\n",
      "Train Epoch 13.7 var loss 4.88522481918335 reconstruction mse 4.88522481918335 imputation mse 1.1114917993545532\n",
      "Train Epoch 13.8 var loss 4.868000507354736 reconstruction mse 4.868000507354736 imputation mse 1.1204427480697632\n",
      "Train Epoch 13.9 var loss 4.84401798248291 reconstruction mse 4.84401798248291 imputation mse 1.1047900915145874\n",
      "Train Epoch 14.0 var loss 4.448946952819824 reconstruction mse 4.448946952819824 imputation mse 1.0868468284606934\n",
      "Train Epoch 14.1 var loss 4.5312299728393555 reconstruction mse 4.5312299728393555 imputation mse 1.0969834327697754\n",
      "Train Epoch 14.2 var loss 4.716299057006836 reconstruction mse 4.716299057006836 imputation mse 1.0794538259506226\n",
      "Train Epoch 14.3 var loss 4.734742641448975 reconstruction mse 4.734742641448975 imputation mse 1.074613332748413\n",
      "Train Epoch 14.4 var loss 4.786532402038574 reconstruction mse 4.786532402038574 imputation mse 1.0627379417419434\n",
      "Train Epoch 14.5 var loss 4.7950825691223145 reconstruction mse 4.7950825691223145 imputation mse 1.0680142641067505\n",
      "Train Epoch 14.6 var loss 4.786435604095459 reconstruction mse 4.786435604095459 imputation mse 1.0941227674484253\n",
      "Train Epoch 14.7 var loss 4.765615940093994 reconstruction mse 4.765615940093994 imputation mse 1.1349084377288818\n",
      "Train Epoch 14.8 var loss 4.753173828125 reconstruction mse 4.753173828125 imputation mse 1.1799590587615967\n",
      "Train Epoch 14.9 var loss 4.7573652267456055 reconstruction mse 4.7573652267456055 imputation mse 1.2256213426589966\n",
      "Train Epoch 15.0 var loss 4.0102152824401855 reconstruction mse 4.0102152824401855 imputation mse 1.071804404258728\n",
      "Train Epoch 15.1 var loss 4.298689842224121 reconstruction mse 4.298689842224121 imputation mse 1.107954978942871\n",
      "Train Epoch 15.2 var loss 4.4687347412109375 reconstruction mse 4.4687347412109375 imputation mse 1.1287524700164795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 15.3 var loss 4.593225479125977 reconstruction mse 4.593225479125977 imputation mse 1.1455354690551758\n",
      "Train Epoch 15.4 var loss 4.663137435913086 reconstruction mse 4.663137435913086 imputation mse 1.155791997909546\n",
      "Train Epoch 15.5 var loss 4.673597812652588 reconstruction mse 4.673597812652588 imputation mse 1.1595126390457153\n",
      "Train Epoch 15.6 var loss 4.643106937408447 reconstruction mse 4.643106937408447 imputation mse 1.1639668941497803\n",
      "Train Epoch 15.7 var loss 4.602234840393066 reconstruction mse 4.602234840393066 imputation mse 1.1765488386154175\n",
      "Train Epoch 15.8 var loss 4.575121879577637 reconstruction mse 4.575121879577637 imputation mse 1.2016748189926147\n",
      "Train Epoch 15.9 var loss 4.574089050292969 reconstruction mse 4.574089050292969 imputation mse 1.239866018295288\n",
      "Train Epoch 16.0 var loss 3.860809803009033 reconstruction mse 3.860809803009033 imputation mse 1.0378267765045166\n",
      "Train Epoch 16.1 var loss 4.18718147277832 reconstruction mse 4.18718147277832 imputation mse 1.064371109008789\n",
      "Train Epoch 16.2 var loss 4.369011402130127 reconstruction mse 4.369011402130127 imputation mse 1.096754789352417\n",
      "Train Epoch 16.3 var loss 4.48779296875 reconstruction mse 4.48779296875 imputation mse 1.1243327856063843\n",
      "Train Epoch 16.4 var loss 4.548973560333252 reconstruction mse 4.548973560333252 imputation mse 1.130083680152893\n",
      "Train Epoch 16.5 var loss 4.560617923736572 reconstruction mse 4.560617923736572 imputation mse 1.114880919456482\n",
      "Train Epoch 16.6 var loss 4.530814170837402 reconstruction mse 4.530814170837402 imputation mse 1.0941365957260132\n",
      "Train Epoch 16.7 var loss 4.4703145027160645 reconstruction mse 4.4703145027160645 imputation mse 1.080757975578308\n",
      "Train Epoch 16.8 var loss 4.407547950744629 reconstruction mse 4.407547950744629 imputation mse 1.0828584432601929\n",
      "Train Epoch 16.9 var loss 4.402317523956299 reconstruction mse 4.402317523956299 imputation mse 1.1163212060928345\n",
      "Train Epoch 17.0 var loss 3.70570707321167 reconstruction mse 3.70570707321167 imputation mse 1.098848819732666\n",
      "Train Epoch 17.1 var loss 4.173306465148926 reconstruction mse 4.173306465148926 imputation mse 1.1757532358169556\n",
      "Train Epoch 17.2 var loss 4.548579216003418 reconstruction mse 4.548579216003418 imputation mse 1.261647343635559\n",
      "Train Epoch 17.3 var loss 4.772801876068115 reconstruction mse 4.772801876068115 imputation mse 1.340652346611023\n",
      "Train Epoch 17.4 var loss 4.836977005004883 reconstruction mse 4.836977005004883 imputation mse 1.3848196268081665\n",
      "Train Epoch 17.5 var loss 4.801434516906738 reconstruction mse 4.801434516906738 imputation mse 1.378082275390625\n",
      "Train Epoch 17.6 var loss 4.725244522094727 reconstruction mse 4.725244522094727 imputation mse 1.3314898014068604\n",
      "Train Epoch 17.7 var loss 4.6484904289245605 reconstruction mse 4.6484904289245605 imputation mse 1.2751336097717285\n",
      "Train Epoch 17.8 var loss 4.590057373046875 reconstruction mse 4.590057373046875 imputation mse 1.230605125427246\n",
      "Train Epoch 17.9 var loss 4.561257839202881 reconstruction mse 4.561257839202881 imputation mse 1.2045103311538696\n",
      "Train Epoch 18.0 var loss 3.8092994689941406 reconstruction mse 3.8092994689941406 imputation mse 1.0647863149642944\n",
      "Train Epoch 18.1 var loss 4.128414154052734 reconstruction mse 4.128414154052734 imputation mse 1.1111013889312744\n",
      "Train Epoch 18.2 var loss 4.328275203704834 reconstruction mse 4.328275203704834 imputation mse 1.1164923906326294\n",
      "Train Epoch 18.3 var loss 4.421830177307129 reconstruction mse 4.421830177307129 imputation mse 1.1230354309082031\n",
      "Train Epoch 18.4 var loss 4.446580410003662 reconstruction mse 4.446580410003662 imputation mse 1.1336548328399658\n",
      "Train Epoch 18.5 var loss 4.435028076171875 reconstruction mse 4.435028076171875 imputation mse 1.1429779529571533\n",
      "Train Epoch 18.6 var loss 4.400993824005127 reconstruction mse 4.400993824005127 imputation mse 1.143003225326538\n",
      "Train Epoch 18.7 var loss 4.355391502380371 reconstruction mse 4.355391502380371 imputation mse 1.1360584497451782\n",
      "Train Epoch 18.8 var loss 4.320669174194336 reconstruction mse 4.320669174194336 imputation mse 1.1294879913330078\n",
      "Train Epoch 18.9 var loss 4.321913719177246 reconstruction mse 4.321913719177246 imputation mse 1.1319056749343872\n",
      "Train Epoch 19.0 var loss 3.823899745941162 reconstruction mse 3.823899745941162 imputation mse 1.1462243795394897\n",
      "Train Epoch 19.1 var loss 3.948624610900879 reconstruction mse 3.948624610900879 imputation mse 1.1314960718154907\n",
      "Train Epoch 19.2 var loss 4.143675327301025 reconstruction mse 4.143675327301025 imputation mse 1.169909119606018\n",
      "Train Epoch 19.3 var loss 4.235806941986084 reconstruction mse 4.235806941986084 imputation mse 1.1872848272323608\n",
      "Train Epoch 19.4 var loss 4.356103897094727 reconstruction mse 4.356103897094727 imputation mse 1.233906626701355\n",
      "Train Epoch 19.5 var loss 4.392711162567139 reconstruction mse 4.392711162567139 imputation mse 1.2700259685516357\n",
      "Train Epoch 19.6 var loss 4.393531322479248 reconstruction mse 4.393531322479248 imputation mse 1.3001762628555298\n",
      "Train Epoch 19.7 var loss 4.353480339050293 reconstruction mse 4.353480339050293 imputation mse 1.312207818031311\n",
      "Train Epoch 19.8 var loss 4.3016839027404785 reconstruction mse 4.3016839027404785 imputation mse 1.3099251985549927\n",
      "Train Epoch 19.9 var loss 4.2594804763793945 reconstruction mse 4.2594804763793945 imputation mse 1.294714331626892\n",
      "Train Epoch 20.0 var loss 3.696096420288086 reconstruction mse 3.696096420288086 imputation mse 1.0965261459350586\n",
      "Train Epoch 20.1 var loss 4.0958356857299805 reconstruction mse 4.0958356857299805 imputation mse 1.2049732208251953\n",
      "Train Epoch 20.2 var loss 4.326563835144043 reconstruction mse 4.326563835144043 imputation mse 1.2367855310440063\n",
      "Train Epoch 20.3 var loss 4.375370502471924 reconstruction mse 4.375370502471924 imputation mse 1.2100777626037598\n",
      "Train Epoch 20.4 var loss 4.321010589599609 reconstruction mse 4.321010589599609 imputation mse 1.1700756549835205\n",
      "Train Epoch 20.5 var loss 4.2525529861450195 reconstruction mse 4.2525529861450195 imputation mse 1.1537117958068848\n",
      "Train Epoch 20.6 var loss 4.2287516593933105 reconstruction mse 4.2287516593933105 imputation mse 1.1602493524551392\n",
      "Train Epoch 20.7 var loss 4.258656978607178 reconstruction mse 4.258656978607178 imputation mse 1.177219271659851\n",
      "Train Epoch 20.8 var loss 4.308183670043945 reconstruction mse 4.308183670043945 imputation mse 1.1910383701324463\n",
      "Train Epoch 20.9 var loss 4.3386030197143555 reconstruction mse 4.3386030197143555 imputation mse 1.1969027519226074\n",
      "====> Test imputation mse: 1.65655327\n",
      "====> Test imputation mse: 1.55633819\n",
      "====> Test imputation mse: 1.57373726\n",
      "Train Epoch 21.0 var loss 3.5136687755584717 reconstruction mse 3.5136687755584717 imputation mse 1.047698974609375\n",
      "Train Epoch 21.1 var loss 3.7272422313690186 reconstruction mse 3.7272422313690186 imputation mse 1.0818042755126953\n",
      "Train Epoch 21.2 var loss 3.762531280517578 reconstruction mse 3.762531280517578 imputation mse 1.1003001928329468\n",
      "Train Epoch 21.3 var loss 3.7673516273498535 reconstruction mse 3.7673516273498535 imputation mse 1.1090261936187744\n",
      "Train Epoch 21.4 var loss 3.7658376693725586 reconstruction mse 3.7658376693725586 imputation mse 1.1076059341430664\n",
      "Train Epoch 21.5 var loss 3.759012222290039 reconstruction mse 3.759012222290039 imputation mse 1.0980401039123535\n",
      "Train Epoch 21.6 var loss 3.756340742111206 reconstruction mse 3.756340742111206 imputation mse 1.0857973098754883\n",
      "Train Epoch 21.7 var loss 3.7668333053588867 reconstruction mse 3.7668333053588867 imputation mse 1.0778605937957764\n",
      "Train Epoch 21.8 var loss 3.7949116230010986 reconstruction mse 3.7949116230010986 imputation mse 1.0787920951843262\n",
      "Train Epoch 21.9 var loss 3.8389554023742676 reconstruction mse 3.8389554023742676 imputation mse 1.0891199111938477\n",
      "Train Epoch 22.0 var loss 3.2610762119293213 reconstruction mse 3.2610762119293213 imputation mse 1.0095354318618774\n",
      "Train Epoch 22.1 var loss 3.4686570167541504 reconstruction mse 3.4686570167541504 imputation mse 1.0255801677703857\n",
      "Train Epoch 22.2 var loss 3.539044141769409 reconstruction mse 3.539044141769409 imputation mse 1.0330792665481567\n",
      "Train Epoch 22.3 var loss 3.575770139694214 reconstruction mse 3.575770139694214 imputation mse 1.0418728590011597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 22.4 var loss 3.5913138389587402 reconstruction mse 3.5913138389587402 imputation mse 1.052688717842102\n",
      "Train Epoch 22.5 var loss 3.5952632427215576 reconstruction mse 3.5952632427215576 imputation mse 1.064509391784668\n",
      "Train Epoch 22.6 var loss 3.602904796600342 reconstruction mse 3.602904796600342 imputation mse 1.0760841369628906\n",
      "Train Epoch 22.7 var loss 3.6301651000976562 reconstruction mse 3.6301651000976562 imputation mse 1.0861073732376099\n",
      "Train Epoch 22.8 var loss 3.6871206760406494 reconstruction mse 3.6871206760406494 imputation mse 1.0941162109375\n",
      "Train Epoch 22.9 var loss 3.7684314250946045 reconstruction mse 3.7684314250946045 imputation mse 1.1004258394241333\n",
      "Train Epoch 23.0 var loss 3.1018178462982178 reconstruction mse 3.1018178462982178 imputation mse 1.0070816278457642\n",
      "Train Epoch 23.1 var loss 3.345503807067871 reconstruction mse 3.345503807067871 imputation mse 1.0268869400024414\n",
      "Train Epoch 23.2 var loss 3.4664556980133057 reconstruction mse 3.4664556980133057 imputation mse 1.0317232608795166\n",
      "Train Epoch 23.3 var loss 3.5469167232513428 reconstruction mse 3.5469167232513428 imputation mse 1.0344445705413818\n",
      "Train Epoch 23.4 var loss 3.595069646835327 reconstruction mse 3.595069646835327 imputation mse 1.0363749265670776\n",
      "Train Epoch 23.5 var loss 3.619000196456909 reconstruction mse 3.619000196456909 imputation mse 1.0365946292877197\n",
      "Train Epoch 23.6 var loss 3.634115219116211 reconstruction mse 3.634115219116211 imputation mse 1.0346434116363525\n",
      "Train Epoch 23.7 var loss 3.6592907905578613 reconstruction mse 3.6592907905578613 imputation mse 1.0324195623397827\n",
      "Train Epoch 23.8 var loss 3.7091450691223145 reconstruction mse 3.7091450691223145 imputation mse 1.035221815109253\n",
      "Train Epoch 23.9 var loss 3.7862229347229004 reconstruction mse 3.7862229347229004 imputation mse 1.047572374343872\n",
      "Train Epoch 24.0 var loss 3.0614449977874756 reconstruction mse 3.0614449977874756 imputation mse 1.009971022605896\n",
      "Train Epoch 24.1 var loss 3.399311065673828 reconstruction mse 3.399311065673828 imputation mse 1.0533140897750854\n",
      "Train Epoch 24.2 var loss 3.632479190826416 reconstruction mse 3.632479190826416 imputation mse 1.1037451028823853\n",
      "Train Epoch 24.3 var loss 3.813715696334839 reconstruction mse 3.813715696334839 imputation mse 1.1774166822433472\n",
      "Train Epoch 24.4 var loss 3.936154842376709 reconstruction mse 3.936154842376709 imputation mse 1.2696878910064697\n",
      "Train Epoch 24.5 var loss 4.007565975189209 reconstruction mse 4.007565975189209 imputation mse 1.3618190288543701\n",
      "Train Epoch 24.6 var loss 4.035223960876465 reconstruction mse 4.035223960876465 imputation mse 1.4231926202774048\n",
      "Train Epoch 24.7 var loss 4.020510673522949 reconstruction mse 4.020510673522949 imputation mse 1.433775544166565\n",
      "Train Epoch 24.8 var loss 3.970796585083008 reconstruction mse 3.970796585083008 imputation mse 1.3974050283432007\n",
      "Train Epoch 24.9 var loss 3.9008493423461914 reconstruction mse 3.9008493423461914 imputation mse 1.333917260169983\n",
      "Train Epoch 25.0 var loss 3.0541129112243652 reconstruction mse 3.0541129112243652 imputation mse 1.0277146100997925\n",
      "Train Epoch 25.1 var loss 3.4420952796936035 reconstruction mse 3.4420952796936035 imputation mse 1.0817358493804932\n",
      "Train Epoch 25.2 var loss 3.8087775707244873 reconstruction mse 3.8087775707244873 imputation mse 1.165122389793396\n",
      "Train Epoch 25.3 var loss 4.121532440185547 reconstruction mse 4.121532440185547 imputation mse 1.2474875450134277\n",
      "Train Epoch 25.4 var loss 4.297444820404053 reconstruction mse 4.297444820404053 imputation mse 1.2730907201766968\n",
      "Train Epoch 25.5 var loss 4.352890491485596 reconstruction mse 4.352890491485596 imputation mse 1.2484251260757446\n",
      "Train Epoch 25.6 var loss 4.321498870849609 reconstruction mse 4.321498870849609 imputation mse 1.203530192375183\n",
      "Train Epoch 25.7 var loss 4.239621162414551 reconstruction mse 4.239621162414551 imputation mse 1.1604045629501343\n",
      "Train Epoch 25.8 var loss 4.145620822906494 reconstruction mse 4.145620822906494 imputation mse 1.1273365020751953\n",
      "Train Epoch 25.9 var loss 4.071490287780762 reconstruction mse 4.071490287780762 imputation mse 1.1041077375411987\n",
      "Train Epoch 26.0 var loss 3.221038579940796 reconstruction mse 3.221038579940796 imputation mse 1.0139392614364624\n",
      "Train Epoch 26.1 var loss 3.6536402702331543 reconstruction mse 3.6536402702331543 imputation mse 1.0318968296051025\n",
      "Train Epoch 26.2 var loss 3.9690778255462646 reconstruction mse 3.9690778255462646 imputation mse 1.04067862033844\n",
      "Train Epoch 26.3 var loss 4.194913864135742 reconstruction mse 4.194913864135742 imputation mse 1.0441335439682007\n",
      "Train Epoch 26.4 var loss 4.305877685546875 reconstruction mse 4.305877685546875 imputation mse 1.0423023700714111\n",
      "Train Epoch 26.5 var loss 4.316073417663574 reconstruction mse 4.316073417663574 imputation mse 1.038570761680603\n",
      "Train Epoch 26.6 var loss 4.270254611968994 reconstruction mse 4.270254611968994 imputation mse 1.0382378101348877\n",
      "Train Epoch 26.7 var loss 4.203012466430664 reconstruction mse 4.203012466430664 imputation mse 1.0427751541137695\n",
      "Train Epoch 26.8 var loss 4.141554832458496 reconstruction mse 4.141554832458496 imputation mse 1.0518207550048828\n",
      "Train Epoch 26.9 var loss 4.116664409637451 reconstruction mse 4.116664409637451 imputation mse 1.064588189125061\n",
      "Train Epoch 27.0 var loss 3.3550167083740234 reconstruction mse 3.3550167083740234 imputation mse 1.0379717350006104\n",
      "Train Epoch 27.1 var loss 3.7934651374816895 reconstruction mse 3.7934651374816895 imputation mse 1.0742237567901611\n",
      "Train Epoch 27.2 var loss 4.090756893157959 reconstruction mse 4.090756893157959 imputation mse 1.0982989072799683\n",
      "Train Epoch 27.3 var loss 4.260684013366699 reconstruction mse 4.260684013366699 imputation mse 1.1204102039337158\n",
      "Train Epoch 27.4 var loss 4.304624080657959 reconstruction mse 4.304624080657959 imputation mse 1.139449954032898\n",
      "Train Epoch 27.5 var loss 4.275576114654541 reconstruction mse 4.275576114654541 imputation mse 1.1469266414642334\n",
      "Train Epoch 27.6 var loss 4.22368860244751 reconstruction mse 4.22368860244751 imputation mse 1.1446174383163452\n",
      "Train Epoch 27.7 var loss 4.171724319458008 reconstruction mse 4.171724319458008 imputation mse 1.1401994228363037\n",
      "Train Epoch 27.8 var loss 4.129372596740723 reconstruction mse 4.129372596740723 imputation mse 1.138127326965332\n",
      "Train Epoch 27.9 var loss 4.100040435791016 reconstruction mse 4.100040435791016 imputation mse 1.1416521072387695\n",
      "Train Epoch 28.0 var loss 3.43127179145813 reconstruction mse 3.43127179145813 imputation mse 1.0710121393203735\n",
      "Train Epoch 28.1 var loss 3.7113900184631348 reconstruction mse 3.7113900184631348 imputation mse 1.0941516160964966\n",
      "Train Epoch 28.2 var loss 3.8171422481536865 reconstruction mse 3.8171422481536865 imputation mse 1.1056458950042725\n",
      "Train Epoch 28.3 var loss 3.859896421432495 reconstruction mse 3.859896421432495 imputation mse 1.112566590309143\n",
      "Train Epoch 28.4 var loss 3.8682701587677 reconstruction mse 3.8682701587677 imputation mse 1.1177337169647217\n",
      "Train Epoch 28.5 var loss 3.8519158363342285 reconstruction mse 3.8519158363342285 imputation mse 1.1203441619873047\n",
      "Train Epoch 28.6 var loss 3.825050115585327 reconstruction mse 3.825050115585327 imputation mse 1.1180757284164429\n",
      "Train Epoch 28.7 var loss 3.8000271320343018 reconstruction mse 3.8000271320343018 imputation mse 1.1133235692977905\n",
      "Train Epoch 28.8 var loss 3.781381130218506 reconstruction mse 3.781381130218506 imputation mse 1.1092462539672852\n",
      "Train Epoch 28.9 var loss 3.7692253589630127 reconstruction mse 3.7692253589630127 imputation mse 1.1065552234649658\n",
      "Train Epoch 29.0 var loss 3.2089121341705322 reconstruction mse 3.2089121341705322 imputation mse 1.0540307760238647\n",
      "Train Epoch 29.1 var loss 3.4485549926757812 reconstruction mse 3.4485549926757812 imputation mse 1.0669118165969849\n",
      "Train Epoch 29.2 var loss 3.524482488632202 reconstruction mse 3.524482488632202 imputation mse 1.0692768096923828\n",
      "Train Epoch 29.3 var loss 3.555985450744629 reconstruction mse 3.555985450744629 imputation mse 1.069319725036621\n",
      "Train Epoch 29.4 var loss 3.5710649490356445 reconstruction mse 3.5710649490356445 imputation mse 1.0681229829788208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 29.5 var loss 3.5769715309143066 reconstruction mse 3.5769715309143066 imputation mse 1.0659540891647339\n",
      "Train Epoch 29.6 var loss 3.577104091644287 reconstruction mse 3.577104091644287 imputation mse 1.062965989112854\n",
      "Train Epoch 29.7 var loss 3.574521541595459 reconstruction mse 3.574521541595459 imputation mse 1.0596139430999756\n",
      "Train Epoch 29.8 var loss 3.5720760822296143 reconstruction mse 3.5720760822296143 imputation mse 1.0562784671783447\n",
      "Train Epoch 29.9 var loss 3.5728402137756348 reconstruction mse 3.5728402137756348 imputation mse 1.053379774093628\n",
      "Train Epoch 30.0 var loss 2.986536979675293 reconstruction mse 2.986536979675293 imputation mse 1.0157467126846313\n",
      "Train Epoch 30.1 var loss 3.2497336864471436 reconstruction mse 3.2497336864471436 imputation mse 1.0274782180786133\n",
      "Train Epoch 30.2 var loss 3.3458855152130127 reconstruction mse 3.3458855152130127 imputation mse 1.0305675268173218\n",
      "Train Epoch 30.3 var loss 3.400271415710449 reconstruction mse 3.400271415710449 imputation mse 1.0343674421310425\n",
      "Train Epoch 30.4 var loss 3.433922290802002 reconstruction mse 3.433922290802002 imputation mse 1.0393520593643188\n",
      "Train Epoch 30.5 var loss 3.4492454528808594 reconstruction mse 3.4492454528808594 imputation mse 1.0448355674743652\n",
      "Train Epoch 30.6 var loss 3.4484760761260986 reconstruction mse 3.4484760761260986 imputation mse 1.050447940826416\n",
      "Train Epoch 30.7 var loss 3.4383046627044678 reconstruction mse 3.4383046627044678 imputation mse 1.0556589365005493\n",
      "Train Epoch 30.8 var loss 3.4299633502960205 reconstruction mse 3.4299633502960205 imputation mse 1.060196876525879\n",
      "Train Epoch 30.9 var loss 3.4366886615753174 reconstruction mse 3.4366886615753174 imputation mse 1.0641297101974487\n",
      "====> Test imputation mse: 1.39167595\n",
      "====> Test imputation mse: 1.37452483\n",
      "====> Test imputation mse: 1.26577151\n",
      "Train Epoch 31.0 var loss 2.8682148456573486 reconstruction mse 2.8682148456573486 imputation mse 0.9936882257461548\n",
      "Train Epoch 31.1 var loss 3.1582729816436768 reconstruction mse 3.1582729816436768 imputation mse 1.0108957290649414\n",
      "Train Epoch 31.2 var loss 3.320699453353882 reconstruction mse 3.320699453353882 imputation mse 1.0149891376495361\n",
      "Train Epoch 31.3 var loss 3.4402871131896973 reconstruction mse 3.4402871131896973 imputation mse 1.016844630241394\n",
      "Train Epoch 31.4 var loss 3.5117979049682617 reconstruction mse 3.5117979049682617 imputation mse 1.017004370689392\n",
      "Train Epoch 31.5 var loss 3.5274198055267334 reconstruction mse 3.5274198055267334 imputation mse 1.0148019790649414\n",
      "Train Epoch 31.6 var loss 3.501509666442871 reconstruction mse 3.501509666442871 imputation mse 1.0108827352523804\n",
      "Train Epoch 31.7 var loss 3.4690732955932617 reconstruction mse 3.4690732955932617 imputation mse 1.0070818662643433\n",
      "Train Epoch 31.8 var loss 3.4669456481933594 reconstruction mse 3.4669456481933594 imputation mse 1.0066438913345337\n",
      "Train Epoch 31.9 var loss 3.518944025039673 reconstruction mse 3.518944025039673 imputation mse 1.013425588607788\n",
      "Train Epoch 32.0 var loss 2.8647658824920654 reconstruction mse 2.8647658824920654 imputation mse 0.9790617823600769\n",
      "Train Epoch 32.1 var loss 3.285888433456421 reconstruction mse 3.285888433456421 imputation mse 1.0141595602035522\n",
      "Train Epoch 32.2 var loss 3.61269211769104 reconstruction mse 3.61269211769104 imputation mse 1.0615085363388062\n",
      "Train Epoch 32.3 var loss 3.851288318634033 reconstruction mse 3.851288318634033 imputation mse 1.1289016008377075\n",
      "Train Epoch 32.4 var loss 3.964298963546753 reconstruction mse 3.964298963546753 imputation mse 1.1967666149139404\n",
      "Train Epoch 32.5 var loss 3.9671597480773926 reconstruction mse 3.9671597480773926 imputation mse 1.2400696277618408\n",
      "Train Epoch 32.6 var loss 3.917872905731201 reconstruction mse 3.917872905731201 imputation mse 1.2494443655014038\n",
      "Train Epoch 32.7 var loss 3.882307291030884 reconstruction mse 3.882307291030884 imputation mse 1.233777642250061\n",
      "Train Epoch 32.8 var loss 3.912249803543091 reconstruction mse 3.912249803543091 imputation mse 1.2078001499176025\n",
      "Train Epoch 32.9 var loss 4.012885093688965 reconstruction mse 4.012885093688965 imputation mse 1.1825252771377563\n",
      "Train Epoch 33.0 var loss 3.00178861618042 reconstruction mse 3.00178861618042 imputation mse 1.0277959108352661\n",
      "Train Epoch 33.1 var loss 3.442976236343384 reconstruction mse 3.442976236343384 imputation mse 1.0542120933532715\n",
      "Train Epoch 33.2 var loss 3.750929594039917 reconstruction mse 3.750929594039917 imputation mse 1.0741037130355835\n",
      "Train Epoch 33.3 var loss 3.943784475326538 reconstruction mse 3.943784475326538 imputation mse 1.0963534116744995\n",
      "Train Epoch 33.4 var loss 4.020040988922119 reconstruction mse 4.020040988922119 imputation mse 1.1186991930007935\n",
      "Train Epoch 33.5 var loss 3.9984688758850098 reconstruction mse 3.9984688758850098 imputation mse 1.1330963373184204\n",
      "Train Epoch 33.6 var loss 3.921624183654785 reconstruction mse 3.921624183654785 imputation mse 1.1332341432571411\n",
      "Train Epoch 33.7 var loss 3.826441764831543 reconstruction mse 3.826441764831543 imputation mse 1.121839165687561\n",
      "Train Epoch 33.8 var loss 3.7402806282043457 reconstruction mse 3.7402806282043457 imputation mse 1.108329176902771\n",
      "Train Epoch 33.9 var loss 3.685582399368286 reconstruction mse 3.685582399368286 imputation mse 1.100317358970642\n",
      "Train Epoch 34.0 var loss 3.008317470550537 reconstruction mse 3.008317470550537 imputation mse 1.0375131368637085\n",
      "Train Epoch 34.1 var loss 3.31418514251709 reconstruction mse 3.31418514251709 imputation mse 1.0726439952850342\n",
      "Train Epoch 34.2 var loss 3.549126386642456 reconstruction mse 3.549126386642456 imputation mse 1.1040571928024292\n",
      "Train Epoch 34.3 var loss 3.7570157051086426 reconstruction mse 3.7570157051086426 imputation mse 1.1473798751831055\n",
      "Train Epoch 34.4 var loss 3.9172441959381104 reconstruction mse 3.9172441959381104 imputation mse 1.1990612745285034\n",
      "Train Epoch 34.5 var loss 4.017239093780518 reconstruction mse 4.017239093780518 imputation mse 1.2443714141845703\n",
      "Train Epoch 34.6 var loss 4.056389808654785 reconstruction mse 4.056389808654785 imputation mse 1.2733869552612305\n",
      "Train Epoch 34.7 var loss 4.047043323516846 reconstruction mse 4.047043323516846 imputation mse 1.2819530963897705\n",
      "Train Epoch 34.8 var loss 4.008363246917725 reconstruction mse 4.008363246917725 imputation mse 1.2737951278686523\n",
      "Train Epoch 34.9 var loss 3.9585962295532227 reconstruction mse 3.9585962295532227 imputation mse 1.25642991065979\n",
      "Train Epoch 35.0 var loss 2.909088611602783 reconstruction mse 2.909088611602783 imputation mse 1.0322563648223877\n",
      "Train Epoch 35.1 var loss 3.1683480739593506 reconstruction mse 3.1683480739593506 imputation mse 1.0569206476211548\n",
      "Train Epoch 35.2 var loss 3.3365018367767334 reconstruction mse 3.3365018367767334 imputation mse 1.068418025970459\n",
      "Train Epoch 35.3 var loss 3.4692418575286865 reconstruction mse 3.4692418575286865 imputation mse 1.07232666015625\n",
      "Train Epoch 35.4 var loss 3.565934896469116 reconstruction mse 3.565934896469116 imputation mse 1.0695371627807617\n",
      "Train Epoch 35.5 var loss 3.616696357727051 reconstruction mse 3.616696357727051 imputation mse 1.060569167137146\n",
      "Train Epoch 35.6 var loss 3.6210150718688965 reconstruction mse 3.6210150718688965 imputation mse 1.048911690711975\n",
      "Train Epoch 35.7 var loss 3.592679738998413 reconstruction mse 3.592679738998413 imputation mse 1.0404391288757324\n",
      "Train Epoch 35.8 var loss 3.555015802383423 reconstruction mse 3.555015802383423 imputation mse 1.0383795499801636\n",
      "Train Epoch 35.9 var loss 3.526823043823242 reconstruction mse 3.526823043823242 imputation mse 1.0428489446640015\n",
      "Train Epoch 36.0 var loss 2.820909023284912 reconstruction mse 2.820909023284912 imputation mse 1.029799222946167\n",
      "Train Epoch 36.1 var loss 3.13863468170166 reconstruction mse 3.13863468170166 imputation mse 1.0591381788253784\n",
      "Train Epoch 36.2 var loss 3.3613123893737793 reconstruction mse 3.3613123893737793 imputation mse 1.0879380702972412\n",
      "Train Epoch 36.3 var loss 3.533513069152832 reconstruction mse 3.533513069152832 imputation mse 1.1215541362762451\n",
      "Train Epoch 36.4 var loss 3.6491031646728516 reconstruction mse 3.6491031646728516 imputation mse 1.148547649383545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 36.5 var loss 3.69392991065979 reconstruction mse 3.69392991065979 imputation mse 1.1589802503585815\n",
      "Train Epoch 36.6 var loss 3.6847846508026123 reconstruction mse 3.6847846508026123 imputation mse 1.160784125328064\n",
      "Train Epoch 36.7 var loss 3.6510260105133057 reconstruction mse 3.6510260105133057 imputation mse 1.1612516641616821\n",
      "Train Epoch 36.8 var loss 3.6143438816070557 reconstruction mse 3.6143438816070557 imputation mse 1.1644487380981445\n",
      "Train Epoch 36.9 var loss 3.586603879928589 reconstruction mse 3.586603879928589 imputation mse 1.1719541549682617\n",
      "Train Epoch 37.0 var loss 2.7282893657684326 reconstruction mse 2.7282893657684326 imputation mse 1.0337860584259033\n",
      "Train Epoch 37.1 var loss 2.994641065597534 reconstruction mse 2.994641065597534 imputation mse 1.0645363330841064\n",
      "Train Epoch 37.2 var loss 3.1456458568573 reconstruction mse 3.1456458568573 imputation mse 1.0802165269851685\n",
      "Train Epoch 37.3 var loss 3.2410848140716553 reconstruction mse 3.2410848140716553 imputation mse 1.0926238298416138\n",
      "Train Epoch 37.4 var loss 3.299978017807007 reconstruction mse 3.299978017807007 imputation mse 1.1070250272750854\n",
      "Train Epoch 37.5 var loss 3.3295164108276367 reconstruction mse 3.3295164108276367 imputation mse 1.121667742729187\n",
      "Train Epoch 37.6 var loss 3.3366856575012207 reconstruction mse 3.3366856575012207 imputation mse 1.1325290203094482\n",
      "Train Epoch 37.7 var loss 3.332185745239258 reconstruction mse 3.332185745239258 imputation mse 1.137555480003357\n",
      "Train Epoch 37.8 var loss 3.3268072605133057 reconstruction mse 3.3268072605133057 imputation mse 1.1375548839569092\n",
      "Train Epoch 37.9 var loss 3.328486919403076 reconstruction mse 3.328486919403076 imputation mse 1.1343344449996948\n",
      "Train Epoch 38.0 var loss 2.659745693206787 reconstruction mse 2.659745693206787 imputation mse 1.0434677600860596\n",
      "Train Epoch 38.1 var loss 2.9015085697174072 reconstruction mse 2.9015085697174072 imputation mse 1.0572175979614258\n",
      "Train Epoch 38.2 var loss 3.035276174545288 reconstruction mse 3.035276174545288 imputation mse 1.0580110549926758\n",
      "Train Epoch 38.3 var loss 3.118054151535034 reconstruction mse 3.118054151535034 imputation mse 1.0560797452926636\n",
      "Train Epoch 38.4 var loss 3.1673667430877686 reconstruction mse 3.1673667430877686 imputation mse 1.0539714097976685\n",
      "Train Epoch 38.5 var loss 3.194347620010376 reconstruction mse 3.194347620010376 imputation mse 1.0526416301727295\n",
      "Train Epoch 38.6 var loss 3.2067904472351074 reconstruction mse 3.2067904472351074 imputation mse 1.0523210763931274\n",
      "Train Epoch 38.7 var loss 3.211099863052368 reconstruction mse 3.211099863052368 imputation mse 1.0528593063354492\n",
      "Train Epoch 38.8 var loss 3.2122373580932617 reconstruction mse 3.2122373580932617 imputation mse 1.0544302463531494\n",
      "Train Epoch 38.9 var loss 3.2121050357818604 reconstruction mse 3.2121050357818604 imputation mse 1.0573333501815796\n",
      "Train Epoch 39.0 var loss 2.5219898223876953 reconstruction mse 2.5219898223876953 imputation mse 0.9903124570846558\n",
      "Train Epoch 39.1 var loss 2.753298044204712 reconstruction mse 2.753298044204712 imputation mse 1.0051538944244385\n",
      "Train Epoch 39.2 var loss 2.875272512435913 reconstruction mse 2.875272512435913 imputation mse 1.0096511840820312\n",
      "Train Epoch 39.3 var loss 2.94758677482605 reconstruction mse 2.94758677482605 imputation mse 1.012671709060669\n",
      "Train Epoch 39.4 var loss 2.9852817058563232 reconstruction mse 2.9852817058563232 imputation mse 1.0145384073257446\n",
      "Train Epoch 39.5 var loss 2.9973318576812744 reconstruction mse 2.9973318576812744 imputation mse 1.0150129795074463\n",
      "Train Epoch 39.6 var loss 2.9935858249664307 reconstruction mse 2.9935858249664307 imputation mse 1.0145143270492554\n",
      "Train Epoch 39.7 var loss 2.9854531288146973 reconstruction mse 2.9854531288146973 imputation mse 1.0142019987106323\n",
      "Train Epoch 39.8 var loss 2.9812252521514893 reconstruction mse 2.9812252521514893 imputation mse 1.0148476362228394\n",
      "Train Epoch 39.9 var loss 2.983227252960205 reconstruction mse 2.983227252960205 imputation mse 1.0163055658340454\n",
      "Train Epoch 40.0 var loss 2.405852794647217 reconstruction mse 2.405852794647217 imputation mse 0.9868776202201843\n",
      "Train Epoch 40.1 var loss 2.6151554584503174 reconstruction mse 2.6151554584503174 imputation mse 1.004338026046753\n",
      "Train Epoch 40.2 var loss 2.7310540676116943 reconstruction mse 2.7310540676116943 imputation mse 1.0113989114761353\n",
      "Train Epoch 40.3 var loss 2.808767795562744 reconstruction mse 2.808767795562744 imputation mse 1.0160905122756958\n",
      "Train Epoch 40.4 var loss 2.8590505123138428 reconstruction mse 2.8590505123138428 imputation mse 1.0196809768676758\n",
      "Train Epoch 40.5 var loss 2.8860535621643066 reconstruction mse 2.8860535621643066 imputation mse 1.021845817565918\n",
      "Train Epoch 40.6 var loss 2.8938841819763184 reconstruction mse 2.8938841819763184 imputation mse 1.0222753286361694\n",
      "Train Epoch 40.7 var loss 2.8886537551879883 reconstruction mse 2.8886537551879883 imputation mse 1.0211267471313477\n",
      "Train Epoch 40.8 var loss 2.8778364658355713 reconstruction mse 2.8778364658355713 imputation mse 1.01921808719635\n",
      "Train Epoch 40.9 var loss 2.8701281547546387 reconstruction mse 2.8701281547546387 imputation mse 1.017641305923462\n",
      "====> Test imputation mse: 2.10624170\n",
      "====> Test imputation mse: 2.00167060\n",
      "====> Test imputation mse: 2.07596874\n",
      "Train Epoch 41.0 var loss 2.315922260284424 reconstruction mse 2.315922260284424 imputation mse 0.9428967237472534\n",
      "Train Epoch 41.1 var loss 2.543701648712158 reconstruction mse 2.543701648712158 imputation mse 0.9567185640335083\n",
      "Train Epoch 41.2 var loss 2.704111337661743 reconstruction mse 2.704111337661743 imputation mse 0.9639396071434021\n",
      "Train Epoch 41.3 var loss 2.8399722576141357 reconstruction mse 2.8399722576141357 imputation mse 0.9706212282180786\n",
      "Train Epoch 41.4 var loss 2.948126792907715 reconstruction mse 2.948126792907715 imputation mse 0.9766802191734314\n",
      "Train Epoch 41.5 var loss 3.0183820724487305 reconstruction mse 3.0183820724487305 imputation mse 0.9819810390472412\n",
      "Train Epoch 41.6 var loss 3.048710346221924 reconstruction mse 3.048710346221924 imputation mse 0.9862423539161682\n",
      "Train Epoch 41.7 var loss 3.0482394695281982 reconstruction mse 3.0482394695281982 imputation mse 0.9890119433403015\n",
      "Train Epoch 41.8 var loss 3.0316052436828613 reconstruction mse 3.0316052436828613 imputation mse 0.9904139041900635\n",
      "Train Epoch 41.9 var loss 3.0131442546844482 reconstruction mse 3.0131442546844482 imputation mse 0.9910721182823181\n",
      "Train Epoch 42.0 var loss 2.2958390712738037 reconstruction mse 2.2958390712738037 imputation mse 0.9799025654792786\n",
      "Train Epoch 42.1 var loss 2.5738983154296875 reconstruction mse 2.5738983154296875 imputation mse 1.0013178586959839\n",
      "Train Epoch 42.2 var loss 2.7876715660095215 reconstruction mse 2.7876715660095215 imputation mse 1.0234739780426025\n",
      "Train Epoch 42.3 var loss 2.971855878829956 reconstruction mse 2.971855878829956 imputation mse 1.0484265089035034\n",
      "Train Epoch 42.4 var loss 3.1075732707977295 reconstruction mse 3.1075732707977295 imputation mse 1.0698598623275757\n",
      "Train Epoch 42.5 var loss 3.1823642253875732 reconstruction mse 3.1823642253875732 imputation mse 1.0828111171722412\n",
      "Train Epoch 42.6 var loss 3.2039337158203125 reconstruction mse 3.2039337158203125 imputation mse 1.0857093334197998\n",
      "Train Epoch 42.7 var loss 3.1892001628875732 reconstruction mse 3.1892001628875732 imputation mse 1.0803492069244385\n",
      "Train Epoch 42.8 var loss 3.1567060947418213 reconstruction mse 3.1567060947418213 imputation mse 1.0711783170700073\n",
      "Train Epoch 42.9 var loss 3.120547294616699 reconstruction mse 3.120547294616699 imputation mse 1.0617122650146484\n",
      "Train Epoch 43.0 var loss 2.3005313873291016 reconstruction mse 2.3005313873291016 imputation mse 0.9585109949111938\n",
      "Train Epoch 43.1 var loss 2.5242297649383545 reconstruction mse 2.5242297649383545 imputation mse 0.9733536839485168\n",
      "Train Epoch 43.2 var loss 2.6594109535217285 reconstruction mse 2.6594109535217285 imputation mse 0.9812270402908325\n",
      "Train Epoch 43.3 var loss 2.7553529739379883 reconstruction mse 2.7553529739379883 imputation mse 0.9881606101989746\n",
      "Train Epoch 43.4 var loss 2.8245983123779297 reconstruction mse 2.8245983123779297 imputation mse 0.9945563673973083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 43.5 var loss 2.8711740970611572 reconstruction mse 2.8711740970611572 imputation mse 0.9998350143432617\n",
      "Train Epoch 43.6 var loss 2.896728277206421 reconstruction mse 2.896728277206421 imputation mse 1.003167748451233\n",
      "Train Epoch 43.7 var loss 2.903547763824463 reconstruction mse 2.903547763824463 imputation mse 1.0045933723449707\n",
      "Train Epoch 43.8 var loss 2.897083282470703 reconstruction mse 2.897083282470703 imputation mse 1.0044139623641968\n",
      "Train Epoch 43.9 var loss 2.8856210708618164 reconstruction mse 2.8856210708618164 imputation mse 1.0033488273620605\n",
      "Train Epoch 44.0 var loss 2.2676408290863037 reconstruction mse 2.2676408290863037 imputation mse 0.983416736125946\n",
      "Train Epoch 44.1 var loss 2.506145715713501 reconstruction mse 2.506145715713501 imputation mse 0.9954383969306946\n",
      "Train Epoch 44.2 var loss 2.665285110473633 reconstruction mse 2.665285110473633 imputation mse 1.0010173320770264\n",
      "Train Epoch 44.3 var loss 2.7905972003936768 reconstruction mse 2.7905972003936768 imputation mse 1.0059293508529663\n",
      "Train Epoch 44.4 var loss 2.883260488510132 reconstruction mse 2.883260488510132 imputation mse 1.0112866163253784\n",
      "Train Epoch 44.5 var loss 2.9362363815307617 reconstruction mse 2.9362363815307617 imputation mse 1.0165982246398926\n",
      "Train Epoch 44.6 var loss 2.9493680000305176 reconstruction mse 2.9493680000305176 imputation mse 1.0205117464065552\n",
      "Train Epoch 44.7 var loss 2.932929039001465 reconstruction mse 2.932929039001465 imputation mse 1.022659420967102\n",
      "Train Epoch 44.8 var loss 2.9049367904663086 reconstruction mse 2.9049367904663086 imputation mse 1.023504614830017\n",
      "Train Epoch 44.9 var loss 2.882081985473633 reconstruction mse 2.882081985473633 imputation mse 1.0236763954162598\n",
      "Train Epoch 45.0 var loss 2.230557680130005 reconstruction mse 2.230557680130005 imputation mse 0.9725218415260315\n",
      "Train Epoch 45.1 var loss 2.526953935623169 reconstruction mse 2.526953935623169 imputation mse 0.9904109835624695\n",
      "Train Epoch 45.2 var loss 2.7576143741607666 reconstruction mse 2.7576143741607666 imputation mse 1.00239098072052\n",
      "Train Epoch 45.3 var loss 2.9481239318847656 reconstruction mse 2.9481239318847656 imputation mse 1.0135741233825684\n",
      "Train Epoch 45.4 var loss 3.091998338699341 reconstruction mse 3.091998338699341 imputation mse 1.0246963500976562\n",
      "Train Epoch 45.5 var loss 3.175227165222168 reconstruction mse 3.175227165222168 imputation mse 1.035368800163269\n",
      "Train Epoch 45.6 var loss 3.1938369274139404 reconstruction mse 3.1938369274139404 imputation mse 1.0434861183166504\n",
      "Train Epoch 45.7 var loss 3.1681878566741943 reconstruction mse 3.1681878566741943 imputation mse 1.0475701093673706\n",
      "Train Epoch 45.8 var loss 3.126375913619995 reconstruction mse 3.126375913619995 imputation mse 1.047659993171692\n",
      "Train Epoch 45.9 var loss 3.0873844623565674 reconstruction mse 3.0873844623565674 imputation mse 1.045129656791687\n",
      "Train Epoch 46.0 var loss 2.247447967529297 reconstruction mse 2.247447967529297 imputation mse 0.992753803730011\n",
      "Train Epoch 46.1 var loss 2.5371694564819336 reconstruction mse 2.5371694564819336 imputation mse 1.0100587606430054\n",
      "Train Epoch 46.2 var loss 2.7375426292419434 reconstruction mse 2.7375426292419434 imputation mse 1.0191835165023804\n",
      "Train Epoch 46.3 var loss 2.8818819522857666 reconstruction mse 2.8818819522857666 imputation mse 1.0254969596862793\n",
      "Train Epoch 46.4 var loss 2.975328207015991 reconstruction mse 2.975328207015991 imputation mse 1.0295825004577637\n",
      "Train Epoch 46.5 var loss 3.0203263759613037 reconstruction mse 3.0203263759613037 imputation mse 1.0310910940170288\n",
      "Train Epoch 46.6 var loss 3.0284087657928467 reconstruction mse 3.0284087657928467 imputation mse 1.0304673910140991\n",
      "Train Epoch 46.7 var loss 3.0168347358703613 reconstruction mse 3.0168347358703613 imputation mse 1.0289885997772217\n",
      "Train Epoch 46.8 var loss 2.9985995292663574 reconstruction mse 2.9985995292663574 imputation mse 1.0277012586593628\n",
      "Train Epoch 46.9 var loss 2.9828591346740723 reconstruction mse 2.9828591346740723 imputation mse 1.027279019355774\n",
      "Train Epoch 47.0 var loss 2.2754294872283936 reconstruction mse 2.2754294872283936 imputation mse 1.0027074813842773\n",
      "Train Epoch 47.1 var loss 2.531710624694824 reconstruction mse 2.531710624694824 imputation mse 1.0191611051559448\n",
      "Train Epoch 47.2 var loss 2.6888060569763184 reconstruction mse 2.6888060569763184 imputation mse 1.02903413772583\n",
      "Train Epoch 47.3 var loss 2.7961950302124023 reconstruction mse 2.7961950302124023 imputation mse 1.0387071371078491\n",
      "Train Epoch 47.4 var loss 2.8698067665100098 reconstruction mse 2.8698067665100098 imputation mse 1.048744797706604\n",
      "Train Epoch 47.5 var loss 2.914562702178955 reconstruction mse 2.914562702178955 imputation mse 1.057446837425232\n",
      "Train Epoch 47.6 var loss 2.933365821838379 reconstruction mse 2.933365821838379 imputation mse 1.0623891353607178\n",
      "Train Epoch 47.7 var loss 2.9293324947357178 reconstruction mse 2.9293324947357178 imputation mse 1.0627137422561646\n",
      "Train Epoch 47.8 var loss 2.9080138206481934 reconstruction mse 2.9080138206481934 imputation mse 1.0596630573272705\n",
      "Train Epoch 47.9 var loss 2.8789050579071045 reconstruction mse 2.8789050579071045 imputation mse 1.0557688474655151\n",
      "Train Epoch 48.0 var loss 2.199004888534546 reconstruction mse 2.199004888534546 imputation mse 0.9597670435905457\n",
      "Train Epoch 48.1 var loss 2.436391592025757 reconstruction mse 2.436391592025757 imputation mse 0.9786170721054077\n",
      "Train Epoch 48.2 var loss 2.598064422607422 reconstruction mse 2.598064422607422 imputation mse 0.9861490726470947\n",
      "Train Epoch 48.3 var loss 2.732658863067627 reconstruction mse 2.732658863067627 imputation mse 0.9891268014907837\n",
      "Train Epoch 48.4 var loss 2.8473117351531982 reconstruction mse 2.8473117351531982 imputation mse 0.9896302819252014\n",
      "Train Epoch 48.5 var loss 2.9351491928100586 reconstruction mse 2.9351491928100586 imputation mse 0.9894459843635559\n",
      "Train Epoch 48.6 var loss 2.9897148609161377 reconstruction mse 2.9897148609161377 imputation mse 0.9893221855163574\n",
      "Train Epoch 48.7 var loss 3.0090160369873047 reconstruction mse 3.0090160369873047 imputation mse 0.9895067811012268\n",
      "Train Epoch 48.8 var loss 2.995849370956421 reconstruction mse 2.995849370956421 imputation mse 0.9905673265457153\n",
      "Train Epoch 48.9 var loss 2.9580700397491455 reconstruction mse 2.9580700397491455 imputation mse 0.9930383563041687\n",
      "Train Epoch 49.0 var loss 2.1400187015533447 reconstruction mse 2.1400187015533447 imputation mse 0.9765808582305908\n",
      "Train Epoch 49.1 var loss 2.390591859817505 reconstruction mse 2.390591859817505 imputation mse 0.9957106709480286\n",
      "Train Epoch 49.2 var loss 2.5668792724609375 reconstruction mse 2.5668792724609375 imputation mse 1.0120869874954224\n",
      "Train Epoch 49.3 var loss 2.711115837097168 reconstruction mse 2.711115837097168 imputation mse 1.034315824508667\n",
      "Train Epoch 49.4 var loss 2.8242502212524414 reconstruction mse 2.8242502212524414 imputation mse 1.0640783309936523\n",
      "Train Epoch 49.5 var loss 2.8996803760528564 reconstruction mse 2.8996803760528564 imputation mse 1.097557544708252\n",
      "Train Epoch 49.6 var loss 2.935344696044922 reconstruction mse 2.935344696044922 imputation mse 1.1255218982696533\n",
      "Train Epoch 49.7 var loss 2.936675786972046 reconstruction mse 2.936675786972046 imputation mse 1.140588402748108\n",
      "Train Epoch 49.8 var loss 2.91548490524292 reconstruction mse 2.91548490524292 imputation mse 1.1412209272384644\n",
      "Train Epoch 49.9 var loss 2.886115789413452 reconstruction mse 2.886115789413452 imputation mse 1.1300798654556274\n",
      "Train Epoch 50.0 var loss 2.0993239879608154 reconstruction mse 2.0993239879608154 imputation mse 0.9719703793525696\n",
      "Train Epoch 50.1 var loss 2.325676679611206 reconstruction mse 2.325676679611206 imputation mse 0.9917867183685303\n",
      "Train Epoch 50.2 var loss 2.480663776397705 reconstruction mse 2.480663776397705 imputation mse 1.0030531883239746\n",
      "Train Epoch 50.3 var loss 2.5984036922454834 reconstruction mse 2.5984036922454834 imputation mse 1.0124151706695557\n",
      "Train Epoch 50.4 var loss 2.686999559402466 reconstruction mse 2.686999559402466 imputation mse 1.0211669206619263\n",
      "Train Epoch 50.5 var loss 2.750223398208618 reconstruction mse 2.750223398208618 imputation mse 1.030038595199585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 50.6 var loss 2.7931361198425293 reconstruction mse 2.7931361198425293 imputation mse 1.0396511554718018\n",
      "Train Epoch 50.7 var loss 2.8209426403045654 reconstruction mse 2.8209426403045654 imputation mse 1.0493696928024292\n",
      "Train Epoch 50.8 var loss 2.8377418518066406 reconstruction mse 2.8377418518066406 imputation mse 1.0584149360656738\n",
      "Train Epoch 50.9 var loss 2.8464088439941406 reconstruction mse 2.8464088439941406 imputation mse 1.0668350458145142\n",
      "====> Test imputation mse: 3.28914976\n",
      "====> Test imputation mse: 2.92469835\n",
      "====> Test imputation mse: 2.84176493\n",
      "Train Epoch 51.0 var loss 2.0476293563842773 reconstruction mse 2.0476293563842773 imputation mse 0.9540315866470337\n",
      "Train Epoch 51.1 var loss 2.2482149600982666 reconstruction mse 2.2482149600982666 imputation mse 0.9690876007080078\n",
      "Train Epoch 51.2 var loss 2.3681461811065674 reconstruction mse 2.3681461811065674 imputation mse 0.9783523082733154\n",
      "Train Epoch 51.3 var loss 2.4498233795166016 reconstruction mse 2.4498233795166016 imputation mse 0.9866489171981812\n",
      "Train Epoch 51.4 var loss 2.5081946849823 reconstruction mse 2.5081946849823 imputation mse 0.9951436519622803\n",
      "Train Epoch 51.5 var loss 2.550269365310669 reconstruction mse 2.550269365310669 imputation mse 1.0051027536392212\n",
      "Train Epoch 51.6 var loss 2.5805370807647705 reconstruction mse 2.5805370807647705 imputation mse 1.0182805061340332\n",
      "Train Epoch 51.7 var loss 2.6024117469787598 reconstruction mse 2.6024117469787598 imputation mse 1.0347025394439697\n",
      "Train Epoch 51.8 var loss 2.618284225463867 reconstruction mse 2.618284225463867 imputation mse 1.0527182817459106\n",
      "Train Epoch 51.9 var loss 2.6295270919799805 reconstruction mse 2.6295270919799805 imputation mse 1.0698058605194092\n",
      "Train Epoch 52.0 var loss 1.972534418106079 reconstruction mse 1.972534418106079 imputation mse 0.9478272199630737\n",
      "Train Epoch 52.1 var loss 2.1611294746398926 reconstruction mse 2.1611294746398926 imputation mse 0.96317058801651\n",
      "Train Epoch 52.2 var loss 2.2826175689697266 reconstruction mse 2.2826175689697266 imputation mse 0.9713752865791321\n",
      "Train Epoch 52.3 var loss 2.3677265644073486 reconstruction mse 2.3677265644073486 imputation mse 0.9770219326019287\n",
      "Train Epoch 52.4 var loss 2.42549729347229 reconstruction mse 2.42549729347229 imputation mse 0.980567991733551\n",
      "Train Epoch 52.5 var loss 2.4611988067626953 reconstruction mse 2.4611988067626953 imputation mse 0.9827728271484375\n",
      "Train Epoch 52.6 var loss 2.4800219535827637 reconstruction mse 2.4800219535827637 imputation mse 0.9844558238983154\n",
      "Train Epoch 52.7 var loss 2.48781156539917 reconstruction mse 2.48781156539917 imputation mse 0.9864822626113892\n",
      "Train Epoch 52.8 var loss 2.49072265625 reconstruction mse 2.49072265625 imputation mse 0.9894824624061584\n",
      "Train Epoch 52.9 var loss 2.494323968887329 reconstruction mse 2.494323968887329 imputation mse 0.9935631155967712\n",
      "Train Epoch 53.0 var loss 1.9219900369644165 reconstruction mse 1.9219900369644165 imputation mse 0.9714085459709167\n",
      "Train Epoch 53.1 var loss 2.1121370792388916 reconstruction mse 2.1121370792388916 imputation mse 0.9905562400817871\n",
      "Train Epoch 53.2 var loss 2.2466201782226562 reconstruction mse 2.2466201782226562 imputation mse 1.0020123720169067\n",
      "Train Epoch 53.3 var loss 2.3535990715026855 reconstruction mse 2.3535990715026855 imputation mse 1.0121039152145386\n",
      "Train Epoch 53.4 var loss 2.43662428855896 reconstruction mse 2.43662428855896 imputation mse 1.0218040943145752\n",
      "Train Epoch 53.5 var loss 2.4953935146331787 reconstruction mse 2.4953935146331787 imputation mse 1.0324015617370605\n",
      "Train Epoch 53.6 var loss 2.530362129211426 reconstruction mse 2.530362129211426 imputation mse 1.0440109968185425\n",
      "Train Epoch 53.7 var loss 2.5442817211151123 reconstruction mse 2.5442817211151123 imputation mse 1.055063247680664\n",
      "Train Epoch 53.8 var loss 2.5417709350585938 reconstruction mse 2.5417709350585938 imputation mse 1.063631534576416\n",
      "Train Epoch 53.9 var loss 2.529334783554077 reconstruction mse 2.529334783554077 imputation mse 1.0684754848480225\n",
      "Train Epoch 54.0 var loss 1.8950786590576172 reconstruction mse 1.8950786590576172 imputation mse 0.9555929899215698\n",
      "Train Epoch 54.1 var loss 2.096419334411621 reconstruction mse 2.096419334411621 imputation mse 0.9741801023483276\n",
      "Train Epoch 54.2 var loss 2.2554471492767334 reconstruction mse 2.2554471492767334 imputation mse 0.9837108254432678\n",
      "Train Epoch 54.3 var loss 2.3972113132476807 reconstruction mse 2.3972113132476807 imputation mse 0.9905650615692139\n",
      "Train Epoch 54.4 var loss 2.5160562992095947 reconstruction mse 2.5160562992095947 imputation mse 0.9959697127342224\n",
      "Train Epoch 54.5 var loss 2.5999255180358887 reconstruction mse 2.5999255180358887 imputation mse 1.000524878501892\n",
      "Train Epoch 54.6 var loss 2.641794204711914 reconstruction mse 2.641794204711914 imputation mse 1.0045808553695679\n",
      "Train Epoch 54.7 var loss 2.6443021297454834 reconstruction mse 2.6443021297454834 imputation mse 1.0079265832901\n",
      "Train Epoch 54.8 var loss 2.6189422607421875 reconstruction mse 2.6189422607421875 imputation mse 1.0103435516357422\n",
      "Train Epoch 54.9 var loss 2.5834720134735107 reconstruction mse 2.5834720134735107 imputation mse 1.011664628982544\n",
      "Train Epoch 55.0 var loss 1.902417540550232 reconstruction mse 1.902417540550232 imputation mse 0.9566986560821533\n",
      "Train Epoch 55.1 var loss 2.111898183822632 reconstruction mse 2.111898183822632 imputation mse 0.9723435044288635\n",
      "Train Epoch 55.2 var loss 2.282036304473877 reconstruction mse 2.282036304473877 imputation mse 0.9804168343544006\n",
      "Train Epoch 55.3 var loss 2.4319610595703125 reconstruction mse 2.4319610595703125 imputation mse 0.98708176612854\n",
      "Train Epoch 55.4 var loss 2.556828260421753 reconstruction mse 2.556828260421753 imputation mse 0.9933780431747437\n",
      "Train Epoch 55.5 var loss 2.65095853805542 reconstruction mse 2.65095853805542 imputation mse 0.9991724491119385\n",
      "Train Epoch 55.6 var loss 2.708035945892334 reconstruction mse 2.708035945892334 imputation mse 1.0039794445037842\n",
      "Train Epoch 55.7 var loss 2.7289958000183105 reconstruction mse 2.7289958000183105 imputation mse 1.007962942123413\n",
      "Train Epoch 55.8 var loss 2.7232003211975098 reconstruction mse 2.7232003211975098 imputation mse 1.0111199617385864\n",
      "Train Epoch 55.9 var loss 2.7015910148620605 reconstruction mse 2.7015910148620605 imputation mse 1.0136977434158325\n",
      "Train Epoch 56.0 var loss 1.9026007652282715 reconstruction mse 1.9026007652282715 imputation mse 0.9488526582717896\n",
      "Train Epoch 56.1 var loss 2.080841541290283 reconstruction mse 2.080841541290283 imputation mse 0.9656341075897217\n",
      "Train Epoch 56.2 var loss 2.1930086612701416 reconstruction mse 2.1930086612701416 imputation mse 0.9733854532241821\n",
      "Train Epoch 56.3 var loss 2.276862621307373 reconstruction mse 2.276862621307373 imputation mse 0.9783036708831787\n",
      "Train Epoch 56.4 var loss 2.3439297676086426 reconstruction mse 2.3439297676086426 imputation mse 0.98188716173172\n",
      "Train Epoch 56.5 var loss 2.397725820541382 reconstruction mse 2.397725820541382 imputation mse 0.9851096868515015\n",
      "Train Epoch 56.6 var loss 2.438624620437622 reconstruction mse 2.438624620437622 imputation mse 0.9885942339897156\n",
      "Train Epoch 56.7 var loss 2.466778516769409 reconstruction mse 2.466778516769409 imputation mse 0.9925874471664429\n",
      "Train Epoch 56.8 var loss 2.483527421951294 reconstruction mse 2.483527421951294 imputation mse 0.9973594546318054\n",
      "Train Epoch 56.9 var loss 2.4913742542266846 reconstruction mse 2.4913742542266846 imputation mse 1.0027081966400146\n",
      "Train Epoch 57.0 var loss 1.8577160835266113 reconstruction mse 1.8577160835266113 imputation mse 0.9554089307785034\n",
      "Train Epoch 57.1 var loss 2.0173890590667725 reconstruction mse 2.0173890590667725 imputation mse 0.9737585783004761\n",
      "Train Epoch 57.2 var loss 2.1188461780548096 reconstruction mse 2.1188461780548096 imputation mse 0.9843021035194397\n",
      "Train Epoch 57.3 var loss 2.196014642715454 reconstruction mse 2.196014642715454 imputation mse 0.9920777082443237\n",
      "Train Epoch 57.4 var loss 2.257612705230713 reconstruction mse 2.257612705230713 imputation mse 0.99813312292099\n",
      "Train Epoch 57.5 var loss 2.305701494216919 reconstruction mse 2.305701494216919 imputation mse 1.0028164386749268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 57.6 var loss 2.3402750492095947 reconstruction mse 2.3402750492095947 imputation mse 1.0067161321640015\n",
      "Train Epoch 57.7 var loss 2.3622660636901855 reconstruction mse 2.3622660636901855 imputation mse 1.0102601051330566\n",
      "Train Epoch 57.8 var loss 2.373817205429077 reconstruction mse 2.373817205429077 imputation mse 1.0139367580413818\n",
      "Train Epoch 57.9 var loss 2.378225326538086 reconstruction mse 2.378225326538086 imputation mse 1.018074870109558\n",
      "Train Epoch 58.0 var loss 1.8026800155639648 reconstruction mse 1.8026800155639648 imputation mse 0.9716179966926575\n",
      "Train Epoch 58.1 var loss 1.9622136354446411 reconstruction mse 1.9622136354446411 imputation mse 0.9883421063423157\n",
      "Train Epoch 58.2 var loss 2.068556070327759 reconstruction mse 2.068556070327759 imputation mse 0.9976480007171631\n",
      "Train Epoch 58.3 var loss 2.151287078857422 reconstruction mse 2.151287078857422 imputation mse 1.0049577951431274\n",
      "Train Epoch 58.4 var loss 2.2166831493377686 reconstruction mse 2.2166831493377686 imputation mse 1.0106910467147827\n",
      "Train Epoch 58.5 var loss 2.2658703327178955 reconstruction mse 2.2658703327178955 imputation mse 1.015163540840149\n",
      "Train Epoch 58.6 var loss 2.2990822792053223 reconstruction mse 2.2990822792053223 imputation mse 1.018319010734558\n",
      "Train Epoch 58.7 var loss 2.3176658153533936 reconstruction mse 2.3176658153533936 imputation mse 1.02021324634552\n",
      "Train Epoch 58.8 var loss 2.3249728679656982 reconstruction mse 2.3249728679656982 imputation mse 1.021231770515442\n",
      "Train Epoch 58.9 var loss 2.326094388961792 reconstruction mse 2.326094388961792 imputation mse 1.0220074653625488\n",
      "Train Epoch 59.0 var loss 1.7498364448547363 reconstruction mse 1.7498364448547363 imputation mse 0.9738311767578125\n",
      "Train Epoch 59.1 var loss 1.9051626920700073 reconstruction mse 1.9051626920700073 imputation mse 0.9934335350990295\n",
      "Train Epoch 59.2 var loss 2.020575761795044 reconstruction mse 2.020575761795044 imputation mse 1.0045114755630493\n",
      "Train Epoch 59.3 var loss 2.1220955848693848 reconstruction mse 2.1220955848693848 imputation mse 1.0140690803527832\n",
      "Train Epoch 59.4 var loss 2.2121686935424805 reconstruction mse 2.2121686935424805 imputation mse 1.0229074954986572\n",
      "Train Epoch 59.5 var loss 2.2858402729034424 reconstruction mse 2.2858402729034424 imputation mse 1.0308198928833008\n",
      "Train Epoch 59.6 var loss 2.3371269702911377 reconstruction mse 2.3371269702911377 imputation mse 1.0373183488845825\n",
      "Train Epoch 59.7 var loss 2.3640897274017334 reconstruction mse 2.3640897274017334 imputation mse 1.0423474311828613\n",
      "Train Epoch 59.8 var loss 2.370335578918457 reconstruction mse 2.370335578918457 imputation mse 1.0460962057113647\n",
      "Train Epoch 59.9 var loss 2.362814426422119 reconstruction mse 2.362814426422119 imputation mse 1.0489369630813599\n",
      "Train Epoch 60.0 var loss 1.7253212928771973 reconstruction mse 1.7253212928771973 imputation mse 0.947056233882904\n",
      "Train Epoch 60.1 var loss 1.8880743980407715 reconstruction mse 1.8880743980407715 imputation mse 0.9636633396148682\n",
      "Train Epoch 60.2 var loss 2.0157923698425293 reconstruction mse 2.0157923698425293 imputation mse 0.972460925579071\n",
      "Train Epoch 60.3 var loss 2.1278114318847656 reconstruction mse 2.1278114318847656 imputation mse 0.9787835478782654\n",
      "Train Epoch 60.4 var loss 2.2243411540985107 reconstruction mse 2.2243411540985107 imputation mse 0.9838678240776062\n",
      "Train Epoch 60.5 var loss 2.3023011684417725 reconstruction mse 2.3023011684417725 imputation mse 0.9886533617973328\n",
      "Train Epoch 60.6 var loss 2.357422113418579 reconstruction mse 2.357422113418579 imputation mse 0.9937145709991455\n",
      "Train Epoch 60.7 var loss 2.387503147125244 reconstruction mse 2.387503147125244 imputation mse 0.998751699924469\n",
      "Train Epoch 60.8 var loss 2.3946316242218018 reconstruction mse 2.3946316242218018 imputation mse 1.003184199333191\n",
      "Train Epoch 60.9 var loss 2.384801149368286 reconstruction mse 2.384801149368286 imputation mse 1.006127119064331\n",
      "====> Test imputation mse: 1.37821209\n",
      "====> Test imputation mse: 1.27631950\n",
      "====> Test imputation mse: 1.26718485\n",
      "Train Epoch 61.0 var loss 1.7168904542922974 reconstruction mse 1.7168904542922974 imputation mse 0.9792258739471436\n",
      "Train Epoch 61.1 var loss 1.8802756071090698 reconstruction mse 1.8802756071090698 imputation mse 0.996360182762146\n",
      "Train Epoch 61.2 var loss 1.9982340335845947 reconstruction mse 1.9982340335845947 imputation mse 1.0063490867614746\n",
      "Train Epoch 61.3 var loss 2.0922746658325195 reconstruction mse 2.0922746658325195 imputation mse 1.0137349367141724\n",
      "Train Epoch 61.4 var loss 2.1660585403442383 reconstruction mse 2.1660585403442383 imputation mse 1.0198042392730713\n",
      "Train Epoch 61.5 var loss 2.2208240032196045 reconstruction mse 2.2208240032196045 imputation mse 1.0250881910324097\n",
      "Train Epoch 61.6 var loss 2.2584476470947266 reconstruction mse 2.2584476470947266 imputation mse 1.0297185182571411\n",
      "Train Epoch 61.7 var loss 2.282176971435547 reconstruction mse 2.282176971435547 imputation mse 1.033746361732483\n",
      "Train Epoch 61.8 var loss 2.296177387237549 reconstruction mse 2.296177387237549 imputation mse 1.037200927734375\n",
      "Train Epoch 61.9 var loss 2.304090976715088 reconstruction mse 2.304090976715088 imputation mse 1.0402153730392456\n",
      "Train Epoch 62.0 var loss 1.6920932531356812 reconstruction mse 1.6920932531356812 imputation mse 0.9901010394096375\n",
      "Train Epoch 62.1 var loss 1.842367172241211 reconstruction mse 1.842367172241211 imputation mse 1.0128670930862427\n",
      "Train Epoch 62.2 var loss 1.9535629749298096 reconstruction mse 1.9535629749298096 imputation mse 1.0249621868133545\n",
      "Train Epoch 62.3 var loss 2.044717788696289 reconstruction mse 2.044717788696289 imputation mse 1.034917950630188\n",
      "Train Epoch 62.4 var loss 2.117013931274414 reconstruction mse 2.117013931274414 imputation mse 1.0443940162658691\n",
      "Train Epoch 62.5 var loss 2.1688930988311768 reconstruction mse 2.1688930988311768 imputation mse 1.0539448261260986\n",
      "Train Epoch 62.6 var loss 2.2000744342803955 reconstruction mse 2.2000744342803955 imputation mse 1.0635148286819458\n",
      "Train Epoch 62.7 var loss 2.2130396366119385 reconstruction mse 2.2130396366119385 imputation mse 1.0728745460510254\n",
      "Train Epoch 62.8 var loss 2.212867259979248 reconstruction mse 2.212867259979248 imputation mse 1.0816582441329956\n",
      "Train Epoch 62.9 var loss 2.2064833641052246 reconstruction mse 2.2064833641052246 imputation mse 1.0891960859298706\n",
      "Train Epoch 63.0 var loss 1.6625213623046875 reconstruction mse 1.6625213623046875 imputation mse 0.9413571357727051\n",
      "Train Epoch 63.1 var loss 1.8076344728469849 reconstruction mse 1.8076344728469849 imputation mse 0.9585447311401367\n",
      "Train Epoch 63.2 var loss 1.9215235710144043 reconstruction mse 1.9215235710144043 imputation mse 0.9698488712310791\n",
      "Train Epoch 63.3 var loss 2.021958589553833 reconstruction mse 2.021958589553833 imputation mse 0.9801813960075378\n",
      "Train Epoch 63.4 var loss 2.1090216636657715 reconstruction mse 2.1090216636657715 imputation mse 0.9900264739990234\n",
      "Train Epoch 63.5 var loss 2.1788930892944336 reconstruction mse 2.1788930892944336 imputation mse 0.9995567798614502\n",
      "Train Epoch 63.6 var loss 2.228628396987915 reconstruction mse 2.228628396987915 imputation mse 1.0093896389007568\n",
      "Train Epoch 63.7 var loss 2.2581090927124023 reconstruction mse 2.2581090927124023 imputation mse 1.0189459323883057\n",
      "Train Epoch 63.8 var loss 2.2701103687286377 reconstruction mse 2.2701103687286377 imputation mse 1.0276705026626587\n",
      "Train Epoch 63.9 var loss 2.2696404457092285 reconstruction mse 2.2696404457092285 imputation mse 1.0350217819213867\n",
      "Train Epoch 64.0 var loss 1.6416932344436646 reconstruction mse 1.6416932344436646 imputation mse 0.9425949454307556\n",
      "Train Epoch 64.1 var loss 1.7801274061203003 reconstruction mse 1.7801274061203003 imputation mse 0.960355281829834\n",
      "Train Epoch 64.2 var loss 1.8824940919876099 reconstruction mse 1.8824940919876099 imputation mse 0.97100430727005\n",
      "Train Epoch 64.3 var loss 1.9707608222961426 reconstruction mse 1.9707608222961426 imputation mse 0.9803221821784973\n",
      "Train Epoch 64.4 var loss 2.047377586364746 reconstruction mse 2.047377586364746 imputation mse 0.9896383881568909\n",
      "Train Epoch 64.5 var loss 2.1103858947753906 reconstruction mse 2.1103858947753906 imputation mse 0.9991528391838074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 64.6 var loss 2.1566929817199707 reconstruction mse 2.1566929817199707 imputation mse 1.0086021423339844\n",
      "Train Epoch 64.7 var loss 2.1854801177978516 reconstruction mse 2.1854801177978516 imputation mse 1.017305612564087\n",
      "Train Epoch 64.8 var loss 2.198931932449341 reconstruction mse 2.198931932449341 imputation mse 1.0246371030807495\n",
      "Train Epoch 64.9 var loss 2.202540874481201 reconstruction mse 2.202540874481201 imputation mse 1.0304334163665771\n",
      "Train Epoch 65.0 var loss 1.6281039714813232 reconstruction mse 1.6281039714813232 imputation mse 0.9469174146652222\n",
      "Train Epoch 65.1 var loss 1.772411584854126 reconstruction mse 1.772411584854126 imputation mse 0.9656484127044678\n",
      "Train Epoch 65.2 var loss 1.888127326965332 reconstruction mse 1.888127326965332 imputation mse 0.9764717817306519\n",
      "Train Epoch 65.3 var loss 1.9915108680725098 reconstruction mse 1.9915108680725098 imputation mse 0.985380232334137\n",
      "Train Epoch 65.4 var loss 2.080082893371582 reconstruction mse 2.080082893371582 imputation mse 0.9936113357543945\n",
      "Train Epoch 65.5 var loss 2.149009943008423 reconstruction mse 2.149009943008423 imputation mse 1.0017234086990356\n",
      "Train Epoch 65.6 var loss 2.194469690322876 reconstruction mse 2.194469690322876 imputation mse 1.0100332498550415\n",
      "Train Epoch 65.7 var loss 2.2171669006347656 reconstruction mse 2.2171669006347656 imputation mse 1.018831491470337\n",
      "Train Epoch 65.8 var loss 2.2233242988586426 reconstruction mse 2.2233242988586426 imputation mse 1.0277163982391357\n",
      "Train Epoch 65.9 var loss 2.2224271297454834 reconstruction mse 2.2224271297454834 imputation mse 1.0358957052230835\n",
      "Train Epoch 66.0 var loss 1.6167587041854858 reconstruction mse 1.6167587041854858 imputation mse 0.974436342716217\n",
      "Train Epoch 66.1 var loss 1.7673898935317993 reconstruction mse 1.7673898935317993 imputation mse 0.9937829971313477\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "test()\n",
    "test()\n",
    "for epoch in range(1, 501):\n",
    "    train(epoch)\n",
    "    if epoch%10==0:\n",
    "        test()\n",
    "        test()\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mono",
   "language": "python",
   "name": "mono"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
