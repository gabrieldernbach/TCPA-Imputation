{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "class MissingEntryDataset(Dataset):\n",
    "    def __init__(self, data: torch.tensor, min_missing: float, max_missing: float):\n",
    "        self.data = data\n",
    "\n",
    "        n, d = data.shape\n",
    "        self.min_missing = min_missing\n",
    "        self.max_missing = max_missing\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        prob = np.random.uniform(self.min_missing, self.max_missing)\n",
    "        mask = np.random.binomial(1, prob, sample.shape)\n",
    "        mask = torch.tensor(mask).bool()\n",
    "\n",
    "        return sample, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3784, 946)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"TCPA_data_sel.csv\")\n",
    "\n",
    "# select the 189 real valued columns only\n",
    "X = df.iloc[:, 2:].values.astype(\"float32\")\n",
    "Xs = (X - X.mean(axis=0)) / X.var(axis=0)\n",
    "\n",
    "# split train/test\n",
    "n = len(Xs)\n",
    "idx = np.arange(len(Xs))\n",
    "ncut = int(n * 0.8)\n",
    "Xtrain = MissingEntryDataset(torch.tensor(Xs[idx[:ncut]]), 0.1, 0.4)\n",
    "Xtest = MissingEntryDataset(torch.tensor(Xs[idx[ncut:]]), 0.1, 0.4)\n",
    "len(Xtrain), len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (enc): Sequential(\n",
      "    (0): Linear(in_features=189, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mean): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (log_variance): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (dec): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): LinSkip(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Linear(in_features=512, out_features=189, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinSkip(nn.Module):\n",
    "    def __init__(self, ins):\n",
    "        super(LinSkip, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(ins, ins),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(ins))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, ins=189, hidden=512, latent=64, variational=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.variational = variational\n",
    "        \n",
    "        self.enc = nn.Sequential(nn.Linear(ins, hidden),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.BatchNorm1d(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden))\n",
    "        \n",
    "        self.mean = nn.Linear(hidden, latent)\n",
    "        self.log_variance = nn.Linear(hidden, latent)\n",
    "        \n",
    "        self.dec = nn.Sequential(nn.Linear(latent, hidden),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.BatchNorm1d(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 LinSkip(hidden),\n",
    "                                 nn.Linear(hidden, ins))\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x[mask] = torch.randn(mask.shape)[mask]\n",
    "        x = self.enc(x)\n",
    "        mu = self.mean(x)\n",
    "        log_var = self.log_variance(x)\n",
    "        z = self.sample(mu, log_var) if self.variational else mu\n",
    "        x = self.dec(z)\n",
    "        return x, mu, log_var\n",
    "    \n",
    "    def gibbs(self, x0, mask):\n",
    "        # create copy\n",
    "        xn = x0[:]\n",
    "        # iterativly predict\n",
    "        xns = []\n",
    "        for _ in range(20):\n",
    "            # reconstruction step\n",
    "            xn,_,_ = self.forward(xn, mask)\n",
    "            xns.append(xn)\n",
    "            # reset observed values\n",
    "            xn[~mask] = x0[~mask]\n",
    "        xn = torch.mean(torch.stack(xns, dim=-1), dim=-1)\n",
    "        return xn\n",
    "    \n",
    "vae = VAE(hidden=512, latent=128, variational=True)\n",
    "print(vae)\n",
    "assert vae(torch.tensor(np.random.randn(20, 189).astype(\"float32\")),\n",
    "           torch.tensor(np.random.randn(20, 189).astype(\"bool\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
    "\n",
    "def train(epoch):\n",
    "    sample, mask = Xtrain[:]\n",
    "    reconstruction = sample[:]\n",
    "    for n in range(10):\n",
    "        vae.train()\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, log_var = vae(reconstruction, mask)\n",
    "        mse = F.mse_loss(reconstruction, sample, reduction=\"sum\")\n",
    "        kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = mse + 1 * kl  # set to 1 for variational regularization (centered gaussian)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Train Epoch {epoch}.{n}\", \n",
    "              f\"var loss {loss.item()}\", \n",
    "              f\"reconstruction mse {mse.item()}\",\n",
    "              f\"imputation mse {F.mse_loss(reconstruction[mask], sample[mask], reduction='mean')}\")\n",
    "        reconstruction = reconstruction.detach() # drop previous computation graph before going into next iter\n",
    "        reconstruction[~mask] = sample[~mask]\n",
    "    \n",
    "def test():\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        sample, mask = Xtest[:]\n",
    "        reconstruction = vae.gibbs(sample, mask)   \n",
    "        test_loss = F.mse_loss(reconstruction[mask], sample[mask], reduction=\"mean\").item()\n",
    "    print('====> Test imputation mse: {:.8f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test imputation mse: 1.05301106\n",
      "====> Test imputation mse: 1.04914165\n",
      "====> Test imputation mse: 1.04920268\n",
      "Train Epoch 1.0 var loss 299094671360.0 reconstruction mse 4379207.5 imputation mse 4.079547882080078\n",
      "Train Epoch 1.1 var loss 714089664.0 reconstruction mse 4407217.0 imputation mse 4.034031391143799\n",
      "Train Epoch 1.2 var loss 99893784.0 reconstruction mse 4168317.0 imputation mse 3.8297300338745117\n",
      "Train Epoch 1.3 var loss 131044880.0 reconstruction mse 3920679.0 imputation mse 3.415123701095581\n",
      "Train Epoch 1.4 var loss 83495440.0 reconstruction mse 3682715.25 imputation mse 3.0968034267425537\n",
      "Train Epoch 1.5 var loss 116868720.0 reconstruction mse 3529519.25 imputation mse 2.828763008117676\n",
      "Train Epoch 1.6 var loss 42980480.0 reconstruction mse 3334655.5 imputation mse 2.6962342262268066\n",
      "Train Epoch 1.7 var loss 59920172.0 reconstruction mse 3261506.75 imputation mse 2.613262414932251\n",
      "Train Epoch 1.8 var loss 30513344.0 reconstruction mse 3163430.25 imputation mse 2.4083266258239746\n",
      "Train Epoch 1.9 var loss 18877048.0 reconstruction mse 3089808.25 imputation mse 2.3446619510650635\n",
      "Train Epoch 2.0 var loss 5276526592.0 reconstruction mse 2545465.0 imputation mse 2.347315788269043\n",
      "Train Epoch 2.1 var loss 934950464.0 reconstruction mse 2477955.5 imputation mse 2.2338786125183105\n",
      "Train Epoch 2.2 var loss 138712624.0 reconstruction mse 2436455.0 imputation mse 2.133185386657715\n",
      "Train Epoch 2.3 var loss 330265408.0 reconstruction mse 2466573.5 imputation mse 2.1898365020751953\n",
      "Train Epoch 2.4 var loss 48497516.0 reconstruction mse 2352411.25 imputation mse 2.090526819229126\n",
      "Train Epoch 2.5 var loss 44949864.0 reconstruction mse 2329015.25 imputation mse 1.998306393623352\n",
      "Train Epoch 2.6 var loss 46807524.0 reconstruction mse 2369905.25 imputation mse 2.066943645477295\n",
      "Train Epoch 2.7 var loss 26208538.0 reconstruction mse 2390064.5 imputation mse 2.0895256996154785\n",
      "Train Epoch 2.8 var loss 32296946.0 reconstruction mse 2296886.25 imputation mse 1.9861245155334473\n",
      "Train Epoch 2.9 var loss 23149382.0 reconstruction mse 2365622.5 imputation mse 2.0886683464050293\n",
      "Train Epoch 3.0 var loss 5146708.0 reconstruction mse 2115100.75 imputation mse 2.0393028259277344\n",
      "Train Epoch 3.1 var loss 3491984.5 reconstruction mse 2155854.0 imputation mse 2.0791149139404297\n",
      "Train Epoch 3.2 var loss 4411528.0 reconstruction mse 2167008.75 imputation mse 2.1069796085357666\n",
      "Train Epoch 3.3 var loss 3406313.0 reconstruction mse 2221563.75 imputation mse 2.1911532878875732\n",
      "Train Epoch 3.4 var loss 3937438.0 reconstruction mse 2160631.25 imputation mse 2.1065914630889893\n",
      "Train Epoch 3.5 var loss 3682234.5 reconstruction mse 2170049.5 imputation mse 2.0813097953796387\n",
      "Train Epoch 3.6 var loss 4084980.5 reconstruction mse 2094471.875 imputation mse 2.0270750522613525\n",
      "Train Epoch 3.7 var loss 4086732.75 reconstruction mse 2067713.625 imputation mse 1.9488626718521118\n",
      "Train Epoch 3.8 var loss 3243470.0 reconstruction mse 2180277.0 imputation mse 2.162417411804199\n",
      "Train Epoch 3.9 var loss 3136128.0 reconstruction mse 2125081.25 imputation mse 2.0884389877319336\n",
      "Train Epoch 4.0 var loss 4741239.0 reconstruction mse 1797906.5 imputation mse 1.864833116531372\n",
      "Train Epoch 4.1 var loss 8017839.0 reconstruction mse 1840400.625 imputation mse 1.9423186779022217\n",
      "Train Epoch 4.2 var loss 4582816.0 reconstruction mse 1974177.0 imputation mse 2.1620795726776123\n",
      "Train Epoch 4.3 var loss 4438172.5 reconstruction mse 1904516.375 imputation mse 2.050581216812134\n",
      "Train Epoch 4.4 var loss 6939042.0 reconstruction mse 1876413.125 imputation mse 2.020895004272461\n",
      "Train Epoch 4.5 var loss 7361494.0 reconstruction mse 1832096.875 imputation mse 1.9402172565460205\n",
      "Train Epoch 4.6 var loss 4716056.0 reconstruction mse 1845357.75 imputation mse 1.9706791639328003\n",
      "Train Epoch 4.7 var loss 4071144.25 reconstruction mse 1775735.5 imputation mse 1.8837270736694336\n",
      "Train Epoch 4.8 var loss 3365744.5 reconstruction mse 1860022.125 imputation mse 2.001647710800171\n",
      "Train Epoch 4.9 var loss 6013036.5 reconstruction mse 1659720.625 imputation mse 1.7068209648132324\n",
      "Train Epoch 5.0 var loss 3597220.25 reconstruction mse 1652875.625 imputation mse 1.823075294494629\n",
      "Train Epoch 5.1 var loss 5316515.0 reconstruction mse 1696413.75 imputation mse 1.9370007514953613\n",
      "Train Epoch 5.2 var loss 3354427.5 reconstruction mse 1645532.75 imputation mse 1.8621176481246948\n",
      "Train Epoch 5.3 var loss 3174326.0 reconstruction mse 1642554.125 imputation mse 1.8553121089935303\n",
      "Train Epoch 5.4 var loss 3242098.0 reconstruction mse 1620071.5 imputation mse 1.8381515741348267\n",
      "Train Epoch 5.5 var loss 3132077.75 reconstruction mse 1683482.25 imputation mse 1.9080162048339844\n",
      "Train Epoch 5.6 var loss 2543277.0 reconstruction mse 1729369.875 imputation mse 1.9938619136810303\n",
      "Train Epoch 5.7 var loss 3589228.5 reconstruction mse 1593315.125 imputation mse 1.7746025323867798\n",
      "Train Epoch 5.8 var loss 3383632.25 reconstruction mse 1588053.25 imputation mse 1.7952051162719727\n",
      "Train Epoch 5.9 var loss 2563692.0 reconstruction mse 1646829.125 imputation mse 1.8696507215499878\n",
      "Train Epoch 6.0 var loss 2167208.25 reconstruction mse 1519789.25 imputation mse 1.8417052030563354\n",
      "Train Epoch 6.1 var loss 2204496.5 reconstruction mse 1497218.625 imputation mse 1.8454450368881226\n",
      "Train Epoch 6.2 var loss 2383812.0 reconstruction mse 1553531.0 imputation mse 1.9252192974090576\n",
      "Train Epoch 6.3 var loss 2101265.0 reconstruction mse 1521131.25 imputation mse 1.8766454458236694\n",
      "Train Epoch 6.4 var loss 2119804.75 reconstruction mse 1464255.125 imputation mse 1.803425669670105\n",
      "Train Epoch 6.5 var loss 2160729.5 reconstruction mse 1523677.125 imputation mse 1.892991304397583\n",
      "Train Epoch 6.6 var loss 2146019.75 reconstruction mse 1554641.5 imputation mse 1.9292405843734741\n",
      "Train Epoch 6.7 var loss 2089181.875 reconstruction mse 1518199.25 imputation mse 1.8799265623092651\n",
      "Train Epoch 6.8 var loss 2098352.0 reconstruction mse 1476374.375 imputation mse 1.8230406045913696\n",
      "Train Epoch 6.9 var loss 2057433.125 reconstruction mse 1488085.25 imputation mse 1.8489328622817993\n",
      "Train Epoch 7.0 var loss 2072733.625 reconstruction mse 1371333.875 imputation mse 1.7411704063415527\n",
      "Train Epoch 7.1 var loss 2433742.0 reconstruction mse 1317381.5 imputation mse 1.7195101976394653\n",
      "Train Epoch 7.2 var loss 2249942.0 reconstruction mse 1327902.0 imputation mse 1.7423737049102783\n",
      "Train Epoch 7.3 var loss 2004793.75 reconstruction mse 1327585.5 imputation mse 1.7355016469955444\n",
      "Train Epoch 7.4 var loss 2135287.0 reconstruction mse 1349704.75 imputation mse 1.7794764041900635\n",
      "Train Epoch 7.5 var loss 2102287.25 reconstruction mse 1327783.0 imputation mse 1.7428805828094482\n",
      "Train Epoch 7.6 var loss 2194980.0 reconstruction mse 1352569.625 imputation mse 1.7593690156936646\n",
      "Train Epoch 7.7 var loss 1990012.75 reconstruction mse 1302629.0 imputation mse 1.7058122158050537\n",
      "Train Epoch 7.8 var loss 1928727.0 reconstruction mse 1280758.0 imputation mse 1.6737778186798096\n",
      "Train Epoch 7.9 var loss 2581373.25 reconstruction mse 1296276.625 imputation mse 1.6875011920928955\n",
      "Train Epoch 8.0 var loss 2093501.875 reconstruction mse 1250782.875 imputation mse 1.5794141292572021\n",
      "Train Epoch 8.1 var loss 2050219.75 reconstruction mse 1257860.25 imputation mse 1.671486258506775\n",
      "Train Epoch 8.2 var loss 2019580.0 reconstruction mse 1282115.0 imputation mse 1.7202060222625732\n",
      "Train Epoch 8.3 var loss 1961906.375 reconstruction mse 1218652.75 imputation mse 1.6092994213104248\n",
      "Train Epoch 8.4 var loss 1892658.375 reconstruction mse 1238667.125 imputation mse 1.6401753425598145\n",
      "Train Epoch 8.5 var loss 1869771.25 reconstruction mse 1198562.0 imputation mse 1.5916756391525269\n",
      "Train Epoch 8.6 var loss 1981162.0 reconstruction mse 1213522.875 imputation mse 1.6207537651062012\n",
      "Train Epoch 8.7 var loss 1803049.0 reconstruction mse 1222535.75 imputation mse 1.632919192314148\n",
      "Train Epoch 8.8 var loss 1991878.0 reconstruction mse 1248747.5 imputation mse 1.670114517211914\n",
      "Train Epoch 8.9 var loss 1845997.125 reconstruction mse 1232793.25 imputation mse 1.653386116027832\n",
      "Train Epoch 9.0 var loss 1743068.625 reconstruction mse 1175414.0 imputation mse 1.5502514839172363\n",
      "Train Epoch 9.1 var loss 1687680.125 reconstruction mse 1194014.0 imputation mse 1.6424342393875122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 9.2 var loss 1765222.375 reconstruction mse 1193182.5 imputation mse 1.6415743827819824\n",
      "Train Epoch 9.3 var loss 1726023.0 reconstruction mse 1187348.625 imputation mse 1.6351244449615479\n",
      "Train Epoch 9.4 var loss 1665059.25 reconstruction mse 1171981.125 imputation mse 1.6187598705291748\n",
      "Train Epoch 9.5 var loss 1745665.625 reconstruction mse 1207826.0 imputation mse 1.6662957668304443\n",
      "Train Epoch 9.6 var loss 1690887.125 reconstruction mse 1151322.875 imputation mse 1.5748322010040283\n",
      "Train Epoch 9.7 var loss 1783963.25 reconstruction mse 1174808.375 imputation mse 1.6160300970077515\n",
      "Train Epoch 9.8 var loss 1709975.0 reconstruction mse 1179843.875 imputation mse 1.6269973516464233\n",
      "Train Epoch 9.9 var loss 1688816.375 reconstruction mse 1186778.25 imputation mse 1.6351354122161865\n",
      "Train Epoch 10.0 var loss 1620979.5 reconstruction mse 1153319.5 imputation mse 1.523240327835083\n",
      "Train Epoch 10.1 var loss 1622570.75 reconstruction mse 1173457.875 imputation mse 1.64744234085083\n",
      "Train Epoch 10.2 var loss 1625164.125 reconstruction mse 1166753.75 imputation mse 1.6330007314682007\n",
      "Train Epoch 10.3 var loss 1632791.0 reconstruction mse 1204874.625 imputation mse 1.6951302289962769\n",
      "Train Epoch 10.4 var loss 1610863.875 reconstruction mse 1153291.125 imputation mse 1.6216057538986206\n",
      "Train Epoch 10.5 var loss 1582738.75 reconstruction mse 1137269.625 imputation mse 1.5908855199813843\n",
      "Train Epoch 10.6 var loss 1616759.5 reconstruction mse 1145343.5 imputation mse 1.6038918495178223\n",
      "Train Epoch 10.7 var loss 1637297.0 reconstruction mse 1154526.25 imputation mse 1.6261467933654785\n",
      "Train Epoch 10.8 var loss 1551611.125 reconstruction mse 1121244.125 imputation mse 1.5738886594772339\n",
      "Train Epoch 10.9 var loss 1556369.25 reconstruction mse 1135234.875 imputation mse 1.5875349044799805\n",
      "====> Test imputation mse: 1.05102110\n",
      "====> Test imputation mse: 1.05483985\n",
      "====> Test imputation mse: 1.04716611\n",
      "Train Epoch 11.0 var loss 1543530.75 reconstruction mse 1120583.125 imputation mse 1.5186258554458618\n",
      "Train Epoch 11.1 var loss 1553351.0 reconstruction mse 1144024.625 imputation mse 1.6197654008865356\n",
      "Train Epoch 11.2 var loss 1567527.0 reconstruction mse 1158130.5 imputation mse 1.634503722190857\n",
      "Train Epoch 11.3 var loss 1559473.625 reconstruction mse 1150222.75 imputation mse 1.625659704208374\n",
      "Train Epoch 11.4 var loss 1522516.875 reconstruction mse 1102813.5 imputation mse 1.561735987663269\n",
      "Train Epoch 11.5 var loss 1524201.625 reconstruction mse 1107939.0 imputation mse 1.5638539791107178\n",
      "Train Epoch 11.6 var loss 1526358.0 reconstruction mse 1121973.75 imputation mse 1.5874024629592896\n",
      "Train Epoch 11.7 var loss 1613663.125 reconstruction mse 1118445.5 imputation mse 1.58343505859375\n",
      "Train Epoch 11.8 var loss 1533262.25 reconstruction mse 1084229.375 imputation mse 1.5354945659637451\n",
      "Train Epoch 11.9 var loss 1501201.25 reconstruction mse 1099568.25 imputation mse 1.55887770652771\n",
      "Train Epoch 12.0 var loss 1473709.75 reconstruction mse 1062312.125 imputation mse 1.4589155912399292\n",
      "Train Epoch 12.1 var loss 1476456.625 reconstruction mse 1082357.875 imputation mse 1.5604615211486816\n",
      "Train Epoch 12.2 var loss 1450258.875 reconstruction mse 1059438.5 imputation mse 1.5481078624725342\n",
      "Train Epoch 12.3 var loss 1464209.625 reconstruction mse 1064991.25 imputation mse 1.5348286628723145\n",
      "Train Epoch 12.4 var loss 1450663.5 reconstruction mse 1058509.0 imputation mse 1.5321006774902344\n",
      "Train Epoch 12.5 var loss 1439449.25 reconstruction mse 1041541.4375 imputation mse 1.51614248752594\n",
      "Train Epoch 12.6 var loss 1429522.75 reconstruction mse 1036723.3125 imputation mse 1.5040241479873657\n",
      "Train Epoch 12.7 var loss 1442885.0 reconstruction mse 1045448.0 imputation mse 1.5213345289230347\n",
      "Train Epoch 12.8 var loss 1425634.25 reconstruction mse 1038641.8125 imputation mse 1.5080432891845703\n",
      "Train Epoch 12.9 var loss 1432289.375 reconstruction mse 1029547.875 imputation mse 1.4898018836975098\n",
      "Train Epoch 13.0 var loss 1440984.625 reconstruction mse 1008032.0 imputation mse 1.3683795928955078\n",
      "Train Epoch 13.1 var loss 1424217.25 reconstruction mse 1015545.125 imputation mse 1.4576116800308228\n",
      "Train Epoch 13.2 var loss 1461026.25 reconstruction mse 1016291.125 imputation mse 1.4505287408828735\n",
      "Train Epoch 13.3 var loss 1465828.25 reconstruction mse 1031049.4375 imputation mse 1.4967422485351562\n",
      "Train Epoch 13.4 var loss 1427876.5 reconstruction mse 1005365.625 imputation mse 1.4441949129104614\n",
      "Train Epoch 13.5 var loss 1403788.0 reconstruction mse 1004147.0625 imputation mse 1.4365439414978027\n",
      "Train Epoch 13.6 var loss 1390145.625 reconstruction mse 1002792.5 imputation mse 1.4576947689056396\n",
      "Train Epoch 13.7 var loss 1407564.5 reconstruction mse 1000085.0625 imputation mse 1.439149260520935\n",
      "Train Epoch 13.8 var loss 1405700.75 reconstruction mse 1019066.5 imputation mse 1.4712064266204834\n",
      "Train Epoch 13.9 var loss 1396096.625 reconstruction mse 995279.0625 imputation mse 1.43363356590271\n",
      "Train Epoch 14.0 var loss 1379849.75 reconstruction mse 1021674.125 imputation mse 1.404077410697937\n",
      "Train Epoch 14.1 var loss 1392324.25 reconstruction mse 1036695.75 imputation mse 1.4963452816009521\n",
      "Train Epoch 14.2 var loss 1386819.375 reconstruction mse 1029534.625 imputation mse 1.4879512786865234\n",
      "Train Epoch 14.3 var loss 1383889.125 reconstruction mse 1031097.9375 imputation mse 1.4873799085617065\n",
      "Train Epoch 14.4 var loss 1380367.0 reconstruction mse 1026967.75 imputation mse 1.4822168350219727\n",
      "Train Epoch 14.5 var loss 1376004.25 reconstruction mse 1026335.1875 imputation mse 1.4846950769424438\n",
      "Train Epoch 14.6 var loss 1370752.0 reconstruction mse 1019630.0 imputation mse 1.4742238521575928\n",
      "Train Epoch 14.7 var loss 1365397.5 reconstruction mse 1015925.5 imputation mse 1.466397762298584\n",
      "Train Epoch 14.8 var loss 1355995.0 reconstruction mse 1007799.75 imputation mse 1.4535185098648071\n",
      "Train Epoch 14.9 var loss 1354015.5 reconstruction mse 1006036.9375 imputation mse 1.4565163850784302\n",
      "Train Epoch 15.0 var loss 1330123.75 reconstruction mse 986392.375 imputation mse 1.364123821258545\n",
      "Train Epoch 15.1 var loss 1340791.75 reconstruction mse 998205.5625 imputation mse 1.4596184492111206\n",
      "Train Epoch 15.2 var loss 1337961.0 reconstruction mse 995537.9375 imputation mse 1.456863522529602\n",
      "Train Epoch 15.3 var loss 1329654.875 reconstruction mse 986830.75 imputation mse 1.4422426223754883\n",
      "Train Epoch 15.4 var loss 1327241.25 reconstruction mse 985849.125 imputation mse 1.4419066905975342\n",
      "Train Epoch 15.5 var loss 1322484.5 reconstruction mse 984334.1875 imputation mse 1.4397518634796143\n",
      "Train Epoch 15.6 var loss 1321384.25 reconstruction mse 983854.875 imputation mse 1.4377567768096924\n",
      "Train Epoch 15.7 var loss 1317051.5 reconstruction mse 979138.25 imputation mse 1.425114631652832\n",
      "Train Epoch 15.8 var loss 1307523.0 reconstruction mse 973269.125 imputation mse 1.4185489416122437\n",
      "Train Epoch 15.9 var loss 1304945.5 reconstruction mse 971260.4375 imputation mse 1.4188288450241089\n",
      "Train Epoch 16.0 var loss 1287910.375 reconstruction mse 955656.0625 imputation mse 1.320757269859314\n",
      "Train Epoch 16.1 var loss 1303039.25 reconstruction mse 972874.8125 imputation mse 1.4148834943771362\n",
      "Train Epoch 16.2 var loss 1297456.0 reconstruction mse 966117.5 imputation mse 1.4068750143051147\n",
      "Train Epoch 16.3 var loss 1296127.0 reconstruction mse 967001.9375 imputation mse 1.4117457866668701\n",
      "Train Epoch 16.4 var loss 1293416.25 reconstruction mse 966833.0 imputation mse 1.4045974016189575\n",
      "Train Epoch 16.5 var loss 1288675.625 reconstruction mse 963123.6875 imputation mse 1.3982645273208618\n",
      "Train Epoch 16.6 var loss 1286720.5 reconstruction mse 959197.0 imputation mse 1.3966318368911743\n",
      "Train Epoch 16.7 var loss 1282995.5 reconstruction mse 959328.0625 imputation mse 1.3968749046325684\n",
      "Train Epoch 16.8 var loss 1280884.375 reconstruction mse 957183.8125 imputation mse 1.3948051929473877\n",
      "Train Epoch 16.9 var loss 1276223.75 reconstruction mse 954751.5 imputation mse 1.3869736194610596\n",
      "Train Epoch 17.0 var loss 1255302.25 reconstruction mse 936660.4375 imputation mse 1.2993501424789429\n",
      "Train Epoch 17.1 var loss 1275651.75 reconstruction mse 955654.8125 imputation mse 1.3902167081832886\n",
      "Train Epoch 17.2 var loss 1273247.25 reconstruction mse 954761.9375 imputation mse 1.3876765966415405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 17.3 var loss 1270239.0 reconstruction mse 953509.375 imputation mse 1.382903814315796\n",
      "Train Epoch 17.4 var loss 1268878.0 reconstruction mse 951789.4375 imputation mse 1.3806519508361816\n",
      "Train Epoch 17.5 var loss 1263690.0 reconstruction mse 949112.0 imputation mse 1.375594139099121\n",
      "Train Epoch 17.6 var loss 1260072.0 reconstruction mse 946791.375 imputation mse 1.3711674213409424\n",
      "Train Epoch 17.7 var loss 1259892.25 reconstruction mse 948693.0625 imputation mse 1.376996636390686\n",
      "Train Epoch 17.8 var loss 1259267.0 reconstruction mse 946614.3125 imputation mse 1.3749547004699707\n",
      "Train Epoch 17.9 var loss 1251556.5 reconstruction mse 941774.8125 imputation mse 1.3638010025024414\n",
      "Train Epoch 18.0 var loss 1224837.75 reconstruction mse 916757.1875 imputation mse 1.2830249071121216\n",
      "Train Epoch 18.1 var loss 1241701.75 reconstruction mse 934485.6875 imputation mse 1.3778527975082397\n",
      "Train Epoch 18.2 var loss 1234922.75 reconstruction mse 928998.25 imputation mse 1.3654075860977173\n",
      "Train Epoch 18.3 var loss 1230688.125 reconstruction mse 925530.5 imputation mse 1.3620645999908447\n",
      "Train Epoch 18.4 var loss 1230674.375 reconstruction mse 927230.375 imputation mse 1.369147539138794\n",
      "Train Epoch 18.5 var loss 1227621.875 reconstruction mse 926346.3125 imputation mse 1.361738920211792\n",
      "Train Epoch 18.6 var loss 1224402.75 reconstruction mse 925793.3125 imputation mse 1.363031029701233\n",
      "Train Epoch 18.7 var loss 1222032.375 reconstruction mse 922993.3125 imputation mse 1.358391284942627\n",
      "Train Epoch 18.8 var loss 1218253.625 reconstruction mse 920635.0625 imputation mse 1.3563668727874756\n",
      "Train Epoch 18.9 var loss 1216682.0 reconstruction mse 920108.8125 imputation mse 1.3502429723739624\n",
      "Train Epoch 19.0 var loss 1198896.875 reconstruction mse 902012.8125 imputation mse 1.2546544075012207\n",
      "Train Epoch 19.1 var loss 1203719.625 reconstruction mse 908222.4375 imputation mse 1.3440744876861572\n",
      "Train Epoch 19.2 var loss 1198365.0 reconstruction mse 904317.3125 imputation mse 1.323675513267517\n",
      "Train Epoch 19.3 var loss 1199097.75 reconstruction mse 905824.625 imputation mse 1.3359742164611816\n",
      "Train Epoch 19.4 var loss 1194890.5 reconstruction mse 902949.3125 imputation mse 1.3293139934539795\n",
      "Train Epoch 19.5 var loss 1194882.0 reconstruction mse 903623.25 imputation mse 1.335891604423523\n",
      "Train Epoch 19.6 var loss 1189635.75 reconstruction mse 899094.375 imputation mse 1.322755217552185\n",
      "Train Epoch 19.7 var loss 1186333.125 reconstruction mse 897796.0 imputation mse 1.323496699333191\n",
      "Train Epoch 19.8 var loss 1186327.75 reconstruction mse 898733.75 imputation mse 1.3267362117767334\n",
      "Train Epoch 19.9 var loss 1180864.75 reconstruction mse 893941.9375 imputation mse 1.3164832592010498\n",
      "Train Epoch 20.0 var loss 1175452.75 reconstruction mse 889757.1875 imputation mse 1.2353723049163818\n",
      "Train Epoch 20.1 var loss 1189900.75 reconstruction mse 905423.1875 imputation mse 1.3292630910873413\n",
      "Train Epoch 20.2 var loss 1183326.0 reconstruction mse 900199.75 imputation mse 1.3158369064331055\n",
      "Train Epoch 20.3 var loss 1182795.5 reconstruction mse 899278.25 imputation mse 1.311555027961731\n",
      "Train Epoch 20.4 var loss 1179864.375 reconstruction mse 898404.875 imputation mse 1.3094910383224487\n",
      "Train Epoch 20.5 var loss 1177706.75 reconstruction mse 896693.375 imputation mse 1.3092381954193115\n",
      "Train Epoch 20.6 var loss 1176902.5 reconstruction mse 896454.1875 imputation mse 1.3092948198318481\n",
      "Train Epoch 20.7 var loss 1171896.125 reconstruction mse 893389.25 imputation mse 1.3076691627502441\n",
      "Train Epoch 20.8 var loss 1173864.0 reconstruction mse 895363.125 imputation mse 1.3111871480941772\n",
      "Train Epoch 20.9 var loss 1169239.875 reconstruction mse 892290.875 imputation mse 1.307929515838623\n",
      "====> Test imputation mse: 1.03832424\n",
      "====> Test imputation mse: 1.04776847\n",
      "====> Test imputation mse: 1.04126716\n",
      "Train Epoch 21.0 var loss 1155668.0 reconstruction mse 880174.3125 imputation mse 1.2250730991363525\n",
      "Train Epoch 21.1 var loss 1166362.0 reconstruction mse 892162.6875 imputation mse 1.312828779220581\n",
      "Train Epoch 21.2 var loss 1167089.625 reconstruction mse 892744.25 imputation mse 1.3062646389007568\n",
      "Train Epoch 21.3 var loss 1162989.875 reconstruction mse 890602.6875 imputation mse 1.3017641305923462\n",
      "Train Epoch 21.4 var loss 1161459.75 reconstruction mse 891212.0 imputation mse 1.3098982572555542\n",
      "Train Epoch 21.5 var loss 1158605.75 reconstruction mse 887867.1875 imputation mse 1.299109697341919\n",
      "Train Epoch 21.6 var loss 1155789.875 reconstruction mse 887136.1875 imputation mse 1.2978646755218506\n",
      "Train Epoch 21.7 var loss 1153398.5 reconstruction mse 884944.25 imputation mse 1.2986164093017578\n",
      "Train Epoch 21.8 var loss 1152192.25 reconstruction mse 885190.75 imputation mse 1.296890377998352\n",
      "Train Epoch 21.9 var loss 1151432.25 reconstruction mse 885683.8125 imputation mse 1.2946151494979858\n",
      "Train Epoch 22.0 var loss 1135214.0 reconstruction mse 868493.875 imputation mse 1.2124686241149902\n",
      "Train Epoch 22.1 var loss 1147151.25 reconstruction mse 879799.25 imputation mse 1.298095464706421\n",
      "Train Epoch 22.2 var loss 1142740.875 reconstruction mse 878174.0 imputation mse 1.2918943166732788\n",
      "Train Epoch 22.3 var loss 1141841.0 reconstruction mse 878531.0 imputation mse 1.2887076139450073\n",
      "Train Epoch 22.4 var loss 1140779.125 reconstruction mse 878510.375 imputation mse 1.2950958013534546\n",
      "Train Epoch 22.5 var loss 1137340.0 reconstruction mse 875170.0 imputation mse 1.2901674509048462\n",
      "Train Epoch 22.6 var loss 1134871.25 reconstruction mse 874472.125 imputation mse 1.2846717834472656\n",
      "Train Epoch 22.7 var loss 1134533.375 reconstruction mse 873849.25 imputation mse 1.2861660718917847\n",
      "Train Epoch 22.8 var loss 1133984.875 reconstruction mse 873767.125 imputation mse 1.2843025922775269\n",
      "Train Epoch 22.9 var loss 1131767.5 reconstruction mse 873507.75 imputation mse 1.2826107740402222\n",
      "Train Epoch 23.0 var loss 1130163.0 reconstruction mse 875384.0 imputation mse 1.2251677513122559\n",
      "Train Epoch 23.1 var loss 1149160.0 reconstruction mse 895823.0625 imputation mse 1.308618426322937\n",
      "Train Epoch 23.2 var loss 1143795.875 reconstruction mse 890302.625 imputation mse 1.3028589487075806\n",
      "Train Epoch 23.3 var loss 1142625.75 reconstruction mse 891007.3125 imputation mse 1.3028637170791626\n",
      "Train Epoch 23.4 var loss 1140857.375 reconstruction mse 889748.6875 imputation mse 1.3004175424575806\n",
      "Train Epoch 23.5 var loss 1135690.5 reconstruction mse 885072.0 imputation mse 1.2938141822814941\n",
      "Train Epoch 23.6 var loss 1133589.75 reconstruction mse 883103.5625 imputation mse 1.290832281112671\n",
      "Train Epoch 23.7 var loss 1135392.75 reconstruction mse 885767.625 imputation mse 1.2930762767791748\n",
      "Train Epoch 23.8 var loss 1129850.375 reconstruction mse 881015.875 imputation mse 1.2862915992736816\n",
      "Train Epoch 23.9 var loss 1130843.5 reconstruction mse 881447.0 imputation mse 1.285174012184143\n",
      "Train Epoch 24.0 var loss 1109717.125 reconstruction mse 861602.5625 imputation mse 1.1975550651550293\n",
      "Train Epoch 24.1 var loss 1116580.375 reconstruction mse 868886.0625 imputation mse 1.2766239643096924\n",
      "Train Epoch 24.2 var loss 1114480.75 reconstruction mse 869786.5625 imputation mse 1.2780654430389404\n",
      "Train Epoch 24.3 var loss 1111796.125 reconstruction mse 865486.0 imputation mse 1.2674481868743896\n",
      "Train Epoch 24.4 var loss 1108944.75 reconstruction mse 864554.0 imputation mse 1.2631936073303223\n",
      "Train Epoch 24.5 var loss 1107185.625 reconstruction mse 861977.9375 imputation mse 1.2636797428131104\n",
      "Train Epoch 24.6 var loss 1105304.5 reconstruction mse 861384.25 imputation mse 1.2608675956726074\n",
      "Train Epoch 24.7 var loss 1104978.75 reconstruction mse 861370.3125 imputation mse 1.2688580751419067\n",
      "Train Epoch 24.8 var loss 1103858.25 reconstruction mse 861351.5 imputation mse 1.2610418796539307\n",
      "Train Epoch 24.9 var loss 1099625.5 reconstruction mse 856889.625 imputation mse 1.2559475898742676\n",
      "Train Epoch 25.0 var loss 1090419.5 reconstruction mse 849338.0 imputation mse 1.1906417608261108\n",
      "Train Epoch 25.1 var loss 1105057.125 reconstruction mse 863111.1875 imputation mse 1.2671661376953125\n",
      "Train Epoch 25.2 var loss 1099768.375 reconstruction mse 858996.4375 imputation mse 1.257260799407959\n",
      "Train Epoch 25.3 var loss 1097938.0 reconstruction mse 858299.75 imputation mse 1.2610974311828613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 25.4 var loss 1100459.75 reconstruction mse 858152.0 imputation mse 1.2596590518951416\n",
      "Train Epoch 25.5 var loss 1095834.125 reconstruction mse 855635.75 imputation mse 1.2474653720855713\n",
      "Train Epoch 25.6 var loss 1095344.75 reconstruction mse 856931.3125 imputation mse 1.257463812828064\n",
      "Train Epoch 25.7 var loss 1094055.75 reconstruction mse 856527.0 imputation mse 1.2540138959884644\n",
      "Train Epoch 25.8 var loss 1091700.375 reconstruction mse 854897.9375 imputation mse 1.2521021366119385\n",
      "Train Epoch 25.9 var loss 1090177.75 reconstruction mse 854506.75 imputation mse 1.248626708984375\n",
      "Train Epoch 26.0 var loss 1073779.625 reconstruction mse 838979.5625 imputation mse 1.1688563823699951\n",
      "Train Epoch 26.1 var loss 1087840.5 reconstruction mse 853773.25 imputation mse 1.2367359399795532\n",
      "Train Epoch 26.2 var loss 1087266.5 reconstruction mse 853510.875 imputation mse 1.23895263671875\n",
      "Train Epoch 26.3 var loss 1087103.5 reconstruction mse 853534.125 imputation mse 1.2357420921325684\n",
      "Train Epoch 26.4 var loss 1082140.25 reconstruction mse 850684.125 imputation mse 1.2327877283096313\n",
      "Train Epoch 26.5 var loss 1081074.0 reconstruction mse 849148.375 imputation mse 1.2309846878051758\n",
      "Train Epoch 26.6 var loss 1080557.625 reconstruction mse 849374.9375 imputation mse 1.2329193353652954\n",
      "Train Epoch 26.7 var loss 1080260.125 reconstruction mse 849734.5625 imputation mse 1.2315291166305542\n",
      "Train Epoch 26.8 var loss 1078774.75 reconstruction mse 849231.1875 imputation mse 1.2311689853668213\n",
      "Train Epoch 26.9 var loss 1077146.0 reconstruction mse 847282.8125 imputation mse 1.227555751800537\n",
      "Train Epoch 27.0 var loss 1061621.25 reconstruction mse 833551.5625 imputation mse 1.1623936891555786\n",
      "Train Epoch 27.1 var loss 1067683.0 reconstruction mse 840440.5625 imputation mse 1.2316807508468628\n",
      "Train Epoch 27.2 var loss 1065256.875 reconstruction mse 838625.5 imputation mse 1.2241281270980835\n",
      "Train Epoch 27.3 var loss 1062546.0 reconstruction mse 837397.25 imputation mse 1.2270658016204834\n",
      "Train Epoch 27.4 var loss 1062907.5 reconstruction mse 836910.375 imputation mse 1.2240135669708252\n",
      "Train Epoch 27.5 var loss 1062046.5 reconstruction mse 838023.5625 imputation mse 1.2322325706481934\n",
      "Train Epoch 27.6 var loss 1058583.875 reconstruction mse 835434.125 imputation mse 1.2219982147216797\n",
      "Train Epoch 27.7 var loss 1056755.125 reconstruction mse 834020.5 imputation mse 1.2255498170852661\n",
      "Train Epoch 27.8 var loss 1056118.0 reconstruction mse 833780.625 imputation mse 1.226611614227295\n",
      "Train Epoch 27.9 var loss 1054452.25 reconstruction mse 833405.8125 imputation mse 1.2198299169540405\n",
      "Train Epoch 28.0 var loss 1047929.0 reconstruction mse 827139.5 imputation mse 1.1575062274932861\n",
      "Train Epoch 28.1 var loss 1066748.875 reconstruction mse 846705.3125 imputation mse 1.2253519296646118\n",
      "Train Epoch 28.2 var loss 1064902.0 reconstruction mse 844271.25 imputation mse 1.2204581499099731\n",
      "Train Epoch 28.3 var loss 1063511.125 reconstruction mse 844479.25 imputation mse 1.2241019010543823\n",
      "Train Epoch 28.4 var loss 1061653.25 reconstruction mse 842641.625 imputation mse 1.2174127101898193\n",
      "Train Epoch 28.5 var loss 1061357.5 reconstruction mse 843731.0625 imputation mse 1.2206474542617798\n",
      "Train Epoch 28.6 var loss 1061163.25 reconstruction mse 843866.0 imputation mse 1.2210630178451538\n",
      "Train Epoch 28.7 var loss 1058734.125 reconstruction mse 841565.125 imputation mse 1.216160535812378\n",
      "Train Epoch 28.8 var loss 1055332.75 reconstruction mse 839920.625 imputation mse 1.2162139415740967\n",
      "Train Epoch 28.9 var loss 1054463.75 reconstruction mse 838912.5625 imputation mse 1.2175475358963013\n",
      "Train Epoch 29.0 var loss 1035854.4375 reconstruction mse 821105.0625 imputation mse 1.144378900527954\n",
      "Train Epoch 29.1 var loss 1043579.0 reconstruction mse 830052.9375 imputation mse 1.2131991386413574\n",
      "Train Epoch 29.2 var loss 1042512.5 reconstruction mse 830007.5625 imputation mse 1.2105810642242432\n",
      "Train Epoch 29.3 var loss 1039974.625 reconstruction mse 827455.4375 imputation mse 1.2077844142913818\n",
      "Train Epoch 29.4 var loss 1040033.125 reconstruction mse 826935.5 imputation mse 1.20735764503479\n",
      "Train Epoch 29.5 var loss 1037614.25 reconstruction mse 827084.375 imputation mse 1.2072935104370117\n",
      "Train Epoch 29.6 var loss 1038192.0 reconstruction mse 827062.4375 imputation mse 1.2056840658187866\n",
      "Train Epoch 29.7 var loss 1036870.125 reconstruction mse 825978.0 imputation mse 1.2069367170333862\n",
      "Train Epoch 29.8 var loss 1035171.6875 reconstruction mse 825497.25 imputation mse 1.2028930187225342\n",
      "Train Epoch 29.9 var loss 1031692.25 reconstruction mse 822657.1875 imputation mse 1.196529746055603\n",
      "Train Epoch 30.0 var loss 1023378.625 reconstruction mse 814058.3125 imputation mse 1.1425399780273438\n",
      "Train Epoch 30.1 var loss 1028289.4375 reconstruction mse 820399.125 imputation mse 1.2022905349731445\n",
      "Train Epoch 30.2 var loss 1028076.625 reconstruction mse 820050.0 imputation mse 1.2035448551177979\n",
      "Train Epoch 30.3 var loss 1025353.625 reconstruction mse 818372.5625 imputation mse 1.2017778158187866\n",
      "Train Epoch 30.4 var loss 1025367.5 reconstruction mse 818733.5625 imputation mse 1.2039921283721924\n",
      "Train Epoch 30.5 var loss 1023599.3125 reconstruction mse 817573.875 imputation mse 1.2015867233276367\n",
      "Train Epoch 30.6 var loss 1023050.3125 reconstruction mse 817053.4375 imputation mse 1.2003575563430786\n",
      "Train Epoch 30.7 var loss 1020499.25 reconstruction mse 815521.375 imputation mse 1.198362112045288\n",
      "Train Epoch 30.8 var loss 1020073.9375 reconstruction mse 814739.0625 imputation mse 1.1937888860702515\n",
      "Train Epoch 30.9 var loss 1017009.5625 reconstruction mse 812892.5 imputation mse 1.188492774963379\n",
      "====> Test imputation mse: 1.05401027\n",
      "====> Test imputation mse: 1.04548502\n",
      "====> Test imputation mse: 1.04083586\n",
      "Train Epoch 31.0 var loss 1012662.1875 reconstruction mse 808035.875 imputation mse 1.1289641857147217\n",
      "Train Epoch 31.1 var loss 1030710.8125 reconstruction mse 826851.9375 imputation mse 1.1981773376464844\n",
      "Train Epoch 31.2 var loss 1025146.75 reconstruction mse 821978.125 imputation mse 1.1870648860931396\n",
      "Train Epoch 31.3 var loss 1026826.6875 reconstruction mse 824836.125 imputation mse 1.1938093900680542\n",
      "Train Epoch 31.4 var loss 1025679.625 reconstruction mse 822421.875 imputation mse 1.1915138959884644\n",
      "Train Epoch 31.5 var loss 1025031.75 reconstruction mse 823116.5625 imputation mse 1.189882516860962\n",
      "Train Epoch 31.6 var loss 1022697.875 reconstruction mse 821819.0 imputation mse 1.187040090560913\n",
      "Train Epoch 31.7 var loss 1023712.5625 reconstruction mse 823501.5625 imputation mse 1.1926181316375732\n",
      "Train Epoch 31.8 var loss 1021467.5 reconstruction mse 822234.0625 imputation mse 1.1884020566940308\n",
      "Train Epoch 31.9 var loss 1020142.5625 reconstruction mse 820916.6875 imputation mse 1.184739112854004\n",
      "Train Epoch 32.0 var loss 1004086.625 reconstruction mse 804889.0 imputation mse 1.1252689361572266\n",
      "Train Epoch 32.1 var loss 1008066.875 reconstruction mse 809830.625 imputation mse 1.186846375465393\n",
      "Train Epoch 32.2 var loss 1007214.0 reconstruction mse 809634.5 imputation mse 1.1878212690353394\n",
      "Train Epoch 32.3 var loss 1005032.625 reconstruction mse 808033.4375 imputation mse 1.186605453491211\n",
      "Train Epoch 32.4 var loss 1003953.9375 reconstruction mse 807171.0 imputation mse 1.1812390089035034\n",
      "Train Epoch 32.5 var loss 1002793.0625 reconstruction mse 807756.4375 imputation mse 1.1805721521377563\n",
      "Train Epoch 32.6 var loss 1003412.75 reconstruction mse 808264.25 imputation mse 1.1853086948394775\n",
      "Train Epoch 32.7 var loss 1001865.375 reconstruction mse 807603.625 imputation mse 1.1833795309066772\n",
      "Train Epoch 32.8 var loss 999570.375 reconstruction mse 806127.1875 imputation mse 1.1828221082687378\n",
      "Train Epoch 32.9 var loss 998578.75 reconstruction mse 804265.9375 imputation mse 1.1771689653396606\n",
      "Train Epoch 33.0 var loss 993181.4375 reconstruction mse 799376.875 imputation mse 1.1183924674987793\n",
      "Train Epoch 33.1 var loss 998799.4375 reconstruction mse 806135.125 imputation mse 1.1784085035324097\n",
      "Train Epoch 33.2 var loss 998408.6875 reconstruction mse 805503.625 imputation mse 1.1746277809143066\n",
      "Train Epoch 33.3 var loss 997549.625 reconstruction mse 804003.75 imputation mse 1.1710140705108643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 33.4 var loss 996675.25 reconstruction mse 804703.125 imputation mse 1.1753654479980469\n",
      "Train Epoch 33.5 var loss 996430.8125 reconstruction mse 804996.0 imputation mse 1.172358751296997\n",
      "Train Epoch 33.6 var loss 994830.9375 reconstruction mse 804130.75 imputation mse 1.1704347133636475\n",
      "Train Epoch 33.7 var loss 993231.25 reconstruction mse 803453.4375 imputation mse 1.1670758724212646\n",
      "Train Epoch 33.8 var loss 991984.5 reconstruction mse 802625.125 imputation mse 1.1730494499206543\n",
      "Train Epoch 33.9 var loss 990771.9375 reconstruction mse 801525.8125 imputation mse 1.1667917966842651\n",
      "Train Epoch 34.0 var loss 983571.375 reconstruction mse 794810.4375 imputation mse 1.1164820194244385\n",
      "Train Epoch 34.1 var loss 998865.6875 reconstruction mse 811655.1875 imputation mse 1.1773539781570435\n",
      "Train Epoch 34.2 var loss 998869.375 reconstruction mse 811644.5 imputation mse 1.1764949560165405\n",
      "Train Epoch 34.3 var loss 996570.1875 reconstruction mse 810733.0625 imputation mse 1.1770949363708496\n",
      "Train Epoch 34.4 var loss 997229.1875 reconstruction mse 809850.0 imputation mse 1.1740604639053345\n",
      "Train Epoch 34.5 var loss 994835.3125 reconstruction mse 809040.5 imputation mse 1.1727619171142578\n",
      "Train Epoch 34.6 var loss 994971.5 reconstruction mse 809275.625 imputation mse 1.1726343631744385\n",
      "Train Epoch 34.7 var loss 994591.1875 reconstruction mse 808841.4375 imputation mse 1.1744543313980103\n",
      "Train Epoch 34.8 var loss 992649.5 reconstruction mse 807974.4375 imputation mse 1.16972017288208\n",
      "Train Epoch 34.9 var loss 991569.3125 reconstruction mse 807756.375 imputation mse 1.1688811779022217\n",
      "Train Epoch 35.0 var loss 975637.375 reconstruction mse 792308.5 imputation mse 1.1024696826934814\n",
      "Train Epoch 35.1 var loss 983299.625 reconstruction mse 800003.6875 imputation mse 1.159415364265442\n",
      "Train Epoch 35.2 var loss 981237.25 reconstruction mse 798354.6875 imputation mse 1.155663013458252\n",
      "Train Epoch 35.3 var loss 981004.875 reconstruction mse 798622.9375 imputation mse 1.1543787717819214\n",
      "Train Epoch 35.4 var loss 979713.25 reconstruction mse 797003.25 imputation mse 1.1544647216796875\n",
      "Train Epoch 35.5 var loss 979717.125 reconstruction mse 798779.0 imputation mse 1.157504916191101\n",
      "Train Epoch 35.6 var loss 978339.4375 reconstruction mse 797494.125 imputation mse 1.1559717655181885\n",
      "Train Epoch 35.7 var loss 978453.875 reconstruction mse 797694.1875 imputation mse 1.1599106788635254\n",
      "Train Epoch 35.8 var loss 976756.375 reconstruction mse 797056.25 imputation mse 1.1580891609191895\n",
      "Train Epoch 35.9 var loss 978747.375 reconstruction mse 798561.3125 imputation mse 1.1558536291122437\n",
      "Train Epoch 36.0 var loss 968014.625 reconstruction mse 788484.875 imputation mse 1.1033822298049927\n",
      "Train Epoch 36.1 var loss 976501.0 reconstruction mse 797906.5625 imputation mse 1.158950924873352\n",
      "Train Epoch 36.2 var loss 974496.0625 reconstruction mse 797217.875 imputation mse 1.1590662002563477\n",
      "Train Epoch 36.3 var loss 975363.1875 reconstruction mse 796704.6875 imputation mse 1.15981924533844\n",
      "Train Epoch 36.4 var loss 973937.75 reconstruction mse 796372.3125 imputation mse 1.1581838130950928\n",
      "Train Epoch 36.5 var loss 972393.875 reconstruction mse 795379.4375 imputation mse 1.1540708541870117\n",
      "Train Epoch 36.6 var loss 971567.75 reconstruction mse 795343.1875 imputation mse 1.1540939807891846\n",
      "Train Epoch 36.7 var loss 971397.125 reconstruction mse 794700.375 imputation mse 1.1554216146469116\n",
      "Train Epoch 36.8 var loss 971088.625 reconstruction mse 795839.8125 imputation mse 1.1565067768096924\n",
      "Train Epoch 36.9 var loss 970893.6875 reconstruction mse 795486.1875 imputation mse 1.1521406173706055\n",
      "Train Epoch 37.0 var loss 960193.75 reconstruction mse 784490.625 imputation mse 1.1048729419708252\n",
      "Train Epoch 37.1 var loss 966914.6875 reconstruction mse 791491.9375 imputation mse 1.1558855772018433\n",
      "Train Epoch 37.2 var loss 964794.125 reconstruction mse 789133.6875 imputation mse 1.1472560167312622\n",
      "Train Epoch 37.3 var loss 965707.5625 reconstruction mse 791791.5625 imputation mse 1.152406930923462\n",
      "Train Epoch 37.4 var loss 962656.6875 reconstruction mse 789349.8125 imputation mse 1.1481425762176514\n",
      "Train Epoch 37.5 var loss 962799.1875 reconstruction mse 788282.0 imputation mse 1.1464219093322754\n",
      "Train Epoch 37.6 var loss 962117.5 reconstruction mse 788459.0 imputation mse 1.1505184173583984\n",
      "Train Epoch 37.7 var loss 960895.75 reconstruction mse 787922.375 imputation mse 1.1442251205444336\n",
      "Train Epoch 37.8 var loss 960665.75 reconstruction mse 788336.4375 imputation mse 1.147804617881775\n",
      "Train Epoch 37.9 var loss 960519.375 reconstruction mse 789047.8125 imputation mse 1.1489523649215698\n",
      "Train Epoch 38.0 var loss 950894.0 reconstruction mse 779135.375 imputation mse 1.09132719039917\n",
      "Train Epoch 38.1 var loss 966561.375 reconstruction mse 795651.8125 imputation mse 1.1469674110412598\n",
      "Train Epoch 38.2 var loss 963771.625 reconstruction mse 793047.625 imputation mse 1.1429245471954346\n",
      "Train Epoch 38.3 var loss 963105.9375 reconstruction mse 792872.3125 imputation mse 1.1429574489593506\n",
      "Train Epoch 38.4 var loss 962398.125 reconstruction mse 791490.5 imputation mse 1.1396340131759644\n",
      "Train Epoch 38.5 var loss 962646.125 reconstruction mse 792625.6875 imputation mse 1.1416107416152954\n",
      "Train Epoch 38.6 var loss 963034.875 reconstruction mse 793033.0 imputation mse 1.1417382955551147\n",
      "Train Epoch 38.7 var loss 961244.75 reconstruction mse 791666.3125 imputation mse 1.1425166130065918\n",
      "Train Epoch 38.8 var loss 960429.9375 reconstruction mse 792154.9375 imputation mse 1.1386693716049194\n",
      "Train Epoch 38.9 var loss 958527.0625 reconstruction mse 790213.0 imputation mse 1.138868808746338\n",
      "Train Epoch 39.0 var loss 944920.9375 reconstruction mse 776939.4375 imputation mse 1.0915051698684692\n",
      "Train Epoch 39.1 var loss 957136.9375 reconstruction mse 787736.75 imputation mse 1.1416692733764648\n",
      "Train Epoch 39.2 var loss 955319.8125 reconstruction mse 787392.125 imputation mse 1.1403586864471436\n",
      "Train Epoch 39.3 var loss 955384.5625 reconstruction mse 788664.0625 imputation mse 1.142285943031311\n",
      "Train Epoch 39.4 var loss 953697.9375 reconstruction mse 786655.0 imputation mse 1.1409682035446167\n",
      "Train Epoch 39.5 var loss 953471.0 reconstruction mse 787426.625 imputation mse 1.1422628164291382\n",
      "Train Epoch 39.6 var loss 951916.8125 reconstruction mse 785540.6875 imputation mse 1.1413639783859253\n",
      "Train Epoch 39.7 var loss 952986.8125 reconstruction mse 788137.875 imputation mse 1.1429282426834106\n",
      "Train Epoch 39.8 var loss 951082.3125 reconstruction mse 787367.1875 imputation mse 1.1411234140396118\n",
      "Train Epoch 39.9 var loss 950188.75 reconstruction mse 786506.3125 imputation mse 1.1404955387115479\n",
      "Train Epoch 40.0 var loss 940218.75 reconstruction mse 776588.6875 imputation mse 1.0899075269699097\n",
      "Train Epoch 40.1 var loss 951479.5 reconstruction mse 789005.75 imputation mse 1.1407567262649536\n",
      "Train Epoch 40.2 var loss 950293.75 reconstruction mse 788124.25 imputation mse 1.1412568092346191\n",
      "Train Epoch 40.3 var loss 948868.875 reconstruction mse 786172.8125 imputation mse 1.1338435411453247\n",
      "Train Epoch 40.4 var loss 948696.0 reconstruction mse 787010.9375 imputation mse 1.1364609003067017\n",
      "Train Epoch 40.5 var loss 949626.5 reconstruction mse 788042.6875 imputation mse 1.1400364637374878\n",
      "Train Epoch 40.6 var loss 948609.125 reconstruction mse 786271.8125 imputation mse 1.1367270946502686\n",
      "Train Epoch 40.7 var loss 947914.5625 reconstruction mse 786578.1875 imputation mse 1.135241985321045\n",
      "Train Epoch 40.8 var loss 945856.25 reconstruction mse 785662.125 imputation mse 1.1371338367462158\n",
      "Train Epoch 40.9 var loss 944909.625 reconstruction mse 784514.25 imputation mse 1.1319010257720947\n",
      "====> Test imputation mse: 1.04704106\n",
      "====> Test imputation mse: 1.04942620\n",
      "====> Test imputation mse: 1.04769933\n",
      "Train Epoch 41.0 var loss 934062.0 reconstruction mse 773366.5625 imputation mse 1.0849156379699707\n",
      "Train Epoch 41.1 var loss 942936.875 reconstruction mse 782968.8125 imputation mse 1.131730079650879\n",
      "Train Epoch 41.2 var loss 943801.5625 reconstruction mse 783934.0 imputation mse 1.1320664882659912\n",
      "Train Epoch 41.3 var loss 942401.9375 reconstruction mse 783642.8125 imputation mse 1.1324193477630615\n",
      "Train Epoch 41.4 var loss 941644.25 reconstruction mse 782873.4375 imputation mse 1.1322981119155884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 41.5 var loss 942303.375 reconstruction mse 784465.5 imputation mse 1.1354724168777466\n",
      "Train Epoch 41.6 var loss 941625.375 reconstruction mse 783724.5 imputation mse 1.1309090852737427\n",
      "Train Epoch 41.7 var loss 940992.6875 reconstruction mse 783963.25 imputation mse 1.1327792406082153\n",
      "Train Epoch 41.8 var loss 938116.125 reconstruction mse 780281.0 imputation mse 1.1288317441940308\n",
      "Train Epoch 41.9 var loss 938811.875 reconstruction mse 781393.1875 imputation mse 1.127135992050171\n",
      "Train Epoch 42.0 var loss 927231.75 reconstruction mse 770483.125 imputation mse 1.0805331468582153\n",
      "Train Epoch 42.1 var loss 933489.75 reconstruction mse 777872.375 imputation mse 1.1264326572418213\n",
      "Train Epoch 42.2 var loss 933006.4375 reconstruction mse 777480.3125 imputation mse 1.1238800287246704\n",
      "Train Epoch 42.3 var loss 934107.5 reconstruction mse 778213.875 imputation mse 1.1255552768707275\n",
      "Train Epoch 42.4 var loss 931211.8125 reconstruction mse 776814.8125 imputation mse 1.1239981651306152\n",
      "Train Epoch 42.5 var loss 933553.5625 reconstruction mse 778029.0 imputation mse 1.1275819540023804\n",
      "Train Epoch 42.6 var loss 932068.25 reconstruction mse 777442.8125 imputation mse 1.1265759468078613\n",
      "Train Epoch 42.7 var loss 930896.375 reconstruction mse 776770.0 imputation mse 1.123949646949768\n",
      "Train Epoch 42.8 var loss 929804.125 reconstruction mse 777204.875 imputation mse 1.1274349689483643\n",
      "Train Epoch 42.9 var loss 928518.875 reconstruction mse 775312.9375 imputation mse 1.1215245723724365\n",
      "Train Epoch 43.0 var loss 923650.4375 reconstruction mse 769890.625 imputation mse 1.077871322631836\n",
      "Train Epoch 43.1 var loss 927516.875 reconstruction mse 774012.875 imputation mse 1.1211631298065186\n",
      "Train Epoch 43.2 var loss 927161.4375 reconstruction mse 774247.5625 imputation mse 1.1212513446807861\n",
      "Train Epoch 43.3 var loss 925670.6875 reconstruction mse 773966.625 imputation mse 1.1263827085494995\n",
      "Train Epoch 43.4 var loss 927266.5625 reconstruction mse 774435.6875 imputation mse 1.1225417852401733\n",
      "Train Epoch 43.5 var loss 924680.25 reconstruction mse 773160.1875 imputation mse 1.1219016313552856\n",
      "Train Epoch 43.6 var loss 924098.0625 reconstruction mse 772401.5 imputation mse 1.1218688488006592\n",
      "Train Epoch 43.7 var loss 924600.125 reconstruction mse 773470.4375 imputation mse 1.1200257539749146\n",
      "Train Epoch 43.8 var loss 923761.75 reconstruction mse 772905.4375 imputation mse 1.1250481605529785\n",
      "Train Epoch 43.9 var loss 922853.5625 reconstruction mse 772873.625 imputation mse 1.1229329109191895\n",
      "Train Epoch 44.0 var loss 918048.25 reconstruction mse 767702.625 imputation mse 1.0703071355819702\n",
      "Train Epoch 44.1 var loss 920782.125 reconstruction mse 771234.4375 imputation mse 1.1184780597686768\n",
      "Train Epoch 44.2 var loss 920154.875 reconstruction mse 771170.1875 imputation mse 1.1174840927124023\n",
      "Train Epoch 44.3 var loss 920183.4375 reconstruction mse 770804.8125 imputation mse 1.118630290031433\n",
      "Train Epoch 44.4 var loss 919427.5 reconstruction mse 771047.875 imputation mse 1.1153298616409302\n",
      "Train Epoch 44.5 var loss 918531.125 reconstruction mse 770094.625 imputation mse 1.1196016073226929\n",
      "Train Epoch 44.6 var loss 918955.375 reconstruction mse 770751.8125 imputation mse 1.1180777549743652\n",
      "Train Epoch 44.7 var loss 917763.25 reconstruction mse 769160.5 imputation mse 1.1147501468658447\n",
      "Train Epoch 44.8 var loss 917342.125 reconstruction mse 769939.125 imputation mse 1.1155651807785034\n",
      "Train Epoch 44.9 var loss 916634.125 reconstruction mse 769032.75 imputation mse 1.1150976419448853\n",
      "Train Epoch 45.0 var loss 909608.875 reconstruction mse 763094.75 imputation mse 1.066649079322815\n",
      "Train Epoch 45.1 var loss 914340.375 reconstruction mse 767579.0 imputation mse 1.110757827758789\n",
      "Train Epoch 45.2 var loss 913807.75 reconstruction mse 767175.375 imputation mse 1.1144534349441528\n",
      "Train Epoch 45.3 var loss 911682.875 reconstruction mse 766252.0625 imputation mse 1.1110090017318726\n",
      "Train Epoch 45.4 var loss 912443.3125 reconstruction mse 767164.125 imputation mse 1.115132451057434\n",
      "Train Epoch 45.5 var loss 913603.5 reconstruction mse 767710.6875 imputation mse 1.1136596202850342\n",
      "Train Epoch 45.6 var loss 912039.125 reconstruction mse 766738.875 imputation mse 1.1135749816894531\n",
      "Train Epoch 45.7 var loss 910796.8125 reconstruction mse 765911.625 imputation mse 1.1094657182693481\n",
      "Train Epoch 45.8 var loss 911191.6875 reconstruction mse 765691.0 imputation mse 1.1121059656143188\n",
      "Train Epoch 45.9 var loss 910765.25 reconstruction mse 766491.75 imputation mse 1.1104530096054077\n",
      "Train Epoch 46.0 var loss 904852.375 reconstruction mse 761025.9375 imputation mse 1.0686235427856445\n",
      "Train Epoch 46.1 var loss 917867.875 reconstruction mse 773435.9375 imputation mse 1.1123450994491577\n",
      "Train Epoch 46.2 var loss 918325.5625 reconstruction mse 773737.0625 imputation mse 1.1140685081481934\n",
      "Train Epoch 46.3 var loss 915820.75 reconstruction mse 771204.125 imputation mse 1.1103250980377197\n",
      "Train Epoch 46.4 var loss 915851.4375 reconstruction mse 772558.75 imputation mse 1.1124101877212524\n",
      "Train Epoch 46.5 var loss 916332.625 reconstruction mse 773338.4375 imputation mse 1.1141103506088257\n",
      "Train Epoch 46.6 var loss 914463.5625 reconstruction mse 771246.5625 imputation mse 1.1076056957244873\n",
      "Train Epoch 46.7 var loss 916378.125 reconstruction mse 773192.25 imputation mse 1.114946722984314\n",
      "Train Epoch 46.8 var loss 914792.0 reconstruction mse 772158.25 imputation mse 1.1103527545928955\n",
      "Train Epoch 46.9 var loss 914062.375 reconstruction mse 771709.625 imputation mse 1.1104741096496582\n",
      "Train Epoch 47.0 var loss 901768.125 reconstruction mse 759915.625 imputation mse 1.0626459121704102\n",
      "Train Epoch 47.1 var loss 909683.3125 reconstruction mse 769041.25 imputation mse 1.1086125373840332\n",
      "Train Epoch 47.2 var loss 909039.4375 reconstruction mse 768447.9375 imputation mse 1.1081585884094238\n",
      "Train Epoch 47.3 var loss 910659.6875 reconstruction mse 769602.625 imputation mse 1.1110515594482422\n",
      "Train Epoch 47.4 var loss 908758.9375 reconstruction mse 767875.9375 imputation mse 1.107347011566162\n",
      "Train Epoch 47.5 var loss 908566.4375 reconstruction mse 767725.5625 imputation mse 1.10457181930542\n",
      "Train Epoch 47.6 var loss 907527.9375 reconstruction mse 766425.75 imputation mse 1.105299711227417\n",
      "Train Epoch 47.7 var loss 906914.625 reconstruction mse 767471.9375 imputation mse 1.1065826416015625\n",
      "Train Epoch 47.8 var loss 907163.5 reconstruction mse 767323.0625 imputation mse 1.1066844463348389\n",
      "Train Epoch 47.9 var loss 905591.125 reconstruction mse 767207.375 imputation mse 1.1037614345550537\n",
      "Train Epoch 48.0 var loss 896894.5 reconstruction mse 757943.125 imputation mse 1.0609415769577026\n",
      "Train Epoch 48.1 var loss 902237.75 reconstruction mse 762929.4375 imputation mse 1.1012855768203735\n",
      "Train Epoch 48.2 var loss 900747.875 reconstruction mse 762553.9375 imputation mse 1.1015369892120361\n",
      "Train Epoch 48.3 var loss 901508.0625 reconstruction mse 763391.5 imputation mse 1.1012789011001587\n",
      "Train Epoch 48.4 var loss 901126.125 reconstruction mse 763022.375 imputation mse 1.1025851964950562\n",
      "Train Epoch 48.5 var loss 900243.625 reconstruction mse 762203.375 imputation mse 1.0989793539047241\n",
      "Train Epoch 48.6 var loss 900001.1875 reconstruction mse 762054.6875 imputation mse 1.1004887819290161\n",
      "Train Epoch 48.7 var loss 899997.25 reconstruction mse 763067.9375 imputation mse 1.0989079475402832\n",
      "Train Epoch 48.8 var loss 899069.8125 reconstruction mse 762508.75 imputation mse 1.1007311344146729\n",
      "Train Epoch 48.9 var loss 899907.375 reconstruction mse 762778.4375 imputation mse 1.1009501218795776\n",
      "Train Epoch 49.0 var loss 895364.875 reconstruction mse 758809.0 imputation mse 1.0654560327529907\n",
      "Train Epoch 49.1 var loss 903222.625 reconstruction mse 767305.9375 imputation mse 1.1067757606506348\n",
      "Train Epoch 49.2 var loss 902427.3125 reconstruction mse 766189.8125 imputation mse 1.1050758361816406\n",
      "Train Epoch 49.3 var loss 901984.375 reconstruction mse 765985.875 imputation mse 1.105972170829773\n",
      "Train Epoch 49.4 var loss 901866.75 reconstruction mse 766479.75 imputation mse 1.1060593128204346\n",
      "Train Epoch 49.5 var loss 901013.375 reconstruction mse 765303.625 imputation mse 1.1047831773757935\n",
      "Train Epoch 49.6 var loss 902094.5 reconstruction mse 766143.5625 imputation mse 1.1060841083526611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 49.7 var loss 900325.0625 reconstruction mse 765085.0625 imputation mse 1.1051198244094849\n",
      "Train Epoch 49.8 var loss 899619.25 reconstruction mse 765042.1875 imputation mse 1.1064085960388184\n",
      "Train Epoch 49.9 var loss 900286.4375 reconstruction mse 764816.3125 imputation mse 1.1024872064590454\n",
      "Train Epoch 50.0 var loss 891172.9375 reconstruction mse 757933.8125 imputation mse 1.0624866485595703\n",
      "Train Epoch 50.1 var loss 898966.875 reconstruction mse 765150.375 imputation mse 1.100900650024414\n",
      "Train Epoch 50.2 var loss 897357.5625 reconstruction mse 764090.5625 imputation mse 1.0997148752212524\n",
      "Train Epoch 50.3 var loss 897419.625 reconstruction mse 764477.6875 imputation mse 1.1005408763885498\n",
      "Train Epoch 50.4 var loss 897585.9375 reconstruction mse 764730.6875 imputation mse 1.1014430522918701\n",
      "Train Epoch 50.5 var loss 897273.8125 reconstruction mse 765155.25 imputation mse 1.101602554321289\n",
      "Train Epoch 50.6 var loss 897449.5 reconstruction mse 764913.0625 imputation mse 1.102318525314331\n",
      "Train Epoch 50.7 var loss 897882.5625 reconstruction mse 765335.3125 imputation mse 1.1043944358825684\n",
      "Train Epoch 50.8 var loss 896429.5 reconstruction mse 764133.4375 imputation mse 1.1011141538619995\n",
      "Train Epoch 50.9 var loss 895517.25 reconstruction mse 764174.125 imputation mse 1.1010000705718994\n",
      "====> Test imputation mse: 1.04672146\n",
      "====> Test imputation mse: 1.04622889\n",
      "====> Test imputation mse: 1.04865336\n",
      "Train Epoch 51.0 var loss 885695.875 reconstruction mse 754865.5 imputation mse 1.0584847927093506\n",
      "Train Epoch 51.1 var loss 896235.125 reconstruction mse 765164.8125 imputation mse 1.1015220880508423\n",
      "Train Epoch 51.2 var loss 895691.3125 reconstruction mse 765915.8125 imputation mse 1.1016002893447876\n",
      "Train Epoch 51.3 var loss 894232.75 reconstruction mse 763734.8125 imputation mse 1.0960601568222046\n",
      "Train Epoch 51.4 var loss 894236.3125 reconstruction mse 763857.1875 imputation mse 1.0979349613189697\n",
      "Train Epoch 51.5 var loss 893360.6875 reconstruction mse 763504.25 imputation mse 1.0986824035644531\n",
      "Train Epoch 51.6 var loss 893400.0625 reconstruction mse 762895.3125 imputation mse 1.0958266258239746\n",
      "Train Epoch 51.7 var loss 892793.8125 reconstruction mse 763435.1875 imputation mse 1.0972121953964233\n",
      "Train Epoch 51.8 var loss 892764.375 reconstruction mse 763628.1875 imputation mse 1.096360445022583\n",
      "Train Epoch 51.9 var loss 892701.0625 reconstruction mse 763922.4375 imputation mse 1.0988303422927856\n",
      "Train Epoch 52.0 var loss 882710.0 reconstruction mse 753607.25 imputation mse 1.0561262369155884\n",
      "Train Epoch 52.1 var loss 889524.375 reconstruction mse 760838.5 imputation mse 1.0946744680404663\n",
      "Train Epoch 52.2 var loss 889404.6875 reconstruction mse 759754.5625 imputation mse 1.093537449836731\n",
      "Train Epoch 52.3 var loss 888605.9375 reconstruction mse 759868.1875 imputation mse 1.092509388923645\n",
      "Train Epoch 52.4 var loss 889343.375 reconstruction mse 761141.125 imputation mse 1.0938433408737183\n",
      "Train Epoch 52.5 var loss 888768.375 reconstruction mse 761111.875 imputation mse 1.0929845571517944\n",
      "Train Epoch 52.6 var loss 888202.75 reconstruction mse 760862.25 imputation mse 1.0932663679122925\n",
      "Train Epoch 52.7 var loss 886806.5 reconstruction mse 759909.5 imputation mse 1.090842366218567\n",
      "Train Epoch 52.8 var loss 887277.875 reconstruction mse 760380.375 imputation mse 1.092515230178833\n",
      "Train Epoch 52.9 var loss 885599.9375 reconstruction mse 758790.0 imputation mse 1.0900179147720337\n",
      "Train Epoch 53.0 var loss 877995.0 reconstruction mse 751852.0 imputation mse 1.0510804653167725\n",
      "Train Epoch 53.1 var loss 888308.6875 reconstruction mse 761055.3125 imputation mse 1.0919606685638428\n",
      "Train Epoch 53.2 var loss 888548.0 reconstruction mse 761161.5625 imputation mse 1.0908353328704834\n",
      "Train Epoch 53.3 var loss 886803.6875 reconstruction mse 760031.375 imputation mse 1.0897287130355835\n",
      "Train Epoch 53.4 var loss 888421.625 reconstruction mse 760865.875 imputation mse 1.092881679534912\n",
      "Train Epoch 53.5 var loss 886388.4375 reconstruction mse 759774.3125 imputation mse 1.0880775451660156\n",
      "Train Epoch 53.6 var loss 886504.875 reconstruction mse 760379.3125 imputation mse 1.089727759361267\n",
      "Train Epoch 53.7 var loss 885843.375 reconstruction mse 759920.1875 imputation mse 1.0873949527740479\n",
      "Train Epoch 53.8 var loss 885728.25 reconstruction mse 759596.75 imputation mse 1.087378740310669\n",
      "Train Epoch 53.9 var loss 885562.75 reconstruction mse 760522.6875 imputation mse 1.0882307291030884\n",
      "Train Epoch 54.0 var loss 877234.625 reconstruction mse 751942.75 imputation mse 1.0544087886810303\n",
      "Train Epoch 54.1 var loss 883385.0 reconstruction mse 758329.3125 imputation mse 1.0886369943618774\n",
      "Train Epoch 54.2 var loss 883753.125 reconstruction mse 759888.6875 imputation mse 1.090476632118225\n",
      "Train Epoch 54.3 var loss 884008.875 reconstruction mse 758638.1875 imputation mse 1.0892904996871948\n",
      "Train Epoch 54.4 var loss 884599.0625 reconstruction mse 759561.6875 imputation mse 1.0914514064788818\n",
      "Train Epoch 54.5 var loss 883068.6875 reconstruction mse 758203.875 imputation mse 1.0907291173934937\n",
      "Train Epoch 54.6 var loss 881712.0625 reconstruction mse 757761.5625 imputation mse 1.0869450569152832\n",
      "Train Epoch 54.7 var loss 881506.0 reconstruction mse 758522.875 imputation mse 1.0904985666275024\n",
      "Train Epoch 54.8 var loss 881080.0625 reconstruction mse 757567.3125 imputation mse 1.0879552364349365\n",
      "Train Epoch 54.9 var loss 881824.4375 reconstruction mse 758500.3125 imputation mse 1.0900797843933105\n",
      "Train Epoch 55.0 var loss 873560.0 reconstruction mse 750797.6875 imputation mse 1.0534965991973877\n",
      "Train Epoch 55.1 var loss 880046.0 reconstruction mse 757694.75 imputation mse 1.0900049209594727\n",
      "Train Epoch 55.2 var loss 880863.9375 reconstruction mse 758390.5625 imputation mse 1.090151309967041\n",
      "Train Epoch 55.3 var loss 879151.375 reconstruction mse 757302.125 imputation mse 1.0872141122817993\n",
      "Train Epoch 55.4 var loss 878995.875 reconstruction mse 757538.625 imputation mse 1.08711838722229\n",
      "Train Epoch 55.5 var loss 881286.8125 reconstruction mse 759007.125 imputation mse 1.0886682271957397\n",
      "Train Epoch 55.6 var loss 877172.25 reconstruction mse 755988.4375 imputation mse 1.0860910415649414\n",
      "Train Epoch 55.7 var loss 878423.375 reconstruction mse 756461.5625 imputation mse 1.0865490436553955\n",
      "Train Epoch 55.8 var loss 878944.9375 reconstruction mse 757741.4375 imputation mse 1.0878452062606812\n",
      "Train Epoch 55.9 var loss 878173.4375 reconstruction mse 757316.0625 imputation mse 1.0876786708831787\n",
      "Train Epoch 56.0 var loss 870888.25 reconstruction mse 749777.3125 imputation mse 1.0524612665176392\n",
      "Train Epoch 56.1 var loss 876823.125 reconstruction mse 755738.6875 imputation mse 1.0885006189346313\n",
      "Train Epoch 56.2 var loss 877336.4375 reconstruction mse 756103.375 imputation mse 1.092307686805725\n",
      "Train Epoch 56.3 var loss 876106.25 reconstruction mse 755254.125 imputation mse 1.0880765914916992\n",
      "Train Epoch 56.4 var loss 877023.5 reconstruction mse 756092.25 imputation mse 1.090779185295105\n",
      "Train Epoch 56.5 var loss 875929.6875 reconstruction mse 755903.0 imputation mse 1.088435173034668\n",
      "Train Epoch 56.6 var loss 875512.875 reconstruction mse 755297.25 imputation mse 1.0882211923599243\n",
      "Train Epoch 56.7 var loss 875794.75 reconstruction mse 755729.625 imputation mse 1.0893661975860596\n",
      "Train Epoch 56.8 var loss 875658.5625 reconstruction mse 756189.75 imputation mse 1.087957739830017\n",
      "Train Epoch 56.9 var loss 873889.9375 reconstruction mse 754534.125 imputation mse 1.084152102470398\n",
      "Train Epoch 57.0 var loss 867052.0 reconstruction mse 748727.5625 imputation mse 1.0485137701034546\n",
      "Train Epoch 57.1 var loss 873047.75 reconstruction mse 752967.875 imputation mse 1.084874153137207\n",
      "Train Epoch 57.2 var loss 871640.8125 reconstruction mse 753215.5 imputation mse 1.082741379737854\n",
      "Train Epoch 57.3 var loss 871084.0625 reconstruction mse 752812.625 imputation mse 1.0838667154312134\n",
      "Train Epoch 57.4 var loss 870458.625 reconstruction mse 752550.125 imputation mse 1.0815616846084595\n",
      "Train Epoch 57.5 var loss 871029.625 reconstruction mse 752454.9375 imputation mse 1.0847103595733643\n",
      "Train Epoch 57.6 var loss 870557.125 reconstruction mse 752463.1875 imputation mse 1.0871186256408691\n",
      "Train Epoch 57.7 var loss 870587.5 reconstruction mse 752649.6875 imputation mse 1.0842077732086182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 57.8 var loss 869064.5625 reconstruction mse 751569.75 imputation mse 1.0788382291793823\n",
      "Train Epoch 57.9 var loss 868346.9375 reconstruction mse 750841.875 imputation mse 1.0833985805511475\n",
      "Train Epoch 58.0 var loss 864124.4375 reconstruction mse 747423.375 imputation mse 1.0354877710342407\n",
      "Train Epoch 58.1 var loss 867713.4375 reconstruction mse 750806.125 imputation mse 1.0734562873840332\n",
      "Train Epoch 58.2 var loss 865329.4375 reconstruction mse 749385.625 imputation mse 1.0743733644485474\n",
      "Train Epoch 58.3 var loss 865557.25 reconstruction mse 749405.8125 imputation mse 1.071852684020996\n",
      "Train Epoch 58.4 var loss 864758.875 reconstruction mse 749357.1875 imputation mse 1.0720617771148682\n",
      "Train Epoch 58.5 var loss 864409.625 reconstruction mse 748098.375 imputation mse 1.0682651996612549\n",
      "Train Epoch 58.6 var loss 864844.6875 reconstruction mse 749326.3125 imputation mse 1.0747387409210205\n",
      "Train Epoch 58.7 var loss 864786.375 reconstruction mse 749380.0 imputation mse 1.0706185102462769\n",
      "Train Epoch 58.8 var loss 863839.8125 reconstruction mse 748480.125 imputation mse 1.0711326599121094\n",
      "Train Epoch 58.9 var loss 863744.25 reconstruction mse 749066.3125 imputation mse 1.073279857635498\n",
      "Train Epoch 59.0 var loss 860890.625 reconstruction mse 745661.1875 imputation mse 1.0448750257492065\n",
      "Train Epoch 59.1 var loss 864695.3125 reconstruction mse 750339.1875 imputation mse 1.0789662599563599\n",
      "Train Epoch 59.2 var loss 864044.0625 reconstruction mse 750254.6875 imputation mse 1.0772844552993774\n",
      "Train Epoch 59.3 var loss 862884.75 reconstruction mse 748359.625 imputation mse 1.0745348930358887\n",
      "Train Epoch 59.4 var loss 863146.25 reconstruction mse 749340.3125 imputation mse 1.0764493942260742\n",
      "Train Epoch 59.5 var loss 861741.9375 reconstruction mse 747870.625 imputation mse 1.0756949186325073\n",
      "Train Epoch 59.6 var loss 863611.6875 reconstruction mse 749163.375 imputation mse 1.0766972303390503\n",
      "Train Epoch 59.7 var loss 863250.25 reconstruction mse 749044.5625 imputation mse 1.0754154920578003\n",
      "Train Epoch 59.8 var loss 861822.875 reconstruction mse 748800.9375 imputation mse 1.074994444847107\n",
      "Train Epoch 59.9 var loss 862545.875 reconstruction mse 749361.5625 imputation mse 1.0771212577819824\n",
      "Train Epoch 60.0 var loss 858205.0625 reconstruction mse 744731.875 imputation mse 1.044100284576416\n",
      "Train Epoch 60.1 var loss 865884.875 reconstruction mse 753174.4375 imputation mse 1.0781664848327637\n",
      "Train Epoch 60.2 var loss 867497.75 reconstruction mse 753833.25 imputation mse 1.077911376953125\n",
      "Train Epoch 60.3 var loss 865866.25 reconstruction mse 751529.9375 imputation mse 1.0750113725662231\n",
      "Train Epoch 60.4 var loss 865480.1875 reconstruction mse 752556.0625 imputation mse 1.0765180587768555\n",
      "Train Epoch 60.5 var loss 864961.1875 reconstruction mse 752058.9375 imputation mse 1.0767614841461182\n",
      "Train Epoch 60.6 var loss 865549.6875 reconstruction mse 751665.625 imputation mse 1.074985384941101\n",
      "Train Epoch 60.7 var loss 864652.75 reconstruction mse 751865.3125 imputation mse 1.0764319896697998\n",
      "Train Epoch 60.8 var loss 862302.4375 reconstruction mse 750689.0 imputation mse 1.0745360851287842\n",
      "Train Epoch 60.9 var loss 863604.125 reconstruction mse 750550.0625 imputation mse 1.0726075172424316\n",
      "====> Test imputation mse: 1.03465271\n",
      "====> Test imputation mse: 1.04347849\n",
      "====> Test imputation mse: 1.04480338\n",
      "Train Epoch 61.0 var loss 855410.0625 reconstruction mse 742697.3125 imputation mse 1.0373889207839966\n",
      "Train Epoch 61.1 var loss 862450.3125 reconstruction mse 749735.25 imputation mse 1.0713353157043457\n",
      "Train Epoch 61.2 var loss 861269.0625 reconstruction mse 750292.375 imputation mse 1.0720373392105103\n",
      "Train Epoch 61.3 var loss 861860.1875 reconstruction mse 749994.375 imputation mse 1.072525143623352\n",
      "Train Epoch 61.4 var loss 860404.8125 reconstruction mse 749264.25 imputation mse 1.069254755973816\n",
      "Train Epoch 61.5 var loss 861053.6875 reconstruction mse 749243.3125 imputation mse 1.0716421604156494\n",
      "Train Epoch 61.6 var loss 860153.5 reconstruction mse 749452.4375 imputation mse 1.0722824335098267\n",
      "Train Epoch 61.7 var loss 860890.75 reconstruction mse 749099.25 imputation mse 1.0703823566436768\n",
      "Train Epoch 61.8 var loss 859341.6875 reconstruction mse 748657.25 imputation mse 1.0704174041748047\n",
      "Train Epoch 61.9 var loss 859896.1875 reconstruction mse 749318.25 imputation mse 1.0703459978103638\n",
      "Train Epoch 62.0 var loss 854452.0 reconstruction mse 743408.75 imputation mse 1.039232611656189\n",
      "Train Epoch 62.1 var loss 856614.5625 reconstruction mse 746126.3125 imputation mse 1.0752284526824951\n",
      "Train Epoch 62.2 var loss 855677.75 reconstruction mse 745374.375 imputation mse 1.070932388305664\n",
      "Train Epoch 62.3 var loss 856243.8125 reconstruction mse 745899.25 imputation mse 1.0716997385025024\n",
      "Train Epoch 62.4 var loss 854715.4375 reconstruction mse 744837.4375 imputation mse 1.0742987394332886\n",
      "Train Epoch 62.5 var loss 855740.5 reconstruction mse 745524.625 imputation mse 1.068985939025879\n",
      "Train Epoch 62.6 var loss 856020.9375 reconstruction mse 746220.8125 imputation mse 1.072295069694519\n",
      "Train Epoch 62.7 var loss 853902.375 reconstruction mse 744892.5625 imputation mse 1.0691447257995605\n",
      "Train Epoch 62.8 var loss 855574.125 reconstruction mse 746225.375 imputation mse 1.073428988456726\n",
      "Train Epoch 62.9 var loss 855075.625 reconstruction mse 746017.3125 imputation mse 1.073509693145752\n",
      "Train Epoch 63.0 var loss 850971.25 reconstruction mse 741046.75 imputation mse 1.0358729362487793\n",
      "Train Epoch 63.1 var loss 856476.1875 reconstruction mse 747581.625 imputation mse 1.0718778371810913\n",
      "Train Epoch 63.2 var loss 856217.75 reconstruction mse 747635.875 imputation mse 1.0713614225387573\n",
      "Train Epoch 63.3 var loss 857101.75 reconstruction mse 747201.625 imputation mse 1.0724748373031616\n",
      "Train Epoch 63.4 var loss 855946.0625 reconstruction mse 747049.0625 imputation mse 1.0715831518173218\n",
      "Train Epoch 63.5 var loss 855792.5 reconstruction mse 747201.3125 imputation mse 1.0707119703292847\n",
      "Train Epoch 63.6 var loss 855305.4375 reconstruction mse 747157.1875 imputation mse 1.0712546110153198\n",
      "Train Epoch 63.7 var loss 854836.5 reconstruction mse 746418.8125 imputation mse 1.0705066919326782\n",
      "Train Epoch 63.8 var loss 856268.6875 reconstruction mse 747742.1875 imputation mse 1.0720443725585938\n",
      "Train Epoch 63.9 var loss 854843.0 reconstruction mse 746948.375 imputation mse 1.069596290588379\n",
      "Train Epoch 64.0 var loss 850013.5625 reconstruction mse 742177.1875 imputation mse 1.0413228273391724\n",
      "Train Epoch 64.1 var loss 855973.9375 reconstruction mse 748442.5 imputation mse 1.0698716640472412\n",
      "Train Epoch 64.2 var loss 855613.4375 reconstruction mse 747537.375 imputation mse 1.0673410892486572\n",
      "Train Epoch 64.3 var loss 855668.5 reconstruction mse 748453.3125 imputation mse 1.0709202289581299\n",
      "Train Epoch 64.4 var loss 856078.5 reconstruction mse 748162.125 imputation mse 1.0696539878845215\n",
      "Train Epoch 64.5 var loss 854257.75 reconstruction mse 747583.3125 imputation mse 1.067542552947998\n",
      "Train Epoch 64.6 var loss 855577.75 reconstruction mse 747860.625 imputation mse 1.0705835819244385\n",
      "Train Epoch 64.7 var loss 854496.125 reconstruction mse 747796.375 imputation mse 1.0688750743865967\n",
      "Train Epoch 64.8 var loss 854136.0 reconstruction mse 747850.75 imputation mse 1.0689963102340698\n",
      "Train Epoch 64.9 var loss 852602.0625 reconstruction mse 746950.375 imputation mse 1.0671919584274292\n",
      "Train Epoch 65.0 var loss 847975.3125 reconstruction mse 741765.6875 imputation mse 1.0383480787277222\n",
      "Train Epoch 65.1 var loss 852878.625 reconstruction mse 746716.125 imputation mse 1.0725041627883911\n",
      "Train Epoch 65.2 var loss 853580.125 reconstruction mse 747054.875 imputation mse 1.0713151693344116\n",
      "Train Epoch 65.3 var loss 852670.0625 reconstruction mse 747062.5 imputation mse 1.0725138187408447\n",
      "Train Epoch 65.4 var loss 851756.1875 reconstruction mse 745711.25 imputation mse 1.071293592453003\n",
      "Train Epoch 65.5 var loss 851953.1875 reconstruction mse 745763.8125 imputation mse 1.0700980424880981\n",
      "Train Epoch 65.6 var loss 851249.8125 reconstruction mse 745787.5625 imputation mse 1.0694462060928345\n",
      "Train Epoch 65.7 var loss 851239.75 reconstruction mse 746170.4375 imputation mse 1.0711249113082886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 65.8 var loss 850805.6875 reconstruction mse 746163.875 imputation mse 1.07262122631073\n",
      "Train Epoch 65.9 var loss 851240.8125 reconstruction mse 745948.375 imputation mse 1.071939468383789\n",
      "Train Epoch 66.0 var loss 846111.875 reconstruction mse 741461.4375 imputation mse 1.040862798690796\n",
      "Train Epoch 66.1 var loss 848971.0 reconstruction mse 744181.875 imputation mse 1.0760189294815063\n",
      "Train Epoch 66.2 var loss 849134.1875 reconstruction mse 744476.0 imputation mse 1.0756704807281494\n",
      "Train Epoch 66.3 var loss 848239.625 reconstruction mse 744020.125 imputation mse 1.0732688903808594\n",
      "Train Epoch 66.4 var loss 847448.1875 reconstruction mse 743363.4375 imputation mse 1.07338285446167\n",
      "Train Epoch 66.5 var loss 848721.625 reconstruction mse 744456.3125 imputation mse 1.0747520923614502\n",
      "Train Epoch 66.6 var loss 848259.125 reconstruction mse 744863.25 imputation mse 1.0748871564865112\n",
      "Train Epoch 66.7 var loss 847344.25 reconstruction mse 743433.25 imputation mse 1.0718382596969604\n",
      "Train Epoch 66.8 var loss 849213.25 reconstruction mse 744607.3125 imputation mse 1.0733457803726196\n",
      "Train Epoch 66.9 var loss 848703.625 reconstruction mse 744832.625 imputation mse 1.0731664896011353\n",
      "Train Epoch 67.0 var loss 843192.4375 reconstruction mse 740554.6875 imputation mse 1.0361542701721191\n",
      "Train Epoch 67.1 var loss 847021.8125 reconstruction mse 744303.625 imputation mse 1.0654114484786987\n",
      "Train Epoch 67.2 var loss 846606.625 reconstruction mse 744498.1875 imputation mse 1.0662763118743896\n",
      "Train Epoch 67.3 var loss 846378.6875 reconstruction mse 743565.75 imputation mse 1.0680922269821167\n",
      "Train Epoch 67.4 var loss 846635.5 reconstruction mse 743754.8125 imputation mse 1.065948247909546\n",
      "Train Epoch 67.5 var loss 846850.375 reconstruction mse 743629.875 imputation mse 1.0652861595153809\n",
      "Train Epoch 67.6 var loss 845531.0625 reconstruction mse 743371.3125 imputation mse 1.0652639865875244\n",
      "Train Epoch 67.7 var loss 845767.5625 reconstruction mse 743841.3125 imputation mse 1.0653594732284546\n",
      "Train Epoch 67.8 var loss 845587.75 reconstruction mse 744462.3125 imputation mse 1.0695393085479736\n",
      "Train Epoch 67.9 var loss 845649.9375 reconstruction mse 744226.0 imputation mse 1.0711520910263062\n",
      "Train Epoch 68.0 var loss 840462.5 reconstruction mse 739126.1875 imputation mse 1.0356979370117188\n",
      "Train Epoch 68.1 var loss 848753.75 reconstruction mse 746394.0 imputation mse 1.067063808441162\n",
      "Train Epoch 68.2 var loss 847845.4375 reconstruction mse 747118.5 imputation mse 1.0662989616394043\n",
      "Train Epoch 68.3 var loss 846879.8125 reconstruction mse 746133.8125 imputation mse 1.0658773183822632\n",
      "Train Epoch 68.4 var loss 847877.4375 reconstruction mse 746139.625 imputation mse 1.0662974119186401\n",
      "Train Epoch 68.5 var loss 848030.25 reconstruction mse 746230.0625 imputation mse 1.0662716627120972\n",
      "Train Epoch 68.6 var loss 847094.1875 reconstruction mse 745584.375 imputation mse 1.0645546913146973\n",
      "Train Epoch 68.7 var loss 845946.6875 reconstruction mse 744994.1875 imputation mse 1.063299536705017\n",
      "Train Epoch 68.8 var loss 847727.5625 reconstruction mse 745981.0 imputation mse 1.0651521682739258\n",
      "Train Epoch 68.9 var loss 846109.6875 reconstruction mse 745609.625 imputation mse 1.0653513669967651\n",
      "Train Epoch 69.0 var loss 839636.3125 reconstruction mse 739459.8125 imputation mse 1.0334587097167969\n",
      "Train Epoch 69.1 var loss 844881.625 reconstruction mse 744714.8125 imputation mse 1.0680443048477173\n",
      "Train Epoch 69.2 var loss 844497.6875 reconstruction mse 743944.0 imputation mse 1.066124677658081\n",
      "Train Epoch 69.3 var loss 844037.75 reconstruction mse 744714.5625 imputation mse 1.0662052631378174\n",
      "Train Epoch 69.4 var loss 843654.375 reconstruction mse 743597.375 imputation mse 1.064773678779602\n",
      "Train Epoch 69.5 var loss 843780.875 reconstruction mse 743852.375 imputation mse 1.0651147365570068\n",
      "Train Epoch 69.6 var loss 842525.0625 reconstruction mse 743279.25 imputation mse 1.0639283657073975\n",
      "Train Epoch 69.7 var loss 843324.5 reconstruction mse 742910.6875 imputation mse 1.0615252256393433\n",
      "Train Epoch 69.8 var loss 843260.0 reconstruction mse 744505.5 imputation mse 1.0654033422470093\n",
      "Train Epoch 69.9 var loss 842055.9375 reconstruction mse 743180.4375 imputation mse 1.0620907545089722\n",
      "Train Epoch 70.0 var loss 837425.3125 reconstruction mse 737882.8125 imputation mse 1.0311312675476074\n",
      "Train Epoch 70.1 var loss 841654.0 reconstruction mse 741634.3125 imputation mse 1.0615280866622925\n",
      "Train Epoch 70.2 var loss 841171.875 reconstruction mse 741654.125 imputation mse 1.0599931478500366\n",
      "Train Epoch 70.3 var loss 839440.4375 reconstruction mse 740252.9375 imputation mse 1.058319091796875\n",
      "Train Epoch 70.4 var loss 840276.8125 reconstruction mse 741353.25 imputation mse 1.061240553855896\n",
      "Train Epoch 70.5 var loss 839690.4375 reconstruction mse 740950.125 imputation mse 1.0594722032546997\n",
      "Train Epoch 70.6 var loss 840006.625 reconstruction mse 740889.4375 imputation mse 1.0601407289505005\n",
      "Train Epoch 70.7 var loss 837919.25 reconstruction mse 740151.8125 imputation mse 1.05816650390625\n",
      "Train Epoch 70.8 var loss 838745.25 reconstruction mse 740290.75 imputation mse 1.0606963634490967\n",
      "Train Epoch 70.9 var loss 837822.9375 reconstruction mse 739781.25 imputation mse 1.0587975978851318\n",
      "====> Test imputation mse: 1.03700328\n",
      "====> Test imputation mse: 1.05158854\n",
      "====> Test imputation mse: 1.05020893\n",
      "Train Epoch 71.0 var loss 835725.625 reconstruction mse 737652.3125 imputation mse 1.029819130897522\n",
      "Train Epoch 71.1 var loss 841730.0625 reconstruction mse 743960.0 imputation mse 1.0610541105270386\n",
      "Train Epoch 71.2 var loss 844113.1875 reconstruction mse 745088.125 imputation mse 1.0630292892456055\n",
      "Train Epoch 71.3 var loss 843363.875 reconstruction mse 744250.6875 imputation mse 1.061753511428833\n",
      "Train Epoch 71.4 var loss 842327.4375 reconstruction mse 744120.6875 imputation mse 1.0624703168869019\n",
      "Train Epoch 71.5 var loss 841690.75 reconstruction mse 742547.875 imputation mse 1.0583207607269287\n",
      "Train Epoch 71.6 var loss 842965.25 reconstruction mse 743510.0 imputation mse 1.0605350732803345\n",
      "Train Epoch 71.7 var loss 840637.875 reconstruction mse 742953.75 imputation mse 1.0607637166976929\n",
      "Train Epoch 71.8 var loss 842233.5 reconstruction mse 744345.9375 imputation mse 1.061354398727417\n",
      "Train Epoch 71.9 var loss 841385.4375 reconstruction mse 743172.8125 imputation mse 1.0595825910568237\n",
      "Train Epoch 72.0 var loss 832823.6875 reconstruction mse 735954.6875 imputation mse 1.0310112237930298\n",
      "Train Epoch 72.1 var loss 838890.875 reconstruction mse 742321.3125 imputation mse 1.0635360479354858\n",
      "Train Epoch 72.2 var loss 838891.125 reconstruction mse 741866.4375 imputation mse 1.061622142791748\n",
      "Train Epoch 72.3 var loss 837086.625 reconstruction mse 741016.25 imputation mse 1.062768578529358\n",
      "Train Epoch 72.4 var loss 838418.1875 reconstruction mse 740987.875 imputation mse 1.0619348287582397\n",
      "Train Epoch 72.5 var loss 837075.0 reconstruction mse 740977.1875 imputation mse 1.060872197151184\n",
      "Train Epoch 72.6 var loss 837485.4375 reconstruction mse 740977.6875 imputation mse 1.0595327615737915\n",
      "Train Epoch 72.7 var loss 837231.75 reconstruction mse 740690.5 imputation mse 1.0588434934616089\n",
      "Train Epoch 72.8 var loss 836731.125 reconstruction mse 740988.4375 imputation mse 1.0606423616409302\n",
      "Train Epoch 72.9 var loss 837247.625 reconstruction mse 740640.0625 imputation mse 1.059953212738037\n",
      "Train Epoch 73.0 var loss 830023.625 reconstruction mse 733840.0625 imputation mse 1.02443265914917\n",
      "Train Epoch 73.1 var loss 833072.25 reconstruction mse 738259.625 imputation mse 1.0535043478012085\n",
      "Train Epoch 73.2 var loss 833000.5625 reconstruction mse 738118.4375 imputation mse 1.0514030456542969\n",
      "Train Epoch 73.3 var loss 833859.375 reconstruction mse 738101.4375 imputation mse 1.052322268486023\n",
      "Train Epoch 73.4 var loss 833305.0 reconstruction mse 737748.625 imputation mse 1.0488218069076538\n",
      "Train Epoch 73.5 var loss 833035.75 reconstruction mse 737932.6875 imputation mse 1.0519953966140747\n",
      "Train Epoch 73.6 var loss 832032.75 reconstruction mse 737292.1875 imputation mse 1.0517879724502563\n",
      "Train Epoch 73.7 var loss 832442.125 reconstruction mse 738430.375 imputation mse 1.0525689125061035\n",
      "Train Epoch 73.8 var loss 832820.8125 reconstruction mse 738819.6875 imputation mse 1.0543166399002075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 73.9 var loss 831566.75 reconstruction mse 737329.875 imputation mse 1.0503830909729004\n",
      "Train Epoch 74.0 var loss 827574.0625 reconstruction mse 733283.25 imputation mse 1.0241273641586304\n",
      "Train Epoch 74.1 var loss 831561.0 reconstruction mse 736649.25 imputation mse 1.0491461753845215\n",
      "Train Epoch 74.2 var loss 830491.9375 reconstruction mse 735693.625 imputation mse 1.0506892204284668\n",
      "Train Epoch 74.3 var loss 831745.25 reconstruction mse 736933.125 imputation mse 1.0480345487594604\n",
      "Train Epoch 74.4 var loss 830525.875 reconstruction mse 735999.125 imputation mse 1.0485421419143677\n",
      "Train Epoch 74.5 var loss 830051.4375 reconstruction mse 735834.25 imputation mse 1.0491522550582886\n",
      "Train Epoch 74.6 var loss 829457.4375 reconstruction mse 735542.9375 imputation mse 1.0482393503189087\n",
      "Train Epoch 74.7 var loss 830326.625 reconstruction mse 736635.1875 imputation mse 1.0531584024429321\n",
      "Train Epoch 74.8 var loss 829434.875 reconstruction mse 735674.125 imputation mse 1.0484133958816528\n",
      "Train Epoch 74.9 var loss 829699.625 reconstruction mse 735639.0 imputation mse 1.0477890968322754\n",
      "Train Epoch 75.0 var loss 825991.5625 reconstruction mse 732621.8125 imputation mse 1.026646375656128\n",
      "Train Epoch 75.1 var loss 829125.1875 reconstruction mse 735764.1875 imputation mse 1.0532350540161133\n",
      "Train Epoch 75.2 var loss 828893.1875 reconstruction mse 736623.625 imputation mse 1.0559462308883667\n",
      "Train Epoch 75.3 var loss 827935.375 reconstruction mse 734758.375 imputation mse 1.05437171459198\n",
      "Train Epoch 75.4 var loss 828322.75 reconstruction mse 735360.25 imputation mse 1.0525730848312378\n",
      "Train Epoch 75.5 var loss 828457.5625 reconstruction mse 735691.125 imputation mse 1.053937315940857\n",
      "Train Epoch 75.6 var loss 826902.0625 reconstruction mse 734966.75 imputation mse 1.0542203187942505\n",
      "Train Epoch 75.7 var loss 827360.5625 reconstruction mse 735545.0625 imputation mse 1.0531489849090576\n",
      "Train Epoch 75.8 var loss 828015.375 reconstruction mse 735398.625 imputation mse 1.0543758869171143\n",
      "Train Epoch 75.9 var loss 827605.0625 reconstruction mse 735184.125 imputation mse 1.052366018295288\n",
      "Train Epoch 76.0 var loss 825193.4375 reconstruction mse 732475.125 imputation mse 1.0287361145019531\n",
      "Train Epoch 76.1 var loss 828564.75 reconstruction mse 736513.1875 imputation mse 1.0587489604949951\n",
      "Train Epoch 76.2 var loss 829499.8125 reconstruction mse 737393.8125 imputation mse 1.061631202697754\n",
      "Train Epoch 76.3 var loss 828301.75 reconstruction mse 736534.1875 imputation mse 1.0586597919464111\n",
      "Train Epoch 76.4 var loss 828522.8125 reconstruction mse 736346.5625 imputation mse 1.0584218502044678\n",
      "Train Epoch 76.5 var loss 828807.75 reconstruction mse 737649.875 imputation mse 1.059544563293457\n",
      "Train Epoch 76.6 var loss 828744.5625 reconstruction mse 737462.0625 imputation mse 1.0587010383605957\n",
      "Train Epoch 76.7 var loss 828038.25 reconstruction mse 736547.1875 imputation mse 1.0592854022979736\n",
      "Train Epoch 76.8 var loss 827170.5625 reconstruction mse 735351.125 imputation mse 1.0547688007354736\n",
      "Train Epoch 76.9 var loss 827797.0 reconstruction mse 736269.5625 imputation mse 1.0573737621307373\n",
      "Train Epoch 77.0 var loss 822550.6875 reconstruction mse 731012.1875 imputation mse 1.0233052968978882\n",
      "Train Epoch 77.1 var loss 829497.0 reconstruction mse 737566.3125 imputation mse 1.0542739629745483\n",
      "Train Epoch 77.2 var loss 829078.5 reconstruction mse 737249.9375 imputation mse 1.0547502040863037\n",
      "Train Epoch 77.3 var loss 828593.5625 reconstruction mse 737465.375 imputation mse 1.05495023727417\n",
      "Train Epoch 77.4 var loss 827520.375 reconstruction mse 735750.375 imputation mse 1.053031086921692\n",
      "Train Epoch 77.5 var loss 828578.6875 reconstruction mse 737462.75 imputation mse 1.0536860227584839\n",
      "Train Epoch 77.6 var loss 827556.3125 reconstruction mse 737411.9375 imputation mse 1.055037498474121\n",
      "Train Epoch 77.7 var loss 827179.0 reconstruction mse 736184.0 imputation mse 1.0531693696975708\n",
      "Train Epoch 77.8 var loss 827989.8125 reconstruction mse 737631.4375 imputation mse 1.05479896068573\n",
      "Train Epoch 77.9 var loss 828196.9375 reconstruction mse 736828.6875 imputation mse 1.052424430847168\n",
      "Train Epoch 78.0 var loss 825125.0 reconstruction mse 733144.6875 imputation mse 1.0266337394714355\n",
      "Train Epoch 78.1 var loss 829407.1875 reconstruction mse 739386.0 imputation mse 1.054950475692749\n",
      "Train Epoch 78.2 var loss 829080.3125 reconstruction mse 738739.125 imputation mse 1.0522810220718384\n",
      "Train Epoch 78.3 var loss 829567.4375 reconstruction mse 739003.4375 imputation mse 1.054727554321289\n",
      "Train Epoch 78.4 var loss 829077.8125 reconstruction mse 739126.25 imputation mse 1.0542162656784058\n",
      "Train Epoch 78.5 var loss 829798.0625 reconstruction mse 739517.625 imputation mse 1.0542550086975098\n",
      "Train Epoch 78.6 var loss 828614.8125 reconstruction mse 739306.8125 imputation mse 1.0552912950515747\n",
      "Train Epoch 78.7 var loss 828516.5625 reconstruction mse 739462.0 imputation mse 1.0547655820846558\n",
      "Train Epoch 78.8 var loss 828224.1875 reconstruction mse 738904.5625 imputation mse 1.0549367666244507\n",
      "Train Epoch 78.9 var loss 829097.8125 reconstruction mse 739283.625 imputation mse 1.0535703897476196\n",
      "Train Epoch 79.0 var loss 824179.875 reconstruction mse 734075.875 imputation mse 1.0258469581604004\n",
      "Train Epoch 79.1 var loss 830083.6875 reconstruction mse 740154.1875 imputation mse 1.0535460710525513\n",
      "Train Epoch 79.2 var loss 828768.5625 reconstruction mse 740330.625 imputation mse 1.055418848991394\n",
      "Train Epoch 79.3 var loss 829735.1875 reconstruction mse 740101.5 imputation mse 1.0551714897155762\n",
      "Train Epoch 79.4 var loss 829988.4375 reconstruction mse 741230.5 imputation mse 1.0550661087036133\n",
      "Train Epoch 79.5 var loss 829430.0 reconstruction mse 740299.8125 imputation mse 1.0533006191253662\n",
      "Train Epoch 79.6 var loss 829827.1875 reconstruction mse 740597.375 imputation mse 1.0555225610733032\n",
      "Train Epoch 79.7 var loss 828930.625 reconstruction mse 740988.75 imputation mse 1.0558651685714722\n",
      "Train Epoch 79.8 var loss 828652.625 reconstruction mse 739758.625 imputation mse 1.0536081790924072\n",
      "Train Epoch 79.9 var loss 828798.3125 reconstruction mse 740055.375 imputation mse 1.0536844730377197\n",
      "Train Epoch 80.0 var loss 821435.5 reconstruction mse 732854.75 imputation mse 1.0239614248275757\n",
      "Train Epoch 80.1 var loss 828443.625 reconstruction mse 740613.625 imputation mse 1.0531251430511475\n",
      "Train Epoch 80.2 var loss 828640.75 reconstruction mse 740617.75 imputation mse 1.0528614521026611\n",
      "Train Epoch 80.3 var loss 828183.125 reconstruction mse 740331.25 imputation mse 1.051742434501648\n",
      "Train Epoch 80.4 var loss 827923.5625 reconstruction mse 739563.1875 imputation mse 1.0520012378692627\n",
      "Train Epoch 80.5 var loss 826584.8125 reconstruction mse 739009.125 imputation mse 1.0493452548980713\n",
      "Train Epoch 80.6 var loss 827656.1875 reconstruction mse 739509.625 imputation mse 1.050680160522461\n",
      "Train Epoch 80.7 var loss 827487.1875 reconstruction mse 739776.8125 imputation mse 1.0518426895141602\n",
      "Train Epoch 80.8 var loss 825907.125 reconstruction mse 739176.125 imputation mse 1.0500338077545166\n",
      "Train Epoch 80.9 var loss 825584.875 reconstruction mse 739640.8125 imputation mse 1.0517247915267944\n",
      "====> Test imputation mse: 1.04378331\n",
      "====> Test imputation mse: 1.05233693\n",
      "====> Test imputation mse: 1.04877710\n",
      "Train Epoch 81.0 var loss 819572.9375 reconstruction mse 731982.125 imputation mse 1.0187225341796875\n",
      "Train Epoch 81.1 var loss 822480.25 reconstruction mse 734975.5625 imputation mse 1.051201343536377\n",
      "Train Epoch 81.2 var loss 820974.0625 reconstruction mse 733347.9375 imputation mse 1.0511329174041748\n",
      "Train Epoch 81.3 var loss 821481.5625 reconstruction mse 734222.75 imputation mse 1.0503957271575928\n",
      "Train Epoch 81.4 var loss 821763.375 reconstruction mse 734098.0 imputation mse 1.0515892505645752\n",
      "Train Epoch 81.5 var loss 820567.4375 reconstruction mse 733868.125 imputation mse 1.0507007837295532\n",
      "Train Epoch 81.6 var loss 820703.875 reconstruction mse 733745.6875 imputation mse 1.0465881824493408\n",
      "Train Epoch 81.7 var loss 820727.125 reconstruction mse 734000.8125 imputation mse 1.047916293144226\n",
      "Train Epoch 81.8 var loss 819910.5 reconstruction mse 733808.9375 imputation mse 1.0462250709533691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 81.9 var loss 820300.1875 reconstruction mse 733760.0 imputation mse 1.0472784042358398\n",
      "Train Epoch 82.0 var loss 818075.9375 reconstruction mse 731592.3125 imputation mse 1.0229817628860474\n",
      "Train Epoch 82.1 var loss 819374.625 reconstruction mse 733528.5 imputation mse 1.0500068664550781\n",
      "Train Epoch 82.2 var loss 819553.375 reconstruction mse 733090.75 imputation mse 1.0471786260604858\n",
      "Train Epoch 82.3 var loss 819557.875 reconstruction mse 733757.0 imputation mse 1.0473268032073975\n",
      "Train Epoch 82.4 var loss 818639.5625 reconstruction mse 732671.3125 imputation mse 1.0514216423034668\n",
      "Train Epoch 82.5 var loss 818487.0 reconstruction mse 732527.4375 imputation mse 1.0466912984848022\n",
      "Train Epoch 82.6 var loss 819146.1875 reconstruction mse 733365.6875 imputation mse 1.0474965572357178\n",
      "Train Epoch 82.7 var loss 818275.75 reconstruction mse 732753.1875 imputation mse 1.0464723110198975\n",
      "Train Epoch 82.8 var loss 819007.0 reconstruction mse 734102.0 imputation mse 1.049851417541504\n",
      "Train Epoch 82.9 var loss 817851.3125 reconstruction mse 733122.5 imputation mse 1.0475523471832275\n",
      "Train Epoch 83.0 var loss 818377.25 reconstruction mse 732442.9375 imputation mse 1.026707410812378\n",
      "Train Epoch 83.1 var loss 823503.0 reconstruction mse 738313.625 imputation mse 1.0539101362228394\n",
      "Train Epoch 83.2 var loss 823670.75 reconstruction mse 737718.0 imputation mse 1.052831768989563\n",
      "Train Epoch 83.3 var loss 824027.1875 reconstruction mse 738486.875 imputation mse 1.0544885396957397\n",
      "Train Epoch 83.4 var loss 823031.75 reconstruction mse 738685.5625 imputation mse 1.055668830871582\n",
      "Train Epoch 83.5 var loss 823102.25 reconstruction mse 738481.375 imputation mse 1.0548175573349\n",
      "Train Epoch 83.6 var loss 823022.75 reconstruction mse 738190.25 imputation mse 1.0532265901565552\n",
      "Train Epoch 83.7 var loss 822566.5 reconstruction mse 737370.25 imputation mse 1.0506889820098877\n",
      "Train Epoch 83.8 var loss 821918.6875 reconstruction mse 737102.5 imputation mse 1.0524204969406128\n",
      "Train Epoch 83.9 var loss 822410.375 reconstruction mse 737851.25 imputation mse 1.0513476133346558\n",
      "Train Epoch 84.0 var loss 816839.9375 reconstruction mse 731669.4375 imputation mse 1.0232999324798584\n",
      "Train Epoch 84.1 var loss 819166.625 reconstruction mse 734024.875 imputation mse 1.0508389472961426\n",
      "Train Epoch 84.2 var loss 817547.4375 reconstruction mse 733618.125 imputation mse 1.0494235754013062\n",
      "Train Epoch 84.3 var loss 817887.1875 reconstruction mse 732994.0 imputation mse 1.0481878519058228\n",
      "Train Epoch 84.4 var loss 818144.0625 reconstruction mse 733595.625 imputation mse 1.0473361015319824\n",
      "Train Epoch 84.5 var loss 817338.25 reconstruction mse 732664.1875 imputation mse 1.049371361732483\n",
      "Train Epoch 84.6 var loss 817461.5 reconstruction mse 733287.3125 imputation mse 1.0509415864944458\n",
      "Train Epoch 84.7 var loss 817145.0625 reconstruction mse 732607.125 imputation mse 1.050621747970581\n",
      "Train Epoch 84.8 var loss 817288.375 reconstruction mse 732721.5625 imputation mse 1.0459699630737305\n",
      "Train Epoch 84.9 var loss 816936.125 reconstruction mse 732805.75 imputation mse 1.048580527305603\n",
      "Train Epoch 85.0 var loss 814031.125 reconstruction mse 730385.125 imputation mse 1.0233874320983887\n",
      "Train Epoch 85.1 var loss 820063.4375 reconstruction mse 735234.625 imputation mse 1.0497863292694092\n",
      "Train Epoch 85.2 var loss 819621.6875 reconstruction mse 735939.875 imputation mse 1.049239993095398\n",
      "Train Epoch 85.3 var loss 818975.4375 reconstruction mse 734958.9375 imputation mse 1.047330617904663\n",
      "Train Epoch 85.4 var loss 819364.6875 reconstruction mse 735641.3125 imputation mse 1.0493618249893188\n",
      "Train Epoch 85.5 var loss 819373.75 reconstruction mse 735196.3125 imputation mse 1.0477977991104126\n",
      "Train Epoch 85.6 var loss 818215.375 reconstruction mse 734288.75 imputation mse 1.0475879907608032\n",
      "Train Epoch 85.7 var loss 818346.0 reconstruction mse 735051.375 imputation mse 1.0479964017868042\n",
      "Train Epoch 85.8 var loss 818926.5 reconstruction mse 735227.0 imputation mse 1.0472829341888428\n",
      "Train Epoch 85.9 var loss 817538.0 reconstruction mse 734535.625 imputation mse 1.0475894212722778\n",
      "Train Epoch 86.0 var loss 813664.875 reconstruction mse 730202.0625 imputation mse 1.019303560256958\n",
      "Train Epoch 86.1 var loss 818097.0 reconstruction mse 735269.5 imputation mse 1.0486485958099365\n",
      "Train Epoch 86.2 var loss 817507.375 reconstruction mse 734010.5 imputation mse 1.0475308895111084\n",
      "Train Epoch 86.3 var loss 817233.375 reconstruction mse 734458.5625 imputation mse 1.0467636585235596\n",
      "Train Epoch 86.4 var loss 817130.0625 reconstruction mse 734360.0625 imputation mse 1.0475800037384033\n",
      "Train Epoch 86.5 var loss 815763.9375 reconstruction mse 733963.625 imputation mse 1.043845295906067\n",
      "Train Epoch 86.6 var loss 818127.9375 reconstruction mse 735450.8125 imputation mse 1.0463029146194458\n",
      "Train Epoch 86.7 var loss 816603.25 reconstruction mse 734004.0625 imputation mse 1.043282389640808\n",
      "Train Epoch 86.8 var loss 816914.0 reconstruction mse 734633.75 imputation mse 1.0459516048431396\n",
      "Train Epoch 86.9 var loss 815160.8125 reconstruction mse 733255.0625 imputation mse 1.042320966720581\n",
      "Train Epoch 87.0 var loss 811827.625 reconstruction mse 729059.1875 imputation mse 1.0186723470687866\n",
      "Train Epoch 87.1 var loss 814046.5 reconstruction mse 731866.6875 imputation mse 1.0445024967193604\n",
      "Train Epoch 87.2 var loss 813643.0 reconstruction mse 731477.5625 imputation mse 1.0439958572387695\n",
      "Train Epoch 87.3 var loss 814003.875 reconstruction mse 731089.0625 imputation mse 1.0408567190170288\n",
      "Train Epoch 87.4 var loss 813526.625 reconstruction mse 732008.1875 imputation mse 1.0451873540878296\n",
      "Train Epoch 87.5 var loss 814790.75 reconstruction mse 732555.25 imputation mse 1.0467684268951416\n",
      "Train Epoch 87.6 var loss 813881.625 reconstruction mse 732293.25 imputation mse 1.0449753999710083\n",
      "Train Epoch 87.7 var loss 813323.25 reconstruction mse 731524.0625 imputation mse 1.042349100112915\n",
      "Train Epoch 87.8 var loss 813197.4375 reconstruction mse 731459.8125 imputation mse 1.0439469814300537\n",
      "Train Epoch 87.9 var loss 812900.25 reconstruction mse 731520.6875 imputation mse 1.044218897819519\n",
      "Train Epoch 88.0 var loss 811293.9375 reconstruction mse 730204.625 imputation mse 1.0212732553482056\n",
      "Train Epoch 88.1 var loss 817265.5 reconstruction mse 736393.25 imputation mse 1.0485334396362305\n",
      "Train Epoch 88.2 var loss 817523.9375 reconstruction mse 736662.375 imputation mse 1.0484776496887207\n",
      "Train Epoch 88.3 var loss 817385.9375 reconstruction mse 736009.5 imputation mse 1.047276258468628\n",
      "Train Epoch 88.4 var loss 818545.0 reconstruction mse 736873.625 imputation mse 1.0479397773742676\n",
      "Train Epoch 88.5 var loss 817588.8125 reconstruction mse 736835.375 imputation mse 1.0481888055801392\n",
      "Train Epoch 88.6 var loss 817206.75 reconstruction mse 736216.1875 imputation mse 1.0482743978500366\n",
      "Train Epoch 88.7 var loss 816998.5 reconstruction mse 736169.9375 imputation mse 1.0466961860656738\n",
      "Train Epoch 88.8 var loss 817056.75 reconstruction mse 736134.4375 imputation mse 1.047163963317871\n",
      "Train Epoch 88.9 var loss 816562.125 reconstruction mse 735976.5 imputation mse 1.0476369857788086\n",
      "Train Epoch 89.0 var loss 810774.0625 reconstruction mse 730676.625 imputation mse 1.019279956817627\n",
      "Train Epoch 89.1 var loss 814811.875 reconstruction mse 735084.1875 imputation mse 1.047193169593811\n",
      "Train Epoch 89.2 var loss 814969.25 reconstruction mse 735698.625 imputation mse 1.0470207929611206\n",
      "Train Epoch 89.3 var loss 814608.3125 reconstruction mse 735157.125 imputation mse 1.046938419342041\n",
      "Train Epoch 89.4 var loss 815661.375 reconstruction mse 735579.4375 imputation mse 1.0483442544937134\n",
      "Train Epoch 89.5 var loss 813749.125 reconstruction mse 734929.8125 imputation mse 1.0459758043289185\n",
      "Train Epoch 89.6 var loss 814683.3125 reconstruction mse 735114.3125 imputation mse 1.0451244115829468\n",
      "Train Epoch 89.7 var loss 815092.375 reconstruction mse 735658.8125 imputation mse 1.046007752418518\n",
      "Train Epoch 89.8 var loss 813402.375 reconstruction mse 734692.6875 imputation mse 1.0436124801635742\n",
      "Train Epoch 89.9 var loss 814807.875 reconstruction mse 735375.0625 imputation mse 1.0472818613052368\n",
      "Train Epoch 90.0 var loss 810102.625 reconstruction mse 730537.9375 imputation mse 1.026071548461914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 90.1 var loss 812715.625 reconstruction mse 733629.9375 imputation mse 1.0514928102493286\n",
      "Train Epoch 90.2 var loss 813402.625 reconstruction mse 733673.9375 imputation mse 1.0541194677352905\n",
      "Train Epoch 90.3 var loss 813123.125 reconstruction mse 734013.875 imputation mse 1.0503262281417847\n",
      "Train Epoch 90.4 var loss 812495.625 reconstruction mse 733567.3125 imputation mse 1.0527979135513306\n",
      "Train Epoch 90.5 var loss 812660.125 reconstruction mse 733805.625 imputation mse 1.0520776510238647\n",
      "Train Epoch 90.6 var loss 812951.6875 reconstruction mse 734543.125 imputation mse 1.0522769689559937\n",
      "Train Epoch 90.7 var loss 813539.875 reconstruction mse 733599.5625 imputation mse 1.052328109741211\n",
      "Train Epoch 90.8 var loss 813591.3125 reconstruction mse 734173.375 imputation mse 1.0505027770996094\n",
      "Train Epoch 90.9 var loss 812375.5 reconstruction mse 733477.125 imputation mse 1.050301432609558\n",
      "====> Test imputation mse: 1.04806995\n",
      "====> Test imputation mse: 1.05171514\n",
      "====> Test imputation mse: 1.04499018\n",
      "Train Epoch 91.0 var loss 809133.5625 reconstruction mse 730235.875 imputation mse 1.0195950269699097\n",
      "Train Epoch 91.1 var loss 810846.4375 reconstruction mse 731964.625 imputation mse 1.0462722778320312\n",
      "Train Epoch 91.2 var loss 811826.8125 reconstruction mse 732713.875 imputation mse 1.0469026565551758\n",
      "Train Epoch 91.3 var loss 812610.125 reconstruction mse 733575.75 imputation mse 1.0478812456130981\n",
      "Train Epoch 91.4 var loss 811460.375 reconstruction mse 732578.9375 imputation mse 1.0462260246276855\n",
      "Train Epoch 91.5 var loss 811239.125 reconstruction mse 732359.4375 imputation mse 1.0468320846557617\n",
      "Train Epoch 91.6 var loss 810297.75 reconstruction mse 731962.1875 imputation mse 1.0465128421783447\n",
      "Train Epoch 91.7 var loss 810223.125 reconstruction mse 731711.0 imputation mse 1.0461103916168213\n",
      "Train Epoch 91.8 var loss 810318.375 reconstruction mse 731843.375 imputation mse 1.0446789264678955\n",
      "Train Epoch 91.9 var loss 810203.3125 reconstruction mse 731455.4375 imputation mse 1.0465972423553467\n",
      "Train Epoch 92.0 var loss 806644.5625 reconstruction mse 728793.9375 imputation mse 1.0174237489700317\n",
      "Train Epoch 92.1 var loss 813814.3125 reconstruction mse 735905.625 imputation mse 1.0452560186386108\n",
      "Train Epoch 92.2 var loss 813187.5 reconstruction mse 734813.5 imputation mse 1.0436567068099976\n",
      "Train Epoch 92.3 var loss 814578.1875 reconstruction mse 735933.875 imputation mse 1.0443384647369385\n",
      "Train Epoch 92.4 var loss 813429.3125 reconstruction mse 735341.1875 imputation mse 1.04329252243042\n",
      "Train Epoch 92.5 var loss 813388.75 reconstruction mse 735362.375 imputation mse 1.044182300567627\n",
      "Train Epoch 92.6 var loss 812379.1875 reconstruction mse 734222.875 imputation mse 1.0407931804656982\n",
      "Train Epoch 92.7 var loss 813969.4375 reconstruction mse 735807.375 imputation mse 1.0440213680267334\n",
      "Train Epoch 92.8 var loss 812715.0 reconstruction mse 733899.3125 imputation mse 1.041161298751831\n",
      "Train Epoch 92.9 var loss 812533.0 reconstruction mse 734839.25 imputation mse 1.0433311462402344\n",
      "Train Epoch 93.0 var loss 806948.4375 reconstruction mse 728703.25 imputation mse 1.0186551809310913\n",
      "Train Epoch 93.1 var loss 814247.1875 reconstruction mse 736765.625 imputation mse 1.046431541442871\n",
      "Train Epoch 93.2 var loss 813229.125 reconstruction mse 735955.0 imputation mse 1.0442506074905396\n",
      "Train Epoch 93.3 var loss 814178.5625 reconstruction mse 736378.125 imputation mse 1.0459107160568237\n",
      "Train Epoch 93.4 var loss 812662.3125 reconstruction mse 735308.875 imputation mse 1.044079065322876\n",
      "Train Epoch 93.5 var loss 813483.875 reconstruction mse 735862.4375 imputation mse 1.0445966720581055\n",
      "Train Epoch 93.6 var loss 814027.5625 reconstruction mse 735629.0625 imputation mse 1.0447291135787964\n",
      "Train Epoch 93.7 var loss 812398.125 reconstruction mse 735476.0625 imputation mse 1.045457363128662\n",
      "Train Epoch 93.8 var loss 812303.875 reconstruction mse 735850.9375 imputation mse 1.045087456703186\n",
      "Train Epoch 93.9 var loss 812105.0625 reconstruction mse 735208.8125 imputation mse 1.0435388088226318\n",
      "Train Epoch 94.0 var loss 805931.75 reconstruction mse 729324.125 imputation mse 1.0274513959884644\n",
      "Train Epoch 94.1 var loss 808319.4375 reconstruction mse 730920.75 imputation mse 1.0543402433395386\n",
      "Train Epoch 94.2 var loss 807732.25 reconstruction mse 731076.625 imputation mse 1.0536329746246338\n",
      "Train Epoch 94.3 var loss 808481.9375 reconstruction mse 731713.3125 imputation mse 1.056512713432312\n",
      "Train Epoch 94.4 var loss 807477.1875 reconstruction mse 731070.125 imputation mse 1.0520907640457153\n",
      "Train Epoch 94.5 var loss 808564.125 reconstruction mse 731718.4375 imputation mse 1.0560078620910645\n",
      "Train Epoch 94.6 var loss 806038.8125 reconstruction mse 729999.25 imputation mse 1.0526701211929321\n",
      "Train Epoch 94.7 var loss 807832.875 reconstruction mse 731321.0625 imputation mse 1.0524226427078247\n",
      "Train Epoch 94.8 var loss 807609.875 reconstruction mse 731268.3125 imputation mse 1.0574456453323364\n",
      "Train Epoch 94.9 var loss 807400.75 reconstruction mse 731042.375 imputation mse 1.0530818700790405\n",
      "Train Epoch 95.0 var loss 807034.4375 reconstruction mse 730034.5 imputation mse 1.0196491479873657\n",
      "Train Epoch 95.1 var loss 811778.8125 reconstruction mse 736626.625 imputation mse 1.044933795928955\n",
      "Train Epoch 95.2 var loss 812695.125 reconstruction mse 736377.0 imputation mse 1.0441069602966309\n",
      "Train Epoch 95.3 var loss 810973.875 reconstruction mse 735329.8125 imputation mse 1.0412169694900513\n",
      "Train Epoch 95.4 var loss 812798.75 reconstruction mse 736897.75 imputation mse 1.0441005229949951\n",
      "Train Epoch 95.5 var loss 811567.875 reconstruction mse 735539.9375 imputation mse 1.0432401895523071\n",
      "Train Epoch 95.6 var loss 810987.875 reconstruction mse 735678.3125 imputation mse 1.042212963104248\n",
      "Train Epoch 95.7 var loss 812113.875 reconstruction mse 735904.25 imputation mse 1.043112874031067\n",
      "Train Epoch 95.8 var loss 810899.1875 reconstruction mse 735742.0 imputation mse 1.0426368713378906\n",
      "Train Epoch 95.9 var loss 811469.75 reconstruction mse 735559.125 imputation mse 1.0421842336654663\n",
      "Train Epoch 96.0 var loss 805846.0 reconstruction mse 730112.375 imputation mse 1.0226198434829712\n",
      "Train Epoch 96.1 var loss 811645.3125 reconstruction mse 736339.3125 imputation mse 1.0453070402145386\n",
      "Train Epoch 96.2 var loss 811545.3125 reconstruction mse 735776.875 imputation mse 1.04463529586792\n",
      "Train Epoch 96.3 var loss 810294.5625 reconstruction mse 735906.9375 imputation mse 1.044928789138794\n",
      "Train Epoch 96.4 var loss 811471.75 reconstruction mse 735596.3125 imputation mse 1.0448311567306519\n",
      "Train Epoch 96.5 var loss 810227.25 reconstruction mse 735146.8125 imputation mse 1.0447955131530762\n",
      "Train Epoch 96.6 var loss 811350.75 reconstruction mse 735867.5625 imputation mse 1.04511296749115\n",
      "Train Epoch 96.7 var loss 809641.75 reconstruction mse 735624.3125 imputation mse 1.0429096221923828\n",
      "Train Epoch 96.8 var loss 810238.5 reconstruction mse 735705.0 imputation mse 1.0448516607284546\n",
      "Train Epoch 96.9 var loss 809955.1875 reconstruction mse 735291.8125 imputation mse 1.0428980588912964\n",
      "Train Epoch 97.0 var loss 805405.75 reconstruction mse 730617.9375 imputation mse 1.0225023031234741\n",
      "Train Epoch 97.1 var loss 809391.125 reconstruction mse 734017.375 imputation mse 1.0438015460968018\n",
      "Train Epoch 97.2 var loss 810173.5 reconstruction mse 734109.1875 imputation mse 1.04511559009552\n",
      "Train Epoch 97.3 var loss 809048.5625 reconstruction mse 733972.3125 imputation mse 1.0454727411270142\n",
      "Train Epoch 97.4 var loss 808682.0 reconstruction mse 734198.4375 imputation mse 1.0453405380249023\n",
      "Train Epoch 97.5 var loss 809655.25 reconstruction mse 733895.625 imputation mse 1.043484091758728\n",
      "Train Epoch 97.6 var loss 809129.75 reconstruction mse 734448.1875 imputation mse 1.0434592962265015\n",
      "Train Epoch 97.7 var loss 808875.5 reconstruction mse 733857.5 imputation mse 1.044622778892517\n",
      "Train Epoch 97.8 var loss 808631.3125 reconstruction mse 734249.25 imputation mse 1.045130729675293\n",
      "Train Epoch 97.9 var loss 806981.125 reconstruction mse 733843.6875 imputation mse 1.045479655265808\n",
      "Train Epoch 98.0 var loss 804344.875 reconstruction mse 729958.25 imputation mse 1.0241293907165527\n",
      "Train Epoch 98.1 var loss 806071.1875 reconstruction mse 732364.625 imputation mse 1.0482052564620972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 98.2 var loss 806396.125 reconstruction mse 731854.3125 imputation mse 1.0483109951019287\n",
      "Train Epoch 98.3 var loss 804639.25 reconstruction mse 731116.375 imputation mse 1.044729232788086\n",
      "Train Epoch 98.4 var loss 804997.5625 reconstruction mse 731903.375 imputation mse 1.0474015474319458\n",
      "Train Epoch 98.5 var loss 805405.3125 reconstruction mse 731976.4375 imputation mse 1.046715259552002\n",
      "Train Epoch 98.6 var loss 804885.875 reconstruction mse 731713.75 imputation mse 1.0479974746704102\n",
      "Train Epoch 98.7 var loss 805041.875 reconstruction mse 731223.375 imputation mse 1.0468324422836304\n",
      "Train Epoch 98.8 var loss 804194.3125 reconstruction mse 730793.8125 imputation mse 1.044904112815857\n",
      "Train Epoch 98.9 var loss 804661.125 reconstruction mse 731623.6875 imputation mse 1.0485177040100098\n",
      "Train Epoch 99.0 var loss 801673.5625 reconstruction mse 728290.8125 imputation mse 1.0145375728607178\n",
      "Train Epoch 99.1 var loss 805318.625 reconstruction mse 732952.4375 imputation mse 1.037718653678894\n",
      "Train Epoch 99.2 var loss 805907.0625 reconstruction mse 732803.0625 imputation mse 1.0382640361785889\n",
      "Train Epoch 99.3 var loss 806917.375 reconstruction mse 733900.3125 imputation mse 1.0382534265518188\n",
      "Train Epoch 99.4 var loss 805945.1875 reconstruction mse 732709.4375 imputation mse 1.0367451906204224\n",
      "Train Epoch 99.5 var loss 805983.0 reconstruction mse 733416.125 imputation mse 1.0389682054519653\n",
      "Train Epoch 99.6 var loss 805913.5 reconstruction mse 732428.25 imputation mse 1.0377475023269653\n",
      "Train Epoch 99.7 var loss 805824.625 reconstruction mse 733046.625 imputation mse 1.0391367673873901\n",
      "Train Epoch 99.8 var loss 805130.25 reconstruction mse 731708.375 imputation mse 1.035388469696045\n",
      "Train Epoch 99.9 var loss 805581.5 reconstruction mse 732156.4375 imputation mse 1.0369105339050293\n",
      "Train Epoch 100.0 var loss 800424.75 reconstruction mse 728135.5625 imputation mse 1.0236631631851196\n",
      "Train Epoch 100.1 var loss 803284.9375 reconstruction mse 730543.875 imputation mse 1.0468533039093018\n",
      "Train Epoch 100.2 var loss 803146.0625 reconstruction mse 730677.4375 imputation mse 1.0493069887161255\n",
      "Train Epoch 100.3 var loss 802713.5 reconstruction mse 730424.875 imputation mse 1.0471959114074707\n",
      "Train Epoch 100.4 var loss 802617.8125 reconstruction mse 730174.5625 imputation mse 1.0473711490631104\n",
      "Train Epoch 100.5 var loss 802565.3125 reconstruction mse 730323.875 imputation mse 1.0464799404144287\n",
      "Train Epoch 100.6 var loss 801946.4375 reconstruction mse 729914.4375 imputation mse 1.0467954874038696\n",
      "Train Epoch 100.7 var loss 803301.75 reconstruction mse 730740.75 imputation mse 1.0488897562026978\n",
      "Train Epoch 100.8 var loss 802452.5 reconstruction mse 730830.4375 imputation mse 1.046789526939392\n",
      "Train Epoch 100.9 var loss 802008.5625 reconstruction mse 730130.6875 imputation mse 1.046843409538269\n",
      "====> Test imputation mse: 1.04623020\n",
      "====> Test imputation mse: 1.04558289\n",
      "====> Test imputation mse: 1.05214751\n",
      "Train Epoch 101.0 var loss 800000.5 reconstruction mse 726912.25 imputation mse 1.0149974822998047\n",
      "Train Epoch 101.1 var loss 805114.875 reconstruction mse 732344.3125 imputation mse 1.0384795665740967\n",
      "Train Epoch 101.2 var loss 804447.625 reconstruction mse 732347.5 imputation mse 1.0385514497756958\n",
      "Train Epoch 101.3 var loss 804464.9375 reconstruction mse 732584.25 imputation mse 1.0388926267623901\n",
      "Train Epoch 101.4 var loss 804595.4375 reconstruction mse 732606.125 imputation mse 1.0391314029693604\n",
      "Train Epoch 101.5 var loss 804951.4375 reconstruction mse 732950.125 imputation mse 1.0376354455947876\n",
      "Train Epoch 101.6 var loss 804178.5625 reconstruction mse 731507.3125 imputation mse 1.0364404916763306\n",
      "Train Epoch 101.7 var loss 804306.5 reconstruction mse 732155.8125 imputation mse 1.0377047061920166\n",
      "Train Epoch 101.8 var loss 804262.6875 reconstruction mse 732436.5 imputation mse 1.0380796194076538\n",
      "Train Epoch 101.9 var loss 803881.375 reconstruction mse 732388.9375 imputation mse 1.0366454124450684\n",
      "Train Epoch 102.0 var loss 798974.75 reconstruction mse 726641.0 imputation mse 1.0136967897415161\n",
      "Train Epoch 102.1 var loss 801703.5 reconstruction mse 730041.9375 imputation mse 1.03725004196167\n",
      "Train Epoch 102.2 var loss 801786.4375 reconstruction mse 729951.125 imputation mse 1.0364668369293213\n",
      "Train Epoch 102.3 var loss 802034.0 reconstruction mse 730286.6875 imputation mse 1.0386321544647217\n",
      "Train Epoch 102.4 var loss 801477.9375 reconstruction mse 729478.875 imputation mse 1.0363825559616089\n",
      "Train Epoch 102.5 var loss 801820.5625 reconstruction mse 730598.25 imputation mse 1.0376839637756348\n",
      "Train Epoch 102.6 var loss 800908.6875 reconstruction mse 729478.625 imputation mse 1.0353317260742188\n",
      "Train Epoch 102.7 var loss 801403.6875 reconstruction mse 729732.3125 imputation mse 1.036446213722229\n",
      "Train Epoch 102.8 var loss 802645.4375 reconstruction mse 730099.625 imputation mse 1.0358747243881226\n",
      "Train Epoch 102.9 var loss 801311.9375 reconstruction mse 729725.4375 imputation mse 1.0357067584991455\n",
      "Train Epoch 103.0 var loss 796393.8125 reconstruction mse 725435.3125 imputation mse 1.012355089187622\n",
      "Train Epoch 103.1 var loss 800754.6875 reconstruction mse 728876.0625 imputation mse 1.0350912809371948\n",
      "Train Epoch 103.2 var loss 799842.0625 reconstruction mse 728631.25 imputation mse 1.0350874662399292\n",
      "Train Epoch 103.3 var loss 800199.75 reconstruction mse 728566.5 imputation mse 1.034617304801941\n",
      "Train Epoch 103.4 var loss 800693.5 reconstruction mse 729698.5625 imputation mse 1.037157416343689\n",
      "Train Epoch 103.5 var loss 800523.75 reconstruction mse 728977.5 imputation mse 1.0349595546722412\n",
      "Train Epoch 103.6 var loss 799491.9375 reconstruction mse 728854.3125 imputation mse 1.0361802577972412\n",
      "Train Epoch 103.7 var loss 799741.4375 reconstruction mse 728572.75 imputation mse 1.0347545146942139\n",
      "Train Epoch 103.8 var loss 799079.1875 reconstruction mse 728585.3125 imputation mse 1.035637378692627\n",
      "Train Epoch 103.9 var loss 799828.9375 reconstruction mse 728552.5625 imputation mse 1.033601999282837\n",
      "Train Epoch 104.0 var loss 796003.875 reconstruction mse 725089.5 imputation mse 1.0199940204620361\n",
      "Train Epoch 104.1 var loss 798633.875 reconstruction mse 727931.125 imputation mse 1.0439788103103638\n",
      "Train Epoch 104.2 var loss 796821.8125 reconstruction mse 726678.9375 imputation mse 1.0429233312606812\n",
      "Train Epoch 104.3 var loss 797544.875 reconstruction mse 727188.875 imputation mse 1.0444772243499756\n",
      "Train Epoch 104.4 var loss 797321.0625 reconstruction mse 727359.375 imputation mse 1.0442439317703247\n",
      "Train Epoch 104.5 var loss 796757.125 reconstruction mse 726694.5 imputation mse 1.0445053577423096\n",
      "Train Epoch 104.6 var loss 796784.25 reconstruction mse 727347.8125 imputation mse 1.0447745323181152\n",
      "Train Epoch 104.7 var loss 796994.0 reconstruction mse 726997.9375 imputation mse 1.042464256286621\n",
      "Train Epoch 104.8 var loss 797590.125 reconstruction mse 727088.0 imputation mse 1.0433909893035889\n",
      "Train Epoch 104.9 var loss 796641.0625 reconstruction mse 726579.1875 imputation mse 1.0438168048858643\n",
      "Train Epoch 105.0 var loss 795765.0 reconstruction mse 725611.75 imputation mse 1.0203944444656372\n",
      "Train Epoch 105.1 var loss 798153.375 reconstruction mse 727956.625 imputation mse 1.0412406921386719\n",
      "Train Epoch 105.2 var loss 798361.625 reconstruction mse 728246.4375 imputation mse 1.0417898893356323\n",
      "Train Epoch 105.3 var loss 798028.6875 reconstruction mse 727986.4375 imputation mse 1.0438745021820068\n",
      "Train Epoch 105.4 var loss 797809.6875 reconstruction mse 728330.9375 imputation mse 1.0425742864608765\n",
      "Train Epoch 105.5 var loss 797551.75 reconstruction mse 728317.6875 imputation mse 1.0425355434417725\n",
      "Train Epoch 105.6 var loss 797872.375 reconstruction mse 727798.625 imputation mse 1.0405722856521606\n",
      "Train Epoch 105.7 var loss 797995.5 reconstruction mse 728007.0625 imputation mse 1.0440285205841064\n",
      "Train Epoch 105.8 var loss 798083.9375 reconstruction mse 728241.0625 imputation mse 1.0422919988632202\n",
      "Train Epoch 105.9 var loss 797743.0 reconstruction mse 727987.5 imputation mse 1.0423083305358887\n",
      "Train Epoch 106.0 var loss 794473.1875 reconstruction mse 724472.9375 imputation mse 1.0098694562911987\n",
      "Train Epoch 106.1 var loss 797902.625 reconstruction mse 728442.625 imputation mse 1.0340677499771118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 106.2 var loss 798267.75 reconstruction mse 728743.0625 imputation mse 1.0342036485671997\n",
      "Train Epoch 106.3 var loss 799241.625 reconstruction mse 728127.125 imputation mse 1.0312104225158691\n",
      "Train Epoch 106.4 var loss 798711.625 reconstruction mse 728487.5625 imputation mse 1.0327800512313843\n",
      "Train Epoch 106.5 var loss 797628.75 reconstruction mse 727903.375 imputation mse 1.031639814376831\n",
      "Train Epoch 106.6 var loss 798188.5 reconstruction mse 728342.5625 imputation mse 1.0328444242477417\n",
      "Train Epoch 106.7 var loss 797892.25 reconstruction mse 727885.6875 imputation mse 1.0293856859207153\n",
      "Train Epoch 106.8 var loss 798577.125 reconstruction mse 728564.25 imputation mse 1.0327643156051636\n",
      "Train Epoch 106.9 var loss 797392.75 reconstruction mse 727443.75 imputation mse 1.0327484607696533\n",
      "Train Epoch 107.0 var loss 794654.875 reconstruction mse 725297.8125 imputation mse 1.0153428316116333\n",
      "Train Epoch 107.1 var loss 801309.5625 reconstruction mse 731084.5625 imputation mse 1.0368587970733643\n",
      "Train Epoch 107.2 var loss 800868.4375 reconstruction mse 731621.9375 imputation mse 1.0382766723632812\n",
      "Train Epoch 107.3 var loss 801133.4375 reconstruction mse 732092.375 imputation mse 1.039489507675171\n",
      "Train Epoch 107.4 var loss 801703.5625 reconstruction mse 731711.1875 imputation mse 1.0377980470657349\n",
      "Train Epoch 107.5 var loss 800585.0625 reconstruction mse 731110.8125 imputation mse 1.0372596979141235\n",
      "Train Epoch 107.6 var loss 800366.125 reconstruction mse 731184.25 imputation mse 1.0364406108856201\n",
      "Train Epoch 107.7 var loss 800478.625 reconstruction mse 731682.0625 imputation mse 1.037095546722412\n",
      "Train Epoch 107.8 var loss 800274.8125 reconstruction mse 731101.5 imputation mse 1.0366953611373901\n",
      "Train Epoch 107.9 var loss 800733.6875 reconstruction mse 731523.6875 imputation mse 1.0383641719818115\n",
      "Train Epoch 108.0 var loss 794246.625 reconstruction mse 724402.25 imputation mse 1.012674331665039\n",
      "Train Epoch 108.1 var loss 798910.1875 reconstruction mse 729316.3125 imputation mse 1.0363649129867554\n",
      "Train Epoch 108.2 var loss 797933.5 reconstruction mse 728643.9375 imputation mse 1.035588026046753\n",
      "Train Epoch 108.3 var loss 797071.625 reconstruction mse 728586.875 imputation mse 1.0328869819641113\n",
      "Train Epoch 108.4 var loss 798238.0 reconstruction mse 729189.3125 imputation mse 1.0359421968460083\n",
      "Train Epoch 108.5 var loss 797819.8125 reconstruction mse 728899.3125 imputation mse 1.0351426601409912\n",
      "Train Epoch 108.6 var loss 797335.75 reconstruction mse 728959.4375 imputation mse 1.0351219177246094\n",
      "Train Epoch 108.7 var loss 797632.5 reconstruction mse 728737.8125 imputation mse 1.035133719444275\n",
      "Train Epoch 108.8 var loss 797470.5625 reconstruction mse 728783.625 imputation mse 1.034063458442688\n",
      "Train Epoch 108.9 var loss 796825.125 reconstruction mse 728896.0625 imputation mse 1.0343817472457886\n",
      "Train Epoch 109.0 var loss 791722.4375 reconstruction mse 724381.4375 imputation mse 1.0100195407867432\n",
      "Train Epoch 109.1 var loss 797700.0625 reconstruction mse 729554.4375 imputation mse 1.0331997871398926\n",
      "Train Epoch 109.2 var loss 798103.5625 reconstruction mse 730013.6875 imputation mse 1.0346581935882568\n",
      "Train Epoch 109.3 var loss 796973.625 reconstruction mse 729292.1875 imputation mse 1.0336774587631226\n",
      "Train Epoch 109.4 var loss 796492.4375 reconstruction mse 729011.1875 imputation mse 1.032220721244812\n",
      "Train Epoch 109.5 var loss 796712.9375 reconstruction mse 729507.3125 imputation mse 1.0329029560089111\n",
      "Train Epoch 109.6 var loss 797519.375 reconstruction mse 729816.25 imputation mse 1.0344916582107544\n",
      "Train Epoch 109.7 var loss 796779.75 reconstruction mse 728737.1875 imputation mse 1.0323411226272583\n",
      "Train Epoch 109.8 var loss 797388.125 reconstruction mse 729381.6875 imputation mse 1.0321364402770996\n",
      "Train Epoch 109.9 var loss 796467.625 reconstruction mse 728830.9375 imputation mse 1.0325342416763306\n",
      "Train Epoch 110.0 var loss 791485.4375 reconstruction mse 722882.375 imputation mse 1.011793851852417\n",
      "Train Epoch 110.1 var loss 796102.0625 reconstruction mse 728631.4375 imputation mse 1.0347318649291992\n",
      "Train Epoch 110.2 var loss 796124.25 reconstruction mse 729368.5 imputation mse 1.0359843969345093\n",
      "Train Epoch 110.3 var loss 795675.875 reconstruction mse 728147.3125 imputation mse 1.0342715978622437\n",
      "Train Epoch 110.4 var loss 795219.4375 reconstruction mse 728233.0 imputation mse 1.0341105461120605\n",
      "Train Epoch 110.5 var loss 795448.0625 reconstruction mse 727968.0 imputation mse 1.0338754653930664\n",
      "Train Epoch 110.6 var loss 795656.625 reconstruction mse 728836.25 imputation mse 1.0345748662948608\n",
      "Train Epoch 110.7 var loss 795815.9375 reconstruction mse 728320.5 imputation mse 1.0328996181488037\n",
      "Train Epoch 110.8 var loss 795649.25 reconstruction mse 728525.875 imputation mse 1.034752368927002\n",
      "Train Epoch 110.9 var loss 794407.5625 reconstruction mse 727793.125 imputation mse 1.0340027809143066\n",
      "====> Test imputation mse: 1.03995574\n",
      "====> Test imputation mse: 1.04501379\n",
      "====> Test imputation mse: 1.04205608\n",
      "Train Epoch 111.0 var loss 791765.6875 reconstruction mse 724932.0 imputation mse 1.0200622081756592\n",
      "Train Epoch 111.1 var loss 796724.6875 reconstruction mse 729992.5 imputation mse 1.0434117317199707\n",
      "Train Epoch 111.2 var loss 795723.625 reconstruction mse 729272.5 imputation mse 1.0411953926086426\n",
      "Train Epoch 111.3 var loss 796728.4375 reconstruction mse 729051.0625 imputation mse 1.03956139087677\n",
      "Train Epoch 111.4 var loss 795955.1875 reconstruction mse 729050.1875 imputation mse 1.0411772727966309\n",
      "Train Epoch 111.5 var loss 795129.5625 reconstruction mse 728241.0 imputation mse 1.0402472019195557\n",
      "Train Epoch 111.6 var loss 795921.125 reconstruction mse 729249.3125 imputation mse 1.040253758430481\n",
      "Train Epoch 111.7 var loss 795311.5 reconstruction mse 728540.0625 imputation mse 1.0389615297317505\n",
      "Train Epoch 111.8 var loss 796383.4375 reconstruction mse 728971.6875 imputation mse 1.040352702140808\n",
      "Train Epoch 111.9 var loss 795264.9375 reconstruction mse 729163.3125 imputation mse 1.0397411584854126\n",
      "Train Epoch 112.0 var loss 791587.0 reconstruction mse 724968.75 imputation mse 1.0142834186553955\n",
      "Train Epoch 112.1 var loss 796421.8125 reconstruction mse 730172.375 imputation mse 1.036145806312561\n",
      "Train Epoch 112.2 var loss 796678.4375 reconstruction mse 729811.125 imputation mse 1.0365769863128662\n",
      "Train Epoch 112.3 var loss 796508.875 reconstruction mse 729470.0625 imputation mse 1.0358023643493652\n",
      "Train Epoch 112.4 var loss 796436.5 reconstruction mse 729632.25 imputation mse 1.035815715789795\n",
      "Train Epoch 112.5 var loss 797360.0 reconstruction mse 729573.9375 imputation mse 1.0345544815063477\n",
      "Train Epoch 112.6 var loss 795103.0 reconstruction mse 729323.25 imputation mse 1.033771276473999\n",
      "Train Epoch 112.7 var loss 796069.625 reconstruction mse 728872.5 imputation mse 1.0340914726257324\n",
      "Train Epoch 112.8 var loss 795818.75 reconstruction mse 729208.6875 imputation mse 1.0353472232818604\n",
      "Train Epoch 112.9 var loss 795270.0 reconstruction mse 729275.4375 imputation mse 1.033667802810669\n",
      "Train Epoch 113.0 var loss 791798.8125 reconstruction mse 724350.5 imputation mse 1.0112590789794922\n",
      "Train Epoch 113.1 var loss 792649.75 reconstruction mse 727046.625 imputation mse 1.0323163270950317\n",
      "Train Epoch 113.2 var loss 793662.3125 reconstruction mse 727158.625 imputation mse 1.0332486629486084\n",
      "Train Epoch 113.3 var loss 794177.25 reconstruction mse 727082.3125 imputation mse 1.0332621335983276\n",
      "Train Epoch 113.4 var loss 794108.6875 reconstruction mse 727219.125 imputation mse 1.034594178199768\n",
      "Train Epoch 113.5 var loss 793702.125 reconstruction mse 726923.9375 imputation mse 1.0313160419464111\n",
      "Train Epoch 113.6 var loss 793066.5625 reconstruction mse 726403.625 imputation mse 1.0337867736816406\n",
      "Train Epoch 113.7 var loss 792706.1875 reconstruction mse 726289.0 imputation mse 1.031402826309204\n",
      "Train Epoch 113.8 var loss 792655.875 reconstruction mse 726202.875 imputation mse 1.0314390659332275\n",
      "Train Epoch 113.9 var loss 791937.5625 reconstruction mse 726229.5 imputation mse 1.0309360027313232\n",
      "Train Epoch 114.0 var loss 790092.1875 reconstruction mse 723591.875 imputation mse 1.0089774131774902\n",
      "Train Epoch 114.1 var loss 793545.4375 reconstruction mse 727799.9375 imputation mse 1.0303722620010376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 114.2 var loss 793300.25 reconstruction mse 727433.75 imputation mse 1.029515027999878\n",
      "Train Epoch 114.3 var loss 793969.75 reconstruction mse 727581.0 imputation mse 1.0295621156692505\n",
      "Train Epoch 114.4 var loss 793111.875 reconstruction mse 727131.1875 imputation mse 1.0303395986557007\n",
      "Train Epoch 114.5 var loss 792725.9375 reconstruction mse 726651.4375 imputation mse 1.0285089015960693\n",
      "Train Epoch 114.6 var loss 792551.1875 reconstruction mse 726687.3125 imputation mse 1.0300794839859009\n",
      "Train Epoch 114.7 var loss 791290.9375 reconstruction mse 726035.5625 imputation mse 1.0283366441726685\n",
      "Train Epoch 114.8 var loss 792211.0 reconstruction mse 726365.9375 imputation mse 1.0276929140090942\n",
      "Train Epoch 114.9 var loss 793550.75 reconstruction mse 727549.875 imputation mse 1.02935791015625\n",
      "Train Epoch 115.0 var loss 790956.0 reconstruction mse 725716.125 imputation mse 1.0165162086486816\n",
      "Train Epoch 115.1 var loss 795874.875 reconstruction mse 731326.3125 imputation mse 1.0365314483642578\n",
      "Train Epoch 115.2 var loss 796488.3125 reconstruction mse 731987.75 imputation mse 1.0377908945083618\n",
      "Train Epoch 115.3 var loss 797912.5625 reconstruction mse 731889.9375 imputation mse 1.0379713773727417\n",
      "Train Epoch 115.4 var loss 795683.75 reconstruction mse 731266.0625 imputation mse 1.037123680114746\n",
      "Train Epoch 115.5 var loss 795822.875 reconstruction mse 731066.125 imputation mse 1.0362401008605957\n",
      "Train Epoch 115.6 var loss 795678.5625 reconstruction mse 730686.5 imputation mse 1.0354429483413696\n",
      "Train Epoch 115.7 var loss 794925.3125 reconstruction mse 730533.1875 imputation mse 1.0345869064331055\n",
      "Train Epoch 115.8 var loss 796241.25 reconstruction mse 731377.3125 imputation mse 1.0364793539047241\n",
      "Train Epoch 115.9 var loss 796253.375 reconstruction mse 731223.125 imputation mse 1.0350271463394165\n",
      "Train Epoch 116.0 var loss 789277.375 reconstruction mse 724805.625 imputation mse 1.0106158256530762\n",
      "Train Epoch 116.1 var loss 792277.625 reconstruction mse 727958.6875 imputation mse 1.0311274528503418\n",
      "Train Epoch 116.2 var loss 792860.625 reconstruction mse 728536.625 imputation mse 1.032713770866394\n",
      "Train Epoch 116.3 var loss 792445.875 reconstruction mse 728414.0 imputation mse 1.0327415466308594\n",
      "Train Epoch 116.4 var loss 792615.875 reconstruction mse 728018.9375 imputation mse 1.0307748317718506\n",
      "Train Epoch 116.5 var loss 792253.0 reconstruction mse 728124.4375 imputation mse 1.0332056283950806\n",
      "Train Epoch 116.6 var loss 791308.1875 reconstruction mse 727131.9375 imputation mse 1.0305248498916626\n",
      "Train Epoch 116.7 var loss 791241.5625 reconstruction mse 727716.25 imputation mse 1.031657099723816\n",
      "Train Epoch 116.8 var loss 791091.125 reconstruction mse 727279.6875 imputation mse 1.029540777206421\n",
      "Train Epoch 116.9 var loss 790941.6875 reconstruction mse 727029.5625 imputation mse 1.03105890750885\n",
      "Train Epoch 117.0 var loss 788713.125 reconstruction mse 724916.375 imputation mse 1.0163400173187256\n",
      "Train Epoch 117.1 var loss 790445.875 reconstruction mse 726405.9375 imputation mse 1.0353972911834717\n",
      "Train Epoch 117.2 var loss 790284.125 reconstruction mse 726489.1875 imputation mse 1.0357745885849\n",
      "Train Epoch 117.3 var loss 790276.125 reconstruction mse 726147.25 imputation mse 1.0350342988967896\n",
      "Train Epoch 117.4 var loss 790388.625 reconstruction mse 726521.8125 imputation mse 1.0365920066833496\n",
      "Train Epoch 117.5 var loss 790814.375 reconstruction mse 726421.5625 imputation mse 1.036714792251587\n",
      "Train Epoch 117.6 var loss 789884.625 reconstruction mse 725967.5 imputation mse 1.0353156328201294\n",
      "Train Epoch 117.7 var loss 790516.625 reconstruction mse 726684.875 imputation mse 1.0351158380508423\n",
      "Train Epoch 117.8 var loss 790748.0 reconstruction mse 726259.375 imputation mse 1.0357102155685425\n",
      "Train Epoch 117.9 var loss 789547.6875 reconstruction mse 726062.5625 imputation mse 1.033003807067871\n",
      "Train Epoch 118.0 var loss 787857.375 reconstruction mse 724383.8125 imputation mse 1.014786958694458\n",
      "Train Epoch 118.1 var loss 793479.0 reconstruction mse 729817.4375 imputation mse 1.034747838973999\n",
      "Train Epoch 118.2 var loss 793864.25 reconstruction mse 729761.9375 imputation mse 1.0342621803283691\n",
      "Train Epoch 118.3 var loss 792278.125 reconstruction mse 729212.625 imputation mse 1.0335901975631714\n",
      "Train Epoch 118.4 var loss 791770.625 reconstruction mse 728516.375 imputation mse 1.0330127477645874\n",
      "Train Epoch 118.5 var loss 792051.375 reconstruction mse 728792.1875 imputation mse 1.0324596166610718\n",
      "Train Epoch 118.6 var loss 792451.875 reconstruction mse 729106.8125 imputation mse 1.0334067344665527\n",
      "Train Epoch 118.7 var loss 791935.375 reconstruction mse 728624.375 imputation mse 1.033111572265625\n",
      "Train Epoch 118.8 var loss 792070.125 reconstruction mse 729113.5625 imputation mse 1.0333738327026367\n",
      "Train Epoch 118.9 var loss 792853.125 reconstruction mse 729594.0625 imputation mse 1.033082127571106\n",
      "Train Epoch 119.0 var loss 787383.4375 reconstruction mse 723767.25 imputation mse 1.010404109954834\n",
      "Train Epoch 119.1 var loss 788597.5625 reconstruction mse 725411.8125 imputation mse 1.0283557176589966\n",
      "Train Epoch 119.2 var loss 788326.8125 reconstruction mse 725058.0 imputation mse 1.0294867753982544\n",
      "Train Epoch 119.3 var loss 788564.3125 reconstruction mse 725116.9375 imputation mse 1.0283236503601074\n",
      "Train Epoch 119.4 var loss 788834.4375 reconstruction mse 725486.4375 imputation mse 1.0295404195785522\n",
      "Train Epoch 119.5 var loss 788855.4375 reconstruction mse 724962.125 imputation mse 1.0300836563110352\n",
      "Train Epoch 119.6 var loss 788541.3125 reconstruction mse 724914.9375 imputation mse 1.0280365943908691\n",
      "Train Epoch 119.7 var loss 788496.625 reconstruction mse 724923.4375 imputation mse 1.026879906654358\n",
      "Train Epoch 119.8 var loss 788906.125 reconstruction mse 725465.4375 imputation mse 1.0286645889282227\n",
      "Train Epoch 119.9 var loss 787328.125 reconstruction mse 724397.1875 imputation mse 1.0285158157348633\n",
      "Train Epoch 120.0 var loss 785524.5625 reconstruction mse 722192.125 imputation mse 1.009879469871521\n",
      "Train Epoch 120.1 var loss 789631.25 reconstruction mse 726792.125 imputation mse 1.0312764644622803\n",
      "Train Epoch 120.2 var loss 790021.1875 reconstruction mse 726730.9375 imputation mse 1.0297410488128662\n",
      "Train Epoch 120.3 var loss 789554.875 reconstruction mse 726162.5625 imputation mse 1.0300503969192505\n",
      "Train Epoch 120.4 var loss 789872.6875 reconstruction mse 726331.875 imputation mse 1.0300816297531128\n",
      "Train Epoch 120.5 var loss 789744.6875 reconstruction mse 726662.25 imputation mse 1.0308257341384888\n",
      "Train Epoch 120.6 var loss 789316.9375 reconstruction mse 725684.0 imputation mse 1.0282542705535889\n",
      "Train Epoch 120.7 var loss 788298.0625 reconstruction mse 726031.625 imputation mse 1.0303457975387573\n",
      "Train Epoch 120.8 var loss 789026.875 reconstruction mse 726170.3125 imputation mse 1.0293492078781128\n",
      "Train Epoch 120.9 var loss 788743.25 reconstruction mse 725913.875 imputation mse 1.0280516147613525\n",
      "====> Test imputation mse: 1.05668056\n",
      "====> Test imputation mse: 1.04427731\n",
      "====> Test imputation mse: 1.03771996\n",
      "Train Epoch 121.0 var loss 784692.0625 reconstruction mse 722194.5625 imputation mse 1.0116294622421265\n",
      "Train Epoch 121.1 var loss 789105.625 reconstruction mse 727493.125 imputation mse 1.0336687564849854\n",
      "Train Epoch 121.2 var loss 788925.125 reconstruction mse 726635.6875 imputation mse 1.0309603214263916\n",
      "Train Epoch 121.3 var loss 789619.875 reconstruction mse 726876.1875 imputation mse 1.0327427387237549\n",
      "Train Epoch 121.4 var loss 788577.75 reconstruction mse 726425.0 imputation mse 1.0312585830688477\n",
      "Train Epoch 121.5 var loss 789055.5 reconstruction mse 726844.625 imputation mse 1.0331681966781616\n",
      "Train Epoch 121.6 var loss 789672.0 reconstruction mse 727106.1875 imputation mse 1.0323630571365356\n",
      "Train Epoch 121.7 var loss 788409.25 reconstruction mse 726762.9375 imputation mse 1.0324106216430664\n",
      "Train Epoch 121.8 var loss 788597.625 reconstruction mse 727176.5 imputation mse 1.0331381559371948\n",
      "Train Epoch 121.9 var loss 788477.6875 reconstruction mse 726637.0 imputation mse 1.031135082244873\n",
      "Train Epoch 122.0 var loss 786318.1875 reconstruction mse 724569.125 imputation mse 1.0160619020462036\n",
      "Train Epoch 122.1 var loss 792724.5625 reconstruction mse 729534.5 imputation mse 1.0354745388031006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 122.2 var loss 792775.6875 reconstruction mse 730628.0 imputation mse 1.036392331123352\n",
      "Train Epoch 122.3 var loss 792257.5625 reconstruction mse 729713.3125 imputation mse 1.0354963541030884\n",
      "Train Epoch 122.4 var loss 792796.9375 reconstruction mse 730132.75 imputation mse 1.0350977182388306\n",
      "Train Epoch 122.5 var loss 792059.125 reconstruction mse 729905.875 imputation mse 1.034790277481079\n",
      "Train Epoch 122.6 var loss 792904.125 reconstruction mse 730434.875 imputation mse 1.0366401672363281\n",
      "Train Epoch 122.7 var loss 791788.0625 reconstruction mse 729748.125 imputation mse 1.0347641706466675\n",
      "Train Epoch 122.8 var loss 791438.8125 reconstruction mse 729916.3125 imputation mse 1.0348807573318481\n",
      "Train Epoch 122.9 var loss 790904.6875 reconstruction mse 729648.4375 imputation mse 1.0350995063781738\n",
      "Train Epoch 123.0 var loss 786042.375 reconstruction mse 724666.75 imputation mse 1.0124913454055786\n",
      "Train Epoch 123.1 var loss 789266.875 reconstruction mse 727684.4375 imputation mse 1.0342177152633667\n",
      "Train Epoch 123.2 var loss 788444.625 reconstruction mse 726754.25 imputation mse 1.0323939323425293\n",
      "Train Epoch 123.3 var loss 788906.375 reconstruction mse 727035.5 imputation mse 1.0337378978729248\n",
      "Train Epoch 123.4 var loss 787570.25 reconstruction mse 726886.625 imputation mse 1.0337703227996826\n",
      "Train Epoch 123.5 var loss 788888.25 reconstruction mse 727183.1875 imputation mse 1.0348050594329834\n",
      "Train Epoch 123.6 var loss 787865.5625 reconstruction mse 726453.4375 imputation mse 1.0318355560302734\n",
      "Train Epoch 123.7 var loss 787642.25 reconstruction mse 726528.875 imputation mse 1.0320148468017578\n",
      "Train Epoch 123.8 var loss 786698.5625 reconstruction mse 725982.1875 imputation mse 1.0298523902893066\n",
      "Train Epoch 123.9 var loss 788464.0 reconstruction mse 726757.875 imputation mse 1.0324273109436035\n",
      "Train Epoch 124.0 var loss 784015.25 reconstruction mse 723077.9375 imputation mse 1.000893473625183\n",
      "Train Epoch 124.1 var loss 786434.0625 reconstruction mse 724935.5 imputation mse 1.0224034786224365\n",
      "Train Epoch 124.2 var loss 785435.0 reconstruction mse 724834.0625 imputation mse 1.0222771167755127\n",
      "Train Epoch 124.3 var loss 786116.5 reconstruction mse 725220.5 imputation mse 1.0233891010284424\n",
      "Train Epoch 124.4 var loss 786095.875 reconstruction mse 724786.25 imputation mse 1.022499442100525\n",
      "Train Epoch 124.5 var loss 785219.4375 reconstruction mse 724566.1875 imputation mse 1.0201252698898315\n",
      "Train Epoch 124.6 var loss 785876.8125 reconstruction mse 724410.9375 imputation mse 1.0227534770965576\n",
      "Train Epoch 124.7 var loss 785680.3125 reconstruction mse 724846.5625 imputation mse 1.0237669944763184\n",
      "Train Epoch 124.8 var loss 785670.5625 reconstruction mse 724434.4375 imputation mse 1.0226448774337769\n",
      "Train Epoch 124.9 var loss 784930.625 reconstruction mse 724126.9375 imputation mse 1.0214600563049316\n",
      "Train Epoch 125.0 var loss 785319.3125 reconstruction mse 724869.875 imputation mse 1.0167121887207031\n",
      "Train Epoch 125.1 var loss 790480.8125 reconstruction mse 729569.25 imputation mse 1.0375018119812012\n",
      "Train Epoch 125.2 var loss 790686.6875 reconstruction mse 729867.6875 imputation mse 1.0372936725616455\n",
      "Train Epoch 125.3 var loss 790110.4375 reconstruction mse 728743.875 imputation mse 1.0358178615570068\n",
      "Train Epoch 125.4 var loss 790691.0625 reconstruction mse 729517.1875 imputation mse 1.0357290506362915\n",
      "Train Epoch 125.5 var loss 789898.125 reconstruction mse 728919.75 imputation mse 1.0350899696350098\n",
      "Train Epoch 125.6 var loss 790560.25 reconstruction mse 729582.0 imputation mse 1.035946249961853\n",
      "Train Epoch 125.7 var loss 789536.9375 reconstruction mse 728870.375 imputation mse 1.0346535444259644\n",
      "Train Epoch 125.8 var loss 790546.1875 reconstruction mse 728956.125 imputation mse 1.035025715827942\n",
      "Train Epoch 125.9 var loss 789524.8125 reconstruction mse 729091.625 imputation mse 1.035220742225647\n",
      "Train Epoch 126.0 var loss 785120.0 reconstruction mse 724949.4375 imputation mse 1.0115044116973877\n",
      "Train Epoch 126.1 var loss 786132.5625 reconstruction mse 726075.8125 imputation mse 1.030281901359558\n",
      "Train Epoch 126.2 var loss 785798.4375 reconstruction mse 726162.75 imputation mse 1.0304303169250488\n",
      "Train Epoch 126.3 var loss 786636.8125 reconstruction mse 726346.375 imputation mse 1.0290480852127075\n",
      "Train Epoch 126.4 var loss 786199.4375 reconstruction mse 725931.8125 imputation mse 1.0291599035263062\n",
      "Train Epoch 126.5 var loss 785905.375 reconstruction mse 725762.8125 imputation mse 1.0259485244750977\n",
      "Train Epoch 126.6 var loss 786367.625 reconstruction mse 725993.125 imputation mse 1.0297905206680298\n",
      "Train Epoch 126.7 var loss 786149.3125 reconstruction mse 725747.5 imputation mse 1.0276885032653809\n",
      "Train Epoch 126.8 var loss 786097.5 reconstruction mse 726051.25 imputation mse 1.028562068939209\n",
      "Train Epoch 126.9 var loss 786420.625 reconstruction mse 726214.5625 imputation mse 1.0284479856491089\n",
      "Train Epoch 127.0 var loss 783590.25 reconstruction mse 723560.4375 imputation mse 1.010830283164978\n",
      "Train Epoch 127.1 var loss 785812.625 reconstruction mse 724514.625 imputation mse 1.0307196378707886\n",
      "Train Epoch 127.2 var loss 785475.875 reconstruction mse 725191.9375 imputation mse 1.0318410396575928\n",
      "Train Epoch 127.3 var loss 786164.5 reconstruction mse 725675.5 imputation mse 1.0315731763839722\n",
      "Train Epoch 127.4 var loss 785323.75 reconstruction mse 725036.4375 imputation mse 1.0311185121536255\n",
      "Train Epoch 127.5 var loss 785101.9375 reconstruction mse 725114.5 imputation mse 1.0310794115066528\n",
      "Train Epoch 127.6 var loss 785137.8125 reconstruction mse 724933.5625 imputation mse 1.0317236185073853\n",
      "Train Epoch 127.7 var loss 785090.25 reconstruction mse 724964.75 imputation mse 1.0294771194458008\n",
      "Train Epoch 127.8 var loss 784999.4375 reconstruction mse 724847.5 imputation mse 1.0314724445343018\n",
      "Train Epoch 127.9 var loss 785115.3125 reconstruction mse 724676.5625 imputation mse 1.0304811000823975\n",
      "Train Epoch 128.0 var loss 781090.25 reconstruction mse 721553.0 imputation mse 1.0003018379211426\n",
      "Train Epoch 128.1 var loss 781788.375 reconstruction mse 723009.1875 imputation mse 1.0193918943405151\n",
      "Train Epoch 128.2 var loss 782383.75 reconstruction mse 722785.1875 imputation mse 1.0188624858856201\n",
      "Train Epoch 128.3 var loss 782309.1875 reconstruction mse 722553.0 imputation mse 1.0201411247253418\n",
      "Train Epoch 128.4 var loss 781757.25 reconstruction mse 722322.375 imputation mse 1.0172958374023438\n",
      "Train Epoch 128.5 var loss 782010.3125 reconstruction mse 721983.75 imputation mse 1.0167288780212402\n",
      "Train Epoch 128.6 var loss 781298.875 reconstruction mse 722348.9375 imputation mse 1.0185890197753906\n",
      "Train Epoch 128.7 var loss 781924.5625 reconstruction mse 722514.1875 imputation mse 1.0168895721435547\n",
      "Train Epoch 128.8 var loss 780797.25 reconstruction mse 721671.25 imputation mse 1.0205156803131104\n",
      "Train Epoch 128.9 var loss 781482.875 reconstruction mse 722288.25 imputation mse 1.0182490348815918\n",
      "Train Epoch 129.0 var loss 780290.25 reconstruction mse 720466.875 imputation mse 1.007966160774231\n",
      "Train Epoch 129.1 var loss 786691.5 reconstruction mse 726334.875 imputation mse 1.029503345489502\n",
      "Train Epoch 129.2 var loss 787032.5 reconstruction mse 726530.5625 imputation mse 1.0299230813980103\n",
      "Train Epoch 129.3 var loss 786360.5625 reconstruction mse 726641.0625 imputation mse 1.0307120084762573\n",
      "Train Epoch 129.4 var loss 785485.1875 reconstruction mse 725841.9375 imputation mse 1.0283387899398804\n",
      "Train Epoch 129.5 var loss 785532.25 reconstruction mse 726149.6875 imputation mse 1.0291129350662231\n",
      "Train Epoch 129.6 var loss 786210.125 reconstruction mse 726698.3125 imputation mse 1.0296605825424194\n",
      "Train Epoch 129.7 var loss 786609.25 reconstruction mse 726631.625 imputation mse 1.0297070741653442\n",
      "Train Epoch 129.8 var loss 785747.8125 reconstruction mse 726136.4375 imputation mse 1.0281398296356201\n",
      "Train Epoch 129.9 var loss 786581.8125 reconstruction mse 726377.4375 imputation mse 1.0286214351654053\n",
      "Train Epoch 130.0 var loss 782540.375 reconstruction mse 723035.5 imputation mse 1.0149222612380981\n",
      "Train Epoch 130.1 var loss 786624.3125 reconstruction mse 727580.25 imputation mse 1.0340954065322876\n",
      "Train Epoch 130.2 var loss 786523.8125 reconstruction mse 727192.125 imputation mse 1.0323920249938965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 130.3 var loss 785620.375 reconstruction mse 727243.375 imputation mse 1.033089280128479\n",
      "Train Epoch 130.4 var loss 786130.375 reconstruction mse 727000.6875 imputation mse 1.0316580533981323\n",
      "Train Epoch 130.5 var loss 786608.3125 reconstruction mse 727063.75 imputation mse 1.0323405265808105\n",
      "Train Epoch 130.6 var loss 785715.9375 reconstruction mse 726733.5625 imputation mse 1.030971646308899\n",
      "Train Epoch 130.7 var loss 785923.25 reconstruction mse 726932.9375 imputation mse 1.0319277048110962\n",
      "Train Epoch 130.8 var loss 786029.375 reconstruction mse 727256.3125 imputation mse 1.0329792499542236\n",
      "Train Epoch 130.9 var loss 786244.9375 reconstruction mse 727152.25 imputation mse 1.0314135551452637\n",
      "====> Test imputation mse: 1.02795005\n",
      "====> Test imputation mse: 1.03621948\n",
      "====> Test imputation mse: 1.05188572\n",
      "Train Epoch 131.0 var loss 781667.1875 reconstruction mse 722818.0 imputation mse 1.0075384378433228\n",
      "Train Epoch 131.1 var loss 786330.25 reconstruction mse 727055.625 imputation mse 1.0250269174575806\n",
      "Train Epoch 131.2 var loss 785298.8125 reconstruction mse 727098.125 imputation mse 1.0261131525039673\n",
      "Train Epoch 131.3 var loss 786556.75 reconstruction mse 727746.5625 imputation mse 1.0268476009368896\n",
      "Train Epoch 131.4 var loss 785927.8125 reconstruction mse 727152.75 imputation mse 1.0263017416000366\n",
      "Train Epoch 131.5 var loss 785162.375 reconstruction mse 727330.5625 imputation mse 1.0260084867477417\n",
      "Train Epoch 131.6 var loss 784977.625 reconstruction mse 726952.5625 imputation mse 1.0252934694290161\n",
      "Train Epoch 131.7 var loss 784928.125 reconstruction mse 726849.0625 imputation mse 1.0253905057907104\n",
      "Train Epoch 131.8 var loss 785288.8125 reconstruction mse 726721.0 imputation mse 1.024722933769226\n",
      "Train Epoch 131.9 var loss 785317.3125 reconstruction mse 727040.4375 imputation mse 1.0242929458618164\n",
      "Train Epoch 132.0 var loss 782067.3125 reconstruction mse 723717.5625 imputation mse 1.0091232061386108\n",
      "Train Epoch 132.1 var loss 784564.0625 reconstruction mse 726044.4375 imputation mse 1.0267424583435059\n",
      "Train Epoch 132.2 var loss 784233.8125 reconstruction mse 725630.625 imputation mse 1.0264723300933838\n",
      "Train Epoch 132.3 var loss 784493.5 reconstruction mse 725862.5 imputation mse 1.0272783041000366\n",
      "Train Epoch 132.4 var loss 784842.4375 reconstruction mse 725789.5625 imputation mse 1.0274492502212524\n",
      "Train Epoch 132.5 var loss 784322.5 reconstruction mse 725763.0625 imputation mse 1.0257456302642822\n",
      "Train Epoch 132.6 var loss 783373.625 reconstruction mse 725211.4375 imputation mse 1.0257245302200317\n",
      "Train Epoch 132.7 var loss 784475.3125 reconstruction mse 725795.125 imputation mse 1.026314616203308\n",
      "Train Epoch 132.8 var loss 784808.1875 reconstruction mse 725596.6875 imputation mse 1.02621328830719\n",
      "Train Epoch 132.9 var loss 784224.125 reconstruction mse 725641.5625 imputation mse 1.0265138149261475\n",
      "Train Epoch 133.0 var loss 783226.9375 reconstruction mse 724802.5 imputation mse 1.0155106782913208\n",
      "Train Epoch 133.1 var loss 786922.25 reconstruction mse 728866.75 imputation mse 1.0329138040542603\n",
      "Train Epoch 133.2 var loss 787064.6875 reconstruction mse 729104.4375 imputation mse 1.0337294340133667\n",
      "Train Epoch 133.3 var loss 788114.25 reconstruction mse 729203.8125 imputation mse 1.034454345703125\n",
      "Train Epoch 133.4 var loss 786503.75 reconstruction mse 728980.5 imputation mse 1.0338208675384521\n",
      "Train Epoch 133.5 var loss 786553.1875 reconstruction mse 728558.25 imputation mse 1.0329381227493286\n",
      "Train Epoch 133.6 var loss 786260.375 reconstruction mse 728522.9375 imputation mse 1.0322728157043457\n",
      "Train Epoch 133.7 var loss 786256.75 reconstruction mse 728598.75 imputation mse 1.0330184698104858\n",
      "Train Epoch 133.8 var loss 788054.8125 reconstruction mse 728875.6875 imputation mse 1.033544898033142\n",
      "Train Epoch 133.9 var loss 787160.5 reconstruction mse 729075.375 imputation mse 1.0329084396362305\n",
      "Train Epoch 134.0 var loss 783375.3125 reconstruction mse 724911.9375 imputation mse 1.0114381313323975\n",
      "Train Epoch 134.1 var loss 786717.5625 reconstruction mse 729276.5 imputation mse 1.0292448997497559\n",
      "Train Epoch 134.2 var loss 786344.625 reconstruction mse 728614.0625 imputation mse 1.0281281471252441\n",
      "Train Epoch 134.3 var loss 786001.1875 reconstruction mse 728616.0 imputation mse 1.029759168624878\n",
      "Train Epoch 134.4 var loss 786898.1875 reconstruction mse 729248.75 imputation mse 1.0303864479064941\n",
      "Train Epoch 134.5 var loss 786631.4375 reconstruction mse 728888.5625 imputation mse 1.0292673110961914\n",
      "Train Epoch 134.6 var loss 786903.4375 reconstruction mse 728609.4375 imputation mse 1.0294215679168701\n",
      "Train Epoch 134.7 var loss 786267.375 reconstruction mse 728596.9375 imputation mse 1.0281156301498413\n",
      "Train Epoch 134.8 var loss 786344.0 reconstruction mse 728734.25 imputation mse 1.0273187160491943\n",
      "Train Epoch 134.9 var loss 786434.5 reconstruction mse 728577.6875 imputation mse 1.0287476778030396\n",
      "Train Epoch 135.0 var loss 782248.75 reconstruction mse 724911.9375 imputation mse 1.0143662691116333\n",
      "Train Epoch 135.1 var loss 787787.8125 reconstruction mse 730633.625 imputation mse 1.0326346158981323\n",
      "Train Epoch 135.2 var loss 788207.75 reconstruction mse 730613.25 imputation mse 1.0321581363677979\n",
      "Train Epoch 135.3 var loss 787538.875 reconstruction mse 729939.5625 imputation mse 1.0327223539352417\n",
      "Train Epoch 135.4 var loss 787621.875 reconstruction mse 730325.5625 imputation mse 1.031637191772461\n",
      "Train Epoch 135.5 var loss 787954.9375 reconstruction mse 730047.5625 imputation mse 1.0319944620132446\n",
      "Train Epoch 135.6 var loss 787494.9375 reconstruction mse 730163.875 imputation mse 1.0314691066741943\n",
      "Train Epoch 135.7 var loss 787125.875 reconstruction mse 729636.4375 imputation mse 1.0311357975006104\n",
      "Train Epoch 135.8 var loss 787639.25 reconstruction mse 730215.875 imputation mse 1.031250238418579\n",
      "Train Epoch 135.9 var loss 786948.9375 reconstruction mse 729578.6875 imputation mse 1.0302358865737915\n",
      "Train Epoch 136.0 var loss 782175.625 reconstruction mse 724897.9375 imputation mse 1.0107240676879883\n",
      "Train Epoch 136.1 var loss 783705.3125 reconstruction mse 727524.625 imputation mse 1.0277715921401978\n",
      "Train Epoch 136.2 var loss 784847.375 reconstruction mse 727987.0625 imputation mse 1.0284574031829834\n",
      "Train Epoch 136.3 var loss 784808.6875 reconstruction mse 728358.5625 imputation mse 1.0290858745574951\n",
      "Train Epoch 136.4 var loss 785210.875 reconstruction mse 728142.375 imputation mse 1.0292162895202637\n",
      "Train Epoch 136.5 var loss 784596.9375 reconstruction mse 728015.1875 imputation mse 1.0286394357681274\n",
      "Train Epoch 136.6 var loss 785088.375 reconstruction mse 728422.4375 imputation mse 1.028809666633606\n",
      "Train Epoch 136.7 var loss 784275.0 reconstruction mse 727597.125 imputation mse 1.0288711786270142\n",
      "Train Epoch 136.8 var loss 784635.1875 reconstruction mse 728011.375 imputation mse 1.0277879238128662\n",
      "Train Epoch 136.9 var loss 784475.0 reconstruction mse 727740.0 imputation mse 1.0283461809158325\n",
      "Train Epoch 137.0 var loss 782248.8125 reconstruction mse 725219.625 imputation mse 1.0153226852416992\n",
      "Train Epoch 137.1 var loss 786006.5 reconstruction mse 729645.6875 imputation mse 1.0327659845352173\n",
      "Train Epoch 137.2 var loss 786815.0625 reconstruction mse 729883.0625 imputation mse 1.0324504375457764\n",
      "Train Epoch 137.3 var loss 785802.625 reconstruction mse 729725.875 imputation mse 1.0328302383422852\n",
      "Train Epoch 137.4 var loss 786271.4375 reconstruction mse 729472.9375 imputation mse 1.032109022140503\n",
      "Train Epoch 137.5 var loss 786079.4375 reconstruction mse 730056.0 imputation mse 1.0327821969985962\n",
      "Train Epoch 137.6 var loss 785992.125 reconstruction mse 729706.8125 imputation mse 1.03166663646698\n",
      "Train Epoch 137.7 var loss 786007.375 reconstruction mse 729394.125 imputation mse 1.0308436155319214\n",
      "Train Epoch 137.8 var loss 786611.5 reconstruction mse 729928.375 imputation mse 1.0320111513137817\n",
      "Train Epoch 137.9 var loss 785950.5 reconstruction mse 729605.6875 imputation mse 1.0315346717834473\n",
      "Train Epoch 138.0 var loss 780837.9375 reconstruction mse 725070.625 imputation mse 1.0150772333145142\n",
      "Train Epoch 138.1 var loss 782297.5 reconstruction mse 726161.1875 imputation mse 1.0314055681228638\n",
      "Train Epoch 138.2 var loss 781662.6875 reconstruction mse 726046.125 imputation mse 1.0311750173568726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 138.3 var loss 781850.6875 reconstruction mse 725723.375 imputation mse 1.029941201210022\n",
      "Train Epoch 138.4 var loss 781432.0 reconstruction mse 725860.375 imputation mse 1.0318509340286255\n",
      "Train Epoch 138.5 var loss 781573.9375 reconstruction mse 725911.9375 imputation mse 1.0300980806350708\n",
      "Train Epoch 138.6 var loss 781548.875 reconstruction mse 725735.0625 imputation mse 1.0294854640960693\n",
      "Train Epoch 138.7 var loss 781347.1875 reconstruction mse 725762.3125 imputation mse 1.0305360555648804\n",
      "Train Epoch 138.8 var loss 781755.125 reconstruction mse 725668.5 imputation mse 1.0305265188217163\n",
      "Train Epoch 138.9 var loss 781332.25 reconstruction mse 725748.0 imputation mse 1.0295084714889526\n",
      "Train Epoch 139.0 var loss 779734.125 reconstruction mse 724613.9375 imputation mse 1.0109978914260864\n",
      "Train Epoch 139.1 var loss 780961.5 reconstruction mse 725915.625 imputation mse 1.0281729698181152\n",
      "Train Epoch 139.2 var loss 781187.1875 reconstruction mse 725757.0 imputation mse 1.0291502475738525\n",
      "Train Epoch 139.3 var loss 780390.25 reconstruction mse 725119.625 imputation mse 1.027341365814209\n",
      "Train Epoch 139.4 var loss 780369.1875 reconstruction mse 725413.625 imputation mse 1.0294294357299805\n",
      "Train Epoch 139.5 var loss 780617.0625 reconstruction mse 725674.125 imputation mse 1.0297517776489258\n",
      "Train Epoch 139.6 var loss 780821.1875 reconstruction mse 725133.8125 imputation mse 1.0267804861068726\n",
      "Train Epoch 139.7 var loss 780491.0625 reconstruction mse 725354.25 imputation mse 1.027866244316101\n",
      "Train Epoch 139.8 var loss 780298.9375 reconstruction mse 725039.1875 imputation mse 1.0279667377471924\n",
      "Train Epoch 139.9 var loss 780640.5 reconstruction mse 725257.6875 imputation mse 1.0282769203186035\n",
      "Train Epoch 140.0 var loss 778295.875 reconstruction mse 722371.3125 imputation mse 1.004798412322998\n",
      "Train Epoch 140.1 var loss 781419.8125 reconstruction mse 725012.25 imputation mse 1.025144338607788\n",
      "Train Epoch 140.2 var loss 780770.8125 reconstruction mse 724854.8125 imputation mse 1.0247529745101929\n",
      "Train Epoch 140.3 var loss 780851.125 reconstruction mse 725008.0625 imputation mse 1.0254034996032715\n",
      "Train Epoch 140.4 var loss 780372.5 reconstruction mse 724778.375 imputation mse 1.025704264640808\n",
      "Train Epoch 140.5 var loss 780549.9375 reconstruction mse 724527.25 imputation mse 1.0262304544448853\n",
      "Train Epoch 140.6 var loss 780157.625 reconstruction mse 724065.125 imputation mse 1.0226749181747437\n",
      "Train Epoch 140.7 var loss 779946.5 reconstruction mse 723980.25 imputation mse 1.0237287282943726\n",
      "Train Epoch 140.8 var loss 780728.375 reconstruction mse 724521.75 imputation mse 1.0243916511535645\n",
      "Train Epoch 140.9 var loss 779863.4375 reconstruction mse 724343.0625 imputation mse 1.0254336595535278\n",
      "====> Test imputation mse: 1.04496527\n",
      "====> Test imputation mse: 1.05592561\n",
      "====> Test imputation mse: 1.05199206\n",
      "Train Epoch 141.0 var loss 778619.0 reconstruction mse 721915.875 imputation mse 1.0081558227539062\n",
      "Train Epoch 141.1 var loss 783218.9375 reconstruction mse 726445.9375 imputation mse 1.0272489786148071\n",
      "Train Epoch 141.2 var loss 782959.25 reconstruction mse 726190.0 imputation mse 1.0263179540634155\n",
      "Train Epoch 141.3 var loss 782403.5 reconstruction mse 726471.0625 imputation mse 1.0265789031982422\n",
      "Train Epoch 141.4 var loss 782614.9375 reconstruction mse 726404.0625 imputation mse 1.0259596109390259\n",
      "Train Epoch 141.5 var loss 782556.375 reconstruction mse 726326.8125 imputation mse 1.0262922048568726\n",
      "Train Epoch 141.6 var loss 783218.625 reconstruction mse 726441.6875 imputation mse 1.0269980430603027\n",
      "Train Epoch 141.7 var loss 781144.3125 reconstruction mse 726087.625 imputation mse 1.02606999874115\n",
      "Train Epoch 141.8 var loss 782049.3125 reconstruction mse 725751.0 imputation mse 1.0243803262710571\n",
      "Train Epoch 141.9 var loss 781981.875 reconstruction mse 726114.25 imputation mse 1.0250381231307983\n",
      "Train Epoch 142.0 var loss 776881.0 reconstruction mse 721107.8125 imputation mse 1.0087802410125732\n",
      "Train Epoch 142.1 var loss 783255.0 reconstruction mse 727042.5 imputation mse 1.0278733968734741\n",
      "Train Epoch 142.2 var loss 781855.4375 reconstruction mse 726683.9375 imputation mse 1.0271652936935425\n",
      "Train Epoch 142.3 var loss 782056.0625 reconstruction mse 726772.75 imputation mse 1.0262548923492432\n",
      "Train Epoch 142.4 var loss 782419.25 reconstruction mse 727370.5625 imputation mse 1.0275191068649292\n",
      "Train Epoch 142.5 var loss 782277.0 reconstruction mse 726416.9375 imputation mse 1.026015043258667\n",
      "Train Epoch 142.6 var loss 782163.4375 reconstruction mse 726441.125 imputation mse 1.0261400938034058\n",
      "Train Epoch 142.7 var loss 781302.5625 reconstruction mse 726459.0 imputation mse 1.0255285501480103\n",
      "Train Epoch 142.8 var loss 781774.4375 reconstruction mse 726766.25 imputation mse 1.025856375694275\n",
      "Train Epoch 142.9 var loss 781928.625 reconstruction mse 726332.375 imputation mse 1.0251662731170654\n",
      "Train Epoch 143.0 var loss 775829.3125 reconstruction mse 720971.8125 imputation mse 1.0038275718688965\n",
      "Train Epoch 143.1 var loss 777726.75 reconstruction mse 722273.25 imputation mse 1.0216354131698608\n",
      "Train Epoch 143.2 var loss 777412.875 reconstruction mse 722284.5 imputation mse 1.0199741125106812\n",
      "Train Epoch 143.3 var loss 778602.125 reconstruction mse 722468.0625 imputation mse 1.019850254058838\n",
      "Train Epoch 143.4 var loss 777963.6875 reconstruction mse 722610.8125 imputation mse 1.0215052366256714\n",
      "Train Epoch 143.5 var loss 777289.8125 reconstruction mse 721641.0 imputation mse 1.0189692974090576\n",
      "Train Epoch 143.6 var loss 777628.125 reconstruction mse 722199.9375 imputation mse 1.0193647146224976\n",
      "Train Epoch 143.7 var loss 777712.875 reconstruction mse 722393.9375 imputation mse 1.0196073055267334\n",
      "Train Epoch 143.8 var loss 777164.875 reconstruction mse 722191.875 imputation mse 1.0196716785430908\n",
      "Train Epoch 143.9 var loss 777571.0625 reconstruction mse 722494.875 imputation mse 1.0201915502548218\n",
      "Train Epoch 144.0 var loss 774991.625 reconstruction mse 720529.9375 imputation mse 1.006677508354187\n",
      "Train Epoch 144.1 var loss 778102.0 reconstruction mse 722679.375 imputation mse 1.0253061056137085\n",
      "Train Epoch 144.2 var loss 778684.6875 reconstruction mse 723188.8125 imputation mse 1.0255542993545532\n",
      "Train Epoch 144.3 var loss 777063.25 reconstruction mse 722221.0 imputation mse 1.0241068601608276\n",
      "Train Epoch 144.4 var loss 777576.0625 reconstruction mse 722951.9375 imputation mse 1.023945689201355\n",
      "Train Epoch 144.5 var loss 777682.25 reconstruction mse 723006.9375 imputation mse 1.0259613990783691\n",
      "Train Epoch 144.6 var loss 777359.625 reconstruction mse 722301.0625 imputation mse 1.0237399339675903\n",
      "Train Epoch 144.7 var loss 777012.9375 reconstruction mse 722211.8125 imputation mse 1.0241972208023071\n",
      "Train Epoch 144.8 var loss 777848.3125 reconstruction mse 722606.375 imputation mse 1.0252214670181274\n",
      "Train Epoch 144.9 var loss 778134.0 reconstruction mse 722831.1875 imputation mse 1.0252119302749634\n",
      "Train Epoch 145.0 var loss 775107.875 reconstruction mse 719631.5 imputation mse 1.0044584274291992\n",
      "Train Epoch 145.1 var loss 779595.3125 reconstruction mse 724340.5 imputation mse 1.0212416648864746\n",
      "Train Epoch 145.2 var loss 779882.25 reconstruction mse 724429.5625 imputation mse 1.0217622518539429\n",
      "Train Epoch 145.3 var loss 777880.5625 reconstruction mse 724008.4375 imputation mse 1.0207878351211548\n",
      "Train Epoch 145.4 var loss 779311.1875 reconstruction mse 724623.125 imputation mse 1.0220063924789429\n",
      "Train Epoch 145.5 var loss 779177.75 reconstruction mse 724106.125 imputation mse 1.0205365419387817\n",
      "Train Epoch 145.6 var loss 778591.3125 reconstruction mse 724056.75 imputation mse 1.0211031436920166\n",
      "Train Epoch 145.7 var loss 779276.3125 reconstruction mse 724507.9375 imputation mse 1.0208427906036377\n",
      "Train Epoch 145.8 var loss 778838.1875 reconstruction mse 724070.25 imputation mse 1.0212920904159546\n",
      "Train Epoch 145.9 var loss 778512.1875 reconstruction mse 723863.125 imputation mse 1.0198220014572144\n",
      "Train Epoch 146.0 var loss 773233.3125 reconstruction mse 718792.375 imputation mse 1.0032156705856323\n",
      "Train Epoch 146.1 var loss 774904.0625 reconstruction mse 720513.375 imputation mse 1.0211445093154907\n",
      "Train Epoch 146.2 var loss 775232.8125 reconstruction mse 720242.4375 imputation mse 1.021164894104004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 146.3 var loss 775127.125 reconstruction mse 720451.1875 imputation mse 1.023151159286499\n",
      "Train Epoch 146.4 var loss 775113.25 reconstruction mse 720568.625 imputation mse 1.0216355323791504\n",
      "Train Epoch 146.5 var loss 775191.6875 reconstruction mse 720362.125 imputation mse 1.0219557285308838\n",
      "Train Epoch 146.6 var loss 774527.75 reconstruction mse 720217.0 imputation mse 1.024253249168396\n",
      "Train Epoch 146.7 var loss 774808.125 reconstruction mse 720431.6875 imputation mse 1.02267587184906\n",
      "Train Epoch 146.8 var loss 775075.625 reconstruction mse 720657.9375 imputation mse 1.0219347476959229\n",
      "Train Epoch 146.9 var loss 774717.125 reconstruction mse 720384.8125 imputation mse 1.0224870443344116\n",
      "Train Epoch 147.0 var loss 774841.625 reconstruction mse 721263.375 imputation mse 1.011479377746582\n",
      "Train Epoch 147.1 var loss 779432.5625 reconstruction mse 725304.6875 imputation mse 1.0294255018234253\n",
      "Train Epoch 147.2 var loss 779363.6875 reconstruction mse 725773.625 imputation mse 1.030678391456604\n",
      "Train Epoch 147.3 var loss 779410.125 reconstruction mse 725611.75 imputation mse 1.0294547080993652\n",
      "Train Epoch 147.4 var loss 778768.4375 reconstruction mse 725313.0 imputation mse 1.0281871557235718\n",
      "Train Epoch 147.5 var loss 778923.3125 reconstruction mse 725290.9375 imputation mse 1.0290886163711548\n",
      "Train Epoch 147.6 var loss 778811.1875 reconstruction mse 725698.8125 imputation mse 1.0277867317199707\n",
      "Train Epoch 147.7 var loss 779407.3125 reconstruction mse 724980.1875 imputation mse 1.0280475616455078\n",
      "Train Epoch 147.8 var loss 779260.3125 reconstruction mse 725161.1875 imputation mse 1.0273962020874023\n",
      "Train Epoch 147.9 var loss 778473.1875 reconstruction mse 725362.4375 imputation mse 1.0278431177139282\n",
      "Train Epoch 148.0 var loss 775638.0 reconstruction mse 721991.375 imputation mse 1.0106620788574219\n",
      "Train Epoch 148.1 var loss 777182.875 reconstruction mse 723943.1875 imputation mse 1.027984619140625\n",
      "Train Epoch 148.2 var loss 777767.125 reconstruction mse 724398.75 imputation mse 1.028138518333435\n",
      "Train Epoch 148.3 var loss 777308.0 reconstruction mse 724269.5 imputation mse 1.0290290117263794\n",
      "Train Epoch 148.4 var loss 778025.1875 reconstruction mse 724093.5 imputation mse 1.0275543928146362\n",
      "Train Epoch 148.5 var loss 777512.0 reconstruction mse 723848.875 imputation mse 1.0260785818099976\n",
      "Train Epoch 148.6 var loss 776740.75 reconstruction mse 724003.0625 imputation mse 1.027455449104309\n",
      "Train Epoch 148.7 var loss 777474.3125 reconstruction mse 723783.9375 imputation mse 1.027085304260254\n",
      "Train Epoch 148.8 var loss 777136.375 reconstruction mse 723589.125 imputation mse 1.025940179824829\n",
      "Train Epoch 148.9 var loss 776565.6875 reconstruction mse 723264.75 imputation mse 1.0268625020980835\n",
      "Train Epoch 149.0 var loss 775568.3125 reconstruction mse 721805.9375 imputation mse 1.0136536359786987\n",
      "Train Epoch 149.1 var loss 777287.625 reconstruction mse 723645.8125 imputation mse 1.0286654233932495\n",
      "Train Epoch 149.2 var loss 777663.9375 reconstruction mse 723717.1875 imputation mse 1.0289483070373535\n",
      "Train Epoch 149.3 var loss 776548.8125 reconstruction mse 723331.5625 imputation mse 1.02907133102417\n",
      "Train Epoch 149.4 var loss 776673.9375 reconstruction mse 723208.4375 imputation mse 1.0281809568405151\n",
      "Train Epoch 149.5 var loss 776345.75 reconstruction mse 722968.1875 imputation mse 1.0286715030670166\n",
      "Train Epoch 149.6 var loss 777589.8125 reconstruction mse 723732.6875 imputation mse 1.030078411102295\n",
      "Train Epoch 149.7 var loss 776718.25 reconstruction mse 723210.0 imputation mse 1.0290676355361938\n",
      "Train Epoch 149.8 var loss 777336.6875 reconstruction mse 722914.375 imputation mse 1.0277490615844727\n",
      "Train Epoch 149.9 var loss 776713.625 reconstruction mse 723420.9375 imputation mse 1.0298982858657837\n",
      "Train Epoch 150.0 var loss 773280.1875 reconstruction mse 720442.9375 imputation mse 0.9986578226089478\n",
      "Train Epoch 150.1 var loss 774589.5625 reconstruction mse 721414.0 imputation mse 1.0173004865646362\n",
      "Train Epoch 150.2 var loss 775057.3125 reconstruction mse 721240.0625 imputation mse 1.0167990922927856\n",
      "Train Epoch 150.3 var loss 775719.5625 reconstruction mse 721889.1875 imputation mse 1.0187087059020996\n",
      "Train Epoch 150.4 var loss 775133.5625 reconstruction mse 721321.625 imputation mse 1.015934705734253\n",
      "Train Epoch 150.5 var loss 774918.25 reconstruction mse 721720.6875 imputation mse 1.016409993171692\n",
      "Train Epoch 150.6 var loss 775120.0 reconstruction mse 721370.3125 imputation mse 1.0165886878967285\n",
      "Train Epoch 150.7 var loss 775312.6875 reconstruction mse 721376.0 imputation mse 1.0151841640472412\n",
      "Train Epoch 150.8 var loss 774606.375 reconstruction mse 721033.1875 imputation mse 1.0155612230300903\n",
      "Train Epoch 150.9 var loss 774923.125 reconstruction mse 721457.875 imputation mse 1.0171300172805786\n",
      "====> Test imputation mse: 1.04243600\n",
      "====> Test imputation mse: 1.04547799\n",
      "====> Test imputation mse: 1.04316854\n",
      "Train Epoch 151.0 var loss 772148.25 reconstruction mse 718895.875 imputation mse 1.0053212642669678\n",
      "Train Epoch 151.1 var loss 776753.5 reconstruction mse 723084.125 imputation mse 1.0223731994628906\n",
      "Train Epoch 151.2 var loss 778264.0625 reconstruction mse 723933.3125 imputation mse 1.0216143131256104\n",
      "Train Epoch 151.3 var loss 777248.6875 reconstruction mse 723311.5625 imputation mse 1.0218443870544434\n",
      "Train Epoch 151.4 var loss 777141.1875 reconstruction mse 723401.9375 imputation mse 1.0220943689346313\n",
      "Train Epoch 151.5 var loss 777940.8125 reconstruction mse 723822.875 imputation mse 1.0223807096481323\n",
      "Train Epoch 151.6 var loss 776570.125 reconstruction mse 722830.5 imputation mse 1.0208524465560913\n",
      "Train Epoch 151.7 var loss 776226.125 reconstruction mse 723033.9375 imputation mse 1.0206199884414673\n",
      "Train Epoch 151.8 var loss 776289.0625 reconstruction mse 723334.1875 imputation mse 1.0205368995666504\n",
      "Train Epoch 151.9 var loss 776846.0 reconstruction mse 723669.0625 imputation mse 1.0208383798599243\n",
      "Train Epoch 152.0 var loss 772591.625 reconstruction mse 720088.75 imputation mse 1.0084679126739502\n",
      "Train Epoch 152.1 var loss 777496.875 reconstruction mse 724683.9375 imputation mse 1.0263361930847168\n",
      "Train Epoch 152.2 var loss 777504.125 reconstruction mse 724879.875 imputation mse 1.0269917249679565\n",
      "Train Epoch 152.3 var loss 777533.3125 reconstruction mse 724518.75 imputation mse 1.0256894826889038\n",
      "Train Epoch 152.4 var loss 778623.375 reconstruction mse 725144.125 imputation mse 1.0273549556732178\n",
      "Train Epoch 152.5 var loss 777752.1875 reconstruction mse 724814.75 imputation mse 1.025458574295044\n",
      "Train Epoch 152.6 var loss 777132.875 reconstruction mse 724690.5625 imputation mse 1.0249862670898438\n",
      "Train Epoch 152.7 var loss 777849.75 reconstruction mse 724760.625 imputation mse 1.024845004081726\n",
      "Train Epoch 152.8 var loss 777563.25 reconstruction mse 724319.9375 imputation mse 1.0245263576507568\n",
      "Train Epoch 152.9 var loss 777515.5625 reconstruction mse 724673.625 imputation mse 1.026145577430725\n",
      "Train Epoch 153.0 var loss 775013.8125 reconstruction mse 722208.25 imputation mse 1.0143448114395142\n",
      "Train Epoch 153.1 var loss 778709.0 reconstruction mse 725958.25 imputation mse 1.0299618244171143\n",
      "Train Epoch 153.2 var loss 778499.625 reconstruction mse 725732.6875 imputation mse 1.030238151550293\n",
      "Train Epoch 153.3 var loss 779133.3125 reconstruction mse 725625.625 imputation mse 1.0301131010055542\n",
      "Train Epoch 153.4 var loss 778858.3125 reconstruction mse 725492.0625 imputation mse 1.0296167135238647\n",
      "Train Epoch 153.5 var loss 779251.5625 reconstruction mse 726030.5625 imputation mse 1.0307360887527466\n",
      "Train Epoch 153.6 var loss 779459.6875 reconstruction mse 726375.875 imputation mse 1.0304722785949707\n",
      "Train Epoch 153.7 var loss 777863.1875 reconstruction mse 725534.125 imputation mse 1.0287046432495117\n",
      "Train Epoch 153.8 var loss 777927.375 reconstruction mse 725678.125 imputation mse 1.0292915105819702\n",
      "Train Epoch 153.9 var loss 777903.4375 reconstruction mse 725463.9375 imputation mse 1.029966950416565\n",
      "Train Epoch 154.0 var loss 773790.75 reconstruction mse 721175.75 imputation mse 1.003475308418274\n",
      "Train Epoch 154.1 var loss 777949.4375 reconstruction mse 725522.625 imputation mse 1.0186336040496826\n",
      "Train Epoch 154.2 var loss 778550.25 reconstruction mse 725250.6875 imputation mse 1.0183924436569214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 154.3 var loss 776693.125 reconstruction mse 725105.4375 imputation mse 1.017844796180725\n",
      "Train Epoch 154.4 var loss 778820.375 reconstruction mse 725802.9375 imputation mse 1.019150733947754\n",
      "Train Epoch 154.5 var loss 777468.75 reconstruction mse 725078.3125 imputation mse 1.0183788537979126\n",
      "Train Epoch 154.6 var loss 777458.0625 reconstruction mse 725325.375 imputation mse 1.0181949138641357\n",
      "Train Epoch 154.7 var loss 776297.75 reconstruction mse 724798.9375 imputation mse 1.0162055492401123\n",
      "Train Epoch 154.8 var loss 776783.9375 reconstruction mse 725022.375 imputation mse 1.0184924602508545\n",
      "Train Epoch 154.9 var loss 777074.625 reconstruction mse 724846.5625 imputation mse 1.0175212621688843\n",
      "Train Epoch 155.0 var loss 772790.8125 reconstruction mse 720835.1875 imputation mse 1.0084985494613647\n",
      "Train Epoch 155.1 var loss 773723.8125 reconstruction mse 722045.6875 imputation mse 1.0228707790374756\n",
      "Train Epoch 155.2 var loss 775632.875 reconstruction mse 722732.3125 imputation mse 1.024541974067688\n",
      "Train Epoch 155.3 var loss 774501.4375 reconstruction mse 722196.0 imputation mse 1.0257315635681152\n",
      "Train Epoch 155.4 var loss 773910.8125 reconstruction mse 722165.4375 imputation mse 1.0238336324691772\n",
      "Train Epoch 155.5 var loss 773787.375 reconstruction mse 722173.5 imputation mse 1.0235134363174438\n",
      "Train Epoch 155.6 var loss 773867.25 reconstruction mse 722427.25 imputation mse 1.0236810445785522\n",
      "Train Epoch 155.7 var loss 773899.0 reconstruction mse 722019.5 imputation mse 1.0219752788543701\n",
      "Train Epoch 155.8 var loss 774806.9375 reconstruction mse 722505.4375 imputation mse 1.023242712020874\n",
      "Train Epoch 155.9 var loss 773718.9375 reconstruction mse 721841.6875 imputation mse 1.0212070941925049\n",
      "Train Epoch 156.0 var loss 774024.375 reconstruction mse 721453.6875 imputation mse 1.0100127458572388\n",
      "Train Epoch 156.1 var loss 777748.75 reconstruction mse 725470.3125 imputation mse 1.0266486406326294\n",
      "Train Epoch 156.2 var loss 778091.5 reconstruction mse 725637.5625 imputation mse 1.0271317958831787\n",
      "Train Epoch 156.3 var loss 776936.0 reconstruction mse 725070.75 imputation mse 1.025447130203247\n",
      "Train Epoch 156.4 var loss 776259.8125 reconstruction mse 724680.9375 imputation mse 1.0249192714691162\n",
      "Train Epoch 156.5 var loss 777686.375 reconstruction mse 725254.25 imputation mse 1.025418996810913\n",
      "Train Epoch 156.6 var loss 777005.8125 reconstruction mse 725155.625 imputation mse 1.0241894721984863\n",
      "Train Epoch 156.7 var loss 776315.6875 reconstruction mse 725126.375 imputation mse 1.0247138738632202\n",
      "Train Epoch 156.8 var loss 776643.4375 reconstruction mse 725018.5625 imputation mse 1.025068998336792\n",
      "Train Epoch 156.9 var loss 776613.5 reconstruction mse 725026.125 imputation mse 1.0245064496994019\n",
      "Train Epoch 157.0 var loss 773654.0625 reconstruction mse 721695.375 imputation mse 1.0110259056091309\n",
      "Train Epoch 157.1 var loss 776529.0 reconstruction mse 724340.25 imputation mse 1.0259056091308594\n",
      "Train Epoch 157.2 var loss 776282.875 reconstruction mse 723882.25 imputation mse 1.0252684354782104\n",
      "Train Epoch 157.3 var loss 776421.625 reconstruction mse 724334.6875 imputation mse 1.0269954204559326\n",
      "Train Epoch 157.4 var loss 775786.25 reconstruction mse 723803.375 imputation mse 1.0252236127853394\n",
      "Train Epoch 157.5 var loss 775694.5 reconstruction mse 723779.1875 imputation mse 1.0258045196533203\n",
      "Train Epoch 157.6 var loss 776391.1875 reconstruction mse 723898.5 imputation mse 1.0255788564682007\n",
      "Train Epoch 157.7 var loss 775858.625 reconstruction mse 723627.125 imputation mse 1.0250790119171143\n",
      "Train Epoch 157.8 var loss 776413.875 reconstruction mse 724221.75 imputation mse 1.0247087478637695\n",
      "Train Epoch 157.9 var loss 775601.5 reconstruction mse 724045.625 imputation mse 1.026216983795166\n",
      "Train Epoch 158.0 var loss 773696.75 reconstruction mse 722145.0625 imputation mse 1.0098849534988403\n",
      "Train Epoch 158.1 var loss 774688.3125 reconstruction mse 722812.5625 imputation mse 1.0249065160751343\n",
      "Train Epoch 158.2 var loss 775052.875 reconstruction mse 722627.625 imputation mse 1.0259746313095093\n",
      "Train Epoch 158.3 var loss 773418.6875 reconstruction mse 721940.5625 imputation mse 1.0234366655349731\n",
      "Train Epoch 158.4 var loss 774011.4375 reconstruction mse 722567.375 imputation mse 1.0268489122390747\n",
      "Train Epoch 158.5 var loss 773891.375 reconstruction mse 722546.75 imputation mse 1.0239228010177612\n",
      "Train Epoch 158.6 var loss 773412.375 reconstruction mse 722013.375 imputation mse 1.0241912603378296\n",
      "Train Epoch 158.7 var loss 773989.8125 reconstruction mse 722383.5 imputation mse 1.0272444486618042\n",
      "Train Epoch 158.8 var loss 773934.0625 reconstruction mse 722190.1875 imputation mse 1.0241142511367798\n",
      "Train Epoch 158.9 var loss 773157.0 reconstruction mse 721884.8125 imputation mse 1.0241087675094604\n",
      "Train Epoch 159.0 var loss 774288.25 reconstruction mse 722557.125 imputation mse 1.012692928314209\n",
      "Train Epoch 159.1 var loss 777410.9375 reconstruction mse 726536.25 imputation mse 1.0281593799591064\n",
      "Train Epoch 159.2 var loss 778753.0625 reconstruction mse 726898.6875 imputation mse 1.0276710987091064\n",
      "Train Epoch 159.3 var loss 778924.5 reconstruction mse 726942.0625 imputation mse 1.0292229652404785\n",
      "Train Epoch 159.4 var loss 777512.25 reconstruction mse 726655.5625 imputation mse 1.0283195972442627\n",
      "Train Epoch 159.5 var loss 779222.8125 reconstruction mse 727212.8125 imputation mse 1.0280119180679321\n",
      "Train Epoch 159.6 var loss 777604.375 reconstruction mse 726335.8125 imputation mse 1.0271610021591187\n",
      "Train Epoch 159.7 var loss 776818.3125 reconstruction mse 726275.75 imputation mse 1.0262781381607056\n",
      "Train Epoch 159.8 var loss 777675.1875 reconstruction mse 726759.0 imputation mse 1.0272537469863892\n",
      "Train Epoch 159.9 var loss 778185.0625 reconstruction mse 727220.375 imputation mse 1.027734398841858\n",
      "Train Epoch 160.0 var loss 773575.9375 reconstruction mse 722534.9375 imputation mse 1.0034054517745972\n",
      "Train Epoch 160.1 var loss 775346.375 reconstruction mse 724306.6875 imputation mse 1.0191216468811035\n",
      "Train Epoch 160.2 var loss 774920.625 reconstruction mse 723573.4375 imputation mse 1.018007516860962\n",
      "Train Epoch 160.3 var loss 774826.4375 reconstruction mse 723774.0 imputation mse 1.0179460048675537\n",
      "Train Epoch 160.4 var loss 775955.75 reconstruction mse 724224.0625 imputation mse 1.019639492034912\n",
      "Train Epoch 160.5 var loss 774966.5 reconstruction mse 723857.0625 imputation mse 1.0193181037902832\n",
      "Train Epoch 160.6 var loss 774599.8125 reconstruction mse 723704.9375 imputation mse 1.018426775932312\n",
      "Train Epoch 160.7 var loss 774538.375 reconstruction mse 723828.25 imputation mse 1.0179624557495117\n",
      "Train Epoch 160.8 var loss 774988.375 reconstruction mse 724110.25 imputation mse 1.0191199779510498\n",
      "Train Epoch 160.9 var loss 774835.6875 reconstruction mse 724107.8125 imputation mse 1.019355058670044\n",
      "====> Test imputation mse: 1.05078030\n",
      "====> Test imputation mse: 1.05048215\n",
      "====> Test imputation mse: 1.04559040\n",
      "Train Epoch 161.0 var loss 771447.0625 reconstruction mse 719611.8125 imputation mse 0.9992831945419312\n",
      "Train Epoch 161.1 var loss 774373.3125 reconstruction mse 723379.4375 imputation mse 1.01524019241333\n",
      "Train Epoch 161.2 var loss 773780.9375 reconstruction mse 722932.125 imputation mse 1.0151053667068481\n",
      "Train Epoch 161.3 var loss 774784.625 reconstruction mse 723566.9375 imputation mse 1.0158463716506958\n",
      "Train Epoch 161.4 var loss 774424.4375 reconstruction mse 723205.5625 imputation mse 1.014600157737732\n",
      "Train Epoch 161.5 var loss 774409.625 reconstruction mse 723596.6875 imputation mse 1.0154424905776978\n",
      "Train Epoch 161.6 var loss 775252.0 reconstruction mse 723086.625 imputation mse 1.0140635967254639\n",
      "Train Epoch 161.7 var loss 774864.375 reconstruction mse 723214.0 imputation mse 1.0144245624542236\n",
      "Train Epoch 161.8 var loss 774361.0 reconstruction mse 723116.5 imputation mse 1.0140308141708374\n",
      "Train Epoch 161.9 var loss 774268.5 reconstruction mse 722782.0625 imputation mse 1.01371169090271\n",
      "Train Epoch 162.0 var loss 773268.125 reconstruction mse 721849.0625 imputation mse 1.0119712352752686\n",
      "Train Epoch 162.1 var loss 776930.3125 reconstruction mse 725782.5 imputation mse 1.0273206233978271\n",
      "Train Epoch 162.2 var loss 777215.8125 reconstruction mse 725730.625 imputation mse 1.0282343626022339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 162.3 var loss 775519.375 reconstruction mse 725075.875 imputation mse 1.026511788368225\n",
      "Train Epoch 162.4 var loss 776339.4375 reconstruction mse 725022.1875 imputation mse 1.0271164178848267\n",
      "Train Epoch 162.5 var loss 776277.25 reconstruction mse 724935.5 imputation mse 1.0257434844970703\n",
      "Train Epoch 162.6 var loss 776165.625 reconstruction mse 724626.1875 imputation mse 1.026313066482544\n",
      "Train Epoch 162.7 var loss 775544.125 reconstruction mse 724787.625 imputation mse 1.0262082815170288\n",
      "Train Epoch 162.8 var loss 775678.9375 reconstruction mse 724593.875 imputation mse 1.0253863334655762\n",
      "Train Epoch 162.9 var loss 775410.625 reconstruction mse 724673.1875 imputation mse 1.0258628129959106\n",
      "Train Epoch 163.0 var loss 771128.4375 reconstruction mse 720502.25 imputation mse 1.0049585103988647\n",
      "Train Epoch 163.1 var loss 772940.0625 reconstruction mse 722084.875 imputation mse 1.01921546459198\n",
      "Train Epoch 163.2 var loss 772843.875 reconstruction mse 722081.125 imputation mse 1.020643711090088\n",
      "Train Epoch 163.3 var loss 772992.5 reconstruction mse 722119.0625 imputation mse 1.0217342376708984\n",
      "Train Epoch 163.4 var loss 772468.9375 reconstruction mse 722038.875 imputation mse 1.0209155082702637\n",
      "Train Epoch 163.5 var loss 772877.625 reconstruction mse 722004.8125 imputation mse 1.0207158327102661\n",
      "Train Epoch 163.6 var loss 772759.625 reconstruction mse 721917.1875 imputation mse 1.0208992958068848\n",
      "Train Epoch 163.7 var loss 772611.4375 reconstruction mse 721848.5 imputation mse 1.0196012258529663\n",
      "Train Epoch 163.8 var loss 772033.1875 reconstruction mse 722169.0625 imputation mse 1.019382357597351\n",
      "Train Epoch 163.9 var loss 773363.1875 reconstruction mse 722147.375 imputation mse 1.0204607248306274\n",
      "Train Epoch 164.0 var loss 770642.875 reconstruction mse 719853.5 imputation mse 1.0041695833206177\n",
      "Train Epoch 164.1 var loss 771717.625 reconstruction mse 721351.1875 imputation mse 1.020093321800232\n",
      "Train Epoch 164.2 var loss 771890.25 reconstruction mse 721157.1875 imputation mse 1.0217669010162354\n",
      "Train Epoch 164.3 var loss 771576.75 reconstruction mse 720832.625 imputation mse 1.0222764015197754\n",
      "Train Epoch 164.4 var loss 771976.125 reconstruction mse 721191.0625 imputation mse 1.0219171047210693\n",
      "Train Epoch 164.5 var loss 771616.9375 reconstruction mse 721149.0 imputation mse 1.0218214988708496\n",
      "Train Epoch 164.6 var loss 771102.5625 reconstruction mse 721000.0 imputation mse 1.02121901512146\n",
      "Train Epoch 164.7 var loss 771141.1875 reconstruction mse 720727.6875 imputation mse 1.0218164920806885\n",
      "Train Epoch 164.8 var loss 770920.8125 reconstruction mse 720636.375 imputation mse 1.0206968784332275\n",
      "Train Epoch 164.9 var loss 771421.625 reconstruction mse 721045.0 imputation mse 1.0214179754257202\n",
      "Train Epoch 165.0 var loss 769904.3125 reconstruction mse 719754.3125 imputation mse 1.0080243349075317\n",
      "Train Epoch 165.1 var loss 773521.875 reconstruction mse 721779.625 imputation mse 1.0262824296951294\n",
      "Train Epoch 165.2 var loss 772654.625 reconstruction mse 721976.8125 imputation mse 1.024973750114441\n",
      "Train Epoch 165.3 var loss 773222.75 reconstruction mse 722502.25 imputation mse 1.0257985591888428\n",
      "Train Epoch 165.4 var loss 773401.4375 reconstruction mse 721560.3125 imputation mse 1.0245341062545776\n",
      "Train Epoch 165.5 var loss 773294.3125 reconstruction mse 722118.375 imputation mse 1.0257271528244019\n",
      "Train Epoch 165.6 var loss 772426.25 reconstruction mse 721912.75 imputation mse 1.0250213146209717\n",
      "Train Epoch 165.7 var loss 772431.375 reconstruction mse 721938.875 imputation mse 1.025913119316101\n",
      "Train Epoch 165.8 var loss 772042.125 reconstruction mse 721812.0625 imputation mse 1.025580644607544\n",
      "Train Epoch 165.9 var loss 772524.0 reconstruction mse 721669.0625 imputation mse 1.024750828742981\n",
      "Train Epoch 166.0 var loss 771571.0625 reconstruction mse 721106.0 imputation mse 1.011344075202942\n",
      "Train Epoch 166.1 var loss 774577.4375 reconstruction mse 724263.25 imputation mse 1.0272482633590698\n",
      "Train Epoch 166.2 var loss 774938.3125 reconstruction mse 724547.9375 imputation mse 1.0277464389801025\n",
      "Train Epoch 166.3 var loss 774978.6875 reconstruction mse 724550.0 imputation mse 1.0274438858032227\n",
      "Train Epoch 166.4 var loss 775375.4375 reconstruction mse 724845.5 imputation mse 1.0271916389465332\n",
      "Train Epoch 166.5 var loss 773941.0625 reconstruction mse 724110.9375 imputation mse 1.0255601406097412\n",
      "Train Epoch 166.6 var loss 775040.0625 reconstruction mse 724581.9375 imputation mse 1.0262093544006348\n",
      "Train Epoch 166.7 var loss 775383.5625 reconstruction mse 724878.5 imputation mse 1.0272917747497559\n",
      "Train Epoch 166.8 var loss 774992.4375 reconstruction mse 724565.5625 imputation mse 1.0267139673233032\n",
      "Train Epoch 166.9 var loss 775123.0 reconstruction mse 724540.9375 imputation mse 1.0262778997421265\n",
      "Train Epoch 167.0 var loss 771368.125 reconstruction mse 721243.0 imputation mse 1.0117143392562866\n",
      "Train Epoch 167.1 var loss 773090.25 reconstruction mse 723358.125 imputation mse 1.0283070802688599\n",
      "Train Epoch 167.2 var loss 774135.4375 reconstruction mse 723710.375 imputation mse 1.0286636352539062\n",
      "Train Epoch 167.3 var loss 773658.9375 reconstruction mse 723596.75 imputation mse 1.0278385877609253\n",
      "Train Epoch 167.4 var loss 773238.4375 reconstruction mse 723834.9375 imputation mse 1.0287261009216309\n",
      "Train Epoch 167.5 var loss 772911.25 reconstruction mse 723790.125 imputation mse 1.0286020040512085\n",
      "Train Epoch 167.6 var loss 773502.4375 reconstruction mse 723655.3125 imputation mse 1.0286173820495605\n",
      "Train Epoch 167.7 var loss 773383.3125 reconstruction mse 723753.3125 imputation mse 1.0283257961273193\n",
      "Train Epoch 167.8 var loss 773440.75 reconstruction mse 723552.8125 imputation mse 1.0284087657928467\n",
      "Train Epoch 167.9 var loss 773386.375 reconstruction mse 723654.625 imputation mse 1.0274381637573242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fc526027aeca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-cc7f222cc6c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl\u001b[0m  \u001b[0;31m# set to 1 for variational regularization (centered gaussian)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         print(f\"Train Epoch {epoch}.{n}\", \n",
      "\u001b[0;32m~/mono/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mono/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()\n",
    "test()\n",
    "test()\n",
    "for epoch in range(1, 10000000):\n",
    "    train(epoch)\n",
    "    if epoch%10==0:\n",
    "        test()\n",
    "        test()\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mono",
   "language": "python",
   "name": "mono"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
